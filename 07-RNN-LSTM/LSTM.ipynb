{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long Sort Term Memory(LSTM):\n",
    "\n",
    "LSTM stands for Long Short Term Memory. It was designed to help the vanishing gradient problem on RNNs.\n",
    "\n",
    "   ![image](../images/LSTM-1.png)\n",
    "\n",
    "On steap, `t` there is a `hidden state` **_$h_t$_** and `cell state` **_$c_t$_**. Both **_$h_t$ and $c_t$_** are vector size of `n`. One key distinction of LSTM from Vanila RNN is that LSTM has this `cell state` $c_t$ where long term information will be stored. `LSTM` can read, write, eraseinfromation to and from this $c_t$ cell state. This operation is executed through three special gate **_`i(input),f(forget),o(output)`_**. **_$i,f,o$_** gates are vector size of $n$. These all three gates use sigmod activation function so, their value range form `0 to 1`. The value of these gates vary from closed(0) to open(1).\n",
    "\n",
    "   ![image](../images/lstm-2.png)\n",
    "\n",
    "\n",
    "At every timestep, we have an input vector $x_t$, prcious hidden state $h_{t-1}$, previous cell state $c_{t-1}$ and LSTM computes the next cell state $c_t$ and hidden state $h_t$ as follows:\n",
    "\n",
    "$$f_t=\\sigma(W_{hf}h_{t_1} + W_{xf}x_t)$$\n",
    "$$i_t=\\sigma(W_{hi}h_{t_1} + W_{xi}x_t)$$\n",
    "$$o_t=\\sigma(W_{ho}h_{t_1} + W_{xo}o_t)$$\n",
    "$$g_t=\\tanh(W_{hg}h_{t_1} + W_{xg}x_t)$$\n",
    "\n",
    "![image](../images/lstm-5.png)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "c_t &= f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
    "h_t &= o_t \\odot \\text{tanh}(c_t) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "![image](../images/lstm-6.png)\n",
    "\n",
    "\n",
    "1. **_`Forget Gate(f):`_** Forget gate $f_t$ at time step $t$\n",
    "   controls how much information needs to be “removed” from the previous cell state $c_{t−1}$.This forget gate learns to erase hidden representations from the previous time steps, which is why LSTM will have two hidden represtnations $h_t$ and cell state $c_t$\n",
    "   . This $c_t$ will get propagated over time and learn whether to forget the previous cell state or not.\n",
    "2. **_`Input Gate(i):`_** Input gate $i_t$ at time step $t$\n",
    " controls how much information needs to be “added” to the next cell state $c_t$ from previous hidden state $h_{t−1}$\n",
    " and input $x_t$. This serves as a switch, where values are either almost always zero or almost always one. This “input” gate decides whether to take the RNN output that is produced by the “gate” gate $g$ and multiplies the output with input gate $i$.\n",
    "3. **_`Output Gate(o):`_** Output gate $o_t$ at time step $t$\n",
    "   controls how much information needs to be “shown” as output in the current hidden state $h_t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LSTM Back propagation:\n",
    "![image](../images/lstm-3.png)\n",
    "\n",
    "![image](../images/lstm-4.png)\n",
    "\n",
    "1. The LSTM gradients are easily computed like ResNet\n",
    "2. The LSTM is keeping data on the long or short memory as it trains means it can remember not just the things from last layer but layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
