{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    "An activation function, in the context of artificial neural networks and deep learning, is a mathematical function applied to the output of a neuron or a set of neurons to  normalize the input and produced output which is then passed forward into the subsequent layer. It introduces non-linearity into the network, allowing it to model complex relationships between inputs and outputs. In other words, a neural network without an activation function is essentially just a linear regression model.\n",
    "\n",
    "Activation functions are typically applied element-wise to the output of each neuron in a neural network layer. They transform the weighted sum of inputs plus a bias term into an output signal that is passed to the next layer.\n",
    "\n",
    "Two Types of activation function:\n",
    "1. Linear Activation Function.\n",
    "2. Non Linear Activation function.\n",
    "\n",
    "***1.1.1 Linear activation function:***\n",
    "In the linear activation function, the output of functions is not restricted in between any range. Its range is specified from -infinity to infinity.\n",
    "\n",
    "$$f(x)= x+5$$\n",
    "\n",
    "***1.1.2 Non Linear activation function:***\n",
    "Since the non-linear function comes up with derivative functions, the problems related to backpropagation have been successfully solved.\n",
    "$$f(x)= x^2*w + b$$\n",
    "\n",
    "![Activation-Function:](../images/activation-function.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Sigmoid activation function [0,1]:\n",
    "The sigmoid non-linearity takes a real-valued number and “squashes” it into range between 0 and 1.\n",
    "$$\\sigma(x) = 1 / (1 + e^{-x})$$\n",
    "Advantage:\n",
    "1. Squashes numbers to range [0,1]\n",
    "2. Historically popular since they have nice interpretation as a saturating “firing rate” of a neuron\n",
    "\n",
    "Disadvantage:\n",
    "1. Saturated neurons “kill” the gradients\n",
    "    - Gradients are in most cases near 0 (Big values/small values), that kills the updates if the graph/network are large.\n",
    "2. Sigmoid outputs are not zero-centered\n",
    "    - if the data coming into a neuron is always positive (e.g. $x>0$ elementwise in $f=w^Tx+b$), then the gradient on the weights w will during backpropagation become either all be positive, or all negative (depending on the gradient of the whole expression f)\n",
    "3. $exp()$ is a bit compute expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmooid_function(x):\n",
    "    sig=1/(1+math.exp(-x)) #1/(1+np.exp(-x))\n",
    "    # print(\"Sigmoid Function:\", sig)\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999546021312976"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmooid_function(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tanh activation function [-1,1]\n",
    "$tanh(x)=2σ(2x)−1$ or  $tanh(x)={2/(1+e^{-2x})}−1$\n",
    "\n",
    "Advantage:\n",
    "1. Squashes numbers to range [-1,1]\n",
    "2. zero centered (nice)\n",
    "\n",
    "Disadvantage:\n",
    "1. still kills gradients when saturated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_function(x):\n",
    "    thf= math.tanh(x)\n",
    "    # print(thf)\n",
    "    return thf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh_function(-1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Relu activation function [0,x]\n",
    "$max(0.0, x)$\n",
    "\n",
    "Advantage:\n",
    "- Does not saturate (in +region)\n",
    "- Does not kill the gradient.\n",
    "    - Only small values that are killed. Killed the gradient in the half.\n",
    "- Very computationally efficient\n",
    "- Converges much faster than sigmoid/tanh in practice (e.g. 6x)\n",
    "- Actually more biologically plausible than sigmoid.\n",
    "\n",
    "Disadvantage:\n",
    "- Not zero-centered output\n",
    "- If weights aren't initialized good, maybe 75% of the neurons will be dead and thats a waste computation. But its still works. This is an active area of research to optimize this.\n",
    "    - To solve the issue mentioned above, people might initialize all the biases by 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_function(x):\n",
    "\trelu=max(0.0, x)\n",
    "\t# print(\"Relu Function:\", relu)\n",
    "\treturn relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu_function(-10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Leaky Relu function:\n",
    "$f(x) = max(αx, x)$\n",
    "\n",
    "    - Does not saturate\n",
    "    - Computationally efficient\n",
    "    - Converges much faster than sigmoid/tanh in practice! (e.g. 6x)\n",
    "    - will not “die”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu_function(x):\n",
    "    if x>0:\n",
    "        return x\n",
    "    else:\n",
    "        return .01*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-10.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaky_relu_function(-1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Expotential Relu Function:\n",
    "    - All benefits of ReLU\n",
    "    - Closer to zero mean outputs\n",
    "    - Negative saturation regime compared with Leaky ReLU adds some robustness to noise\n",
    "    - Computation requires exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_relu_function(x):\n",
    "    '''\n",
    "    '''\n",
    "    if x>0:\n",
    "        return x\n",
    "    else:\n",
    "        return .01*(np.exp(x)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.009999546000702375"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_relu_function(-10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Maxout activation:\n",
    "- $maxout(x) = max(w_1.T*x + b_1, w_2.T*x + b_2)$\n",
    "- Generalizes RELU and Leaky RELU\n",
    "- Doesn't die!\n",
    "- Problems:\n",
    "    - doubles the number of parameters per neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxout_function(x, weights, biases):\n",
    "    linear_outputs=np.dot(x, weights)+biases\n",
    "    max_output= np.max(linear_outputs, axis=1)\n",
    "    return max_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [3.  6.6]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3], [4, 5, 6]])  # Input\n",
    "weights = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])  # Weights\n",
    "biases = np.array([0.1, 0.2])  # Biases\n",
    "\n",
    "output = maxout_function(x, weights, biases)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Point:\n",
    "    - Use ReLU. Be careful with your learning rates\n",
    "    - Try out Leaky ReLU / Maxout / ELU\n",
    "    - Try out tanh but don’t expect much\n",
    "    - Don’t use sigmoid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Softmax Function:\n",
    "The softmax function is a commonly used activation function in machine learning, particularly in multi-class classification problems. It takes a vector of real numbers as input and transforms them into a probability distribution over multiple classes. The softmax function ensures that the output probabilities sum up to 1.\n",
    "\n",
    "$f_j(z) = \\frac{e^{z_j}}{\\sum_k e^{z_k}}$\n",
    "\n",
    "Here,\n",
    "- $z$ is the input vector\n",
    "- $k$ The number of classes in the multi-class classifier.\n",
    "\n",
    "Example:\n",
    "$z = \\left[ \\begin{array}{rr} 8  \\\\ 5 \\\\ 0 \\end{array}\\right] \\hspace{1cm} $<br>\n",
    "\n",
    "Calculation:\n",
    "\n",
    "$e^{z_1}=e^8= 2981$<br>\n",
    "$e^{z_2}=e^5= 148.4$<br>\n",
    "$e^{z_3}=e^0= 1.0$<br>\n",
    "$\\sum_k e^{z_k}= e^8+e^5+e^0=3130.4$ <br>\n",
    "$f_1(z_1) = \\frac{e^{z_1}}{\\sum_k e^{z_k}}$ $=2981/3130.4=0.953$<br>\n",
    "$f_2(z_2) = \\frac{e^{z_2}}{\\sum_k e^{z_k}}$ $=148.4/3130.4=0.0474$<br>\n",
    "$f_3(z_3) = \\frac{e^{z_3}}{\\sum_k e^{z_k}}$ $=1/3130.4=0.0003$<br>\n",
    "\n",
    "It is informative to check that we have three output values which are all valid probabilities, that is they lie between 0 and 1, and they sum to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax numpy: [0.65900114 0.24243297 0.09856589]\n"
     ]
    }
   ],
   "source": [
    "# Softmax function\n",
    "import numpy as np\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "x = np.array([2.0, 1.0, 0.1])\n",
    "outputs = softmax(x)\n",
    "print('softmax numpy:', outputs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Argmax Function:\n",
    "The argmax function is a mathematical function that returns the argument (input) that maximizes a given function or expression. In the context of machine learning and classification tasks, the argmax function is commonly used to determine the class with the highest predicted probability or score.\n",
    "\n",
    "$$argmax(f) = arg \\ max f(z)$$\n",
    "\n",
    "$z = \\left[ \\begin{array}{rr} 8  \\\\ 5 \\\\ 0 \\end{array}\\right] \\hspace{1cm}$<br>\n",
    "$argmax(f) = arg \\ max f(z)= \\left[ \\begin{array}{rr} 1  \\\\ 0 \\\\ 0 \\end{array}\\right] \\hspace{1cm}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "scores = np.array([3, 4, 2, 5])\n",
    "argmax_index = np.argmax(scores)\n",
    "\n",
    "print(argmax_index) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
