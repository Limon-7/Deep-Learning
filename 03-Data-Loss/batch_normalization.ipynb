{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization:\n",
    "- step-1: Calculate the mean `mean = (1 / N) * sum(x_i)`\n",
    "- step-2: Subtract the mean from each input `x-mean`\n",
    "- setep-3: calculate the varience\n",
    "- step-4: normalize the input\n",
    "- step-5: calculate the scale and shif\n",
    "- for training cache the output for backpropagation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before normalized:\n",
      " mean=[-10.33110021 -13.35263687  14.88236006] std:[28.55721837 30.99596294 34.26431573]\n",
      "After batch normalization mean close to (beta=0) and std near to (gamma=1)\n",
      " mean:[ 2.03725925e-16  2.30648833e-16 -8.88178420e-18] std:[1. 1. 1.]\n",
      "After batch normalization mean should close to beta=( [11. 12. 13.] )and std should close to gamam (gamma= [1. 2. 3.] )\n",
      " (mean: [11. 12. 13.] std: [1. 2. 3.] ) \n"
     ]
    }
   ],
   "source": [
    "# a simple mini-batch implementation using numpy \n",
    "def batch_normalization_forward(x, gamma, beta, running_mean, running_var, eps=1e-8, training=True, momentum=0.9):\n",
    "    '''In Batch Normalization, the momentum parameter is used to control the update of running statistics (mean and variance) during training. It determines the contribution of the current batch's statistics to the overall running statistics.(0<=momemntum<=1)'''\n",
    "    if(training):\n",
    "        \n",
    "        mu = np.mean(x, axis=0) # Step 1: Calculate mean\n",
    "        \n",
    "        xmu = x - mu   # Step 2: Subtract mean\n",
    "        \n",
    "        var = np.mean(xmu**2, axis=0)   # Step 3: Calculate variance\n",
    "\n",
    "        std = np.sqrt(var + eps)         # Step 4: Calculate standard deviation\n",
    "        \n",
    "        istd = 1.0 / std # Step 5: Invert standard deviation\n",
    "        \n",
    "        xhat = xmu*istd         # Step 6: Normalize\n",
    "       \n",
    "        out =xhat*gamma + beta         # Step 7: Scale and shift using element wise multiplication\n",
    "\n",
    "        '''Update the running mean'''\n",
    "        running_mean= momentum*running_mean + (1-momentum)*mu\n",
    "        running_var = momentum*running_var+(1-momentum)*running_var\n",
    "\n",
    "        cache = (xhat, gamma, xmu, istd, std, var, eps)         # Store intermediate values for backpropagation\n",
    "    else:\n",
    "        xhat= (x- running_mean)/np.sqrt(running_var+eps)\n",
    "        out= gamma*xhat+ beta\n",
    "        cache=None\n",
    "    \n",
    "    return out, cache\n",
    "\n",
    "def calcute_mean(a):\n",
    "    am= np.mean(a, axis=0)\n",
    "    asd= np.std(a, axis=0)\n",
    "    return am, asd\n",
    "\n",
    "# Initialize\n",
    "np.random.seed(100)\n",
    "N, D1, D2, D3 = 200, 50, 60, 3\n",
    "X = np.random.randn(N, D1)\n",
    "W1 = np.random.randn(D1, D2)\n",
    "W2 = np.random.randn(D2, D3)\n",
    "W1Xo= X.dot(W1)\n",
    "\n",
    "a = np.maximum(0, W1Xo).dot(W2)\n",
    "\n",
    "a_mean, a_asd=calcute_mean(a)\n",
    "print(f\"Before normalized:\\n mean={a_mean} std:{a_asd}\")\n",
    "\n",
    "\n",
    "gamma = np.ones(D3)           # Scale parameter\n",
    "beta = np.zeros((D3,))           # Shift parameter\n",
    "\n",
    "# Initialize running mean and variance as zeros\n",
    "running_mean = np.zeros(D3)\n",
    "running_var = np.zeros(D3)\n",
    "\n",
    "\n",
    "out, cache= batch_normalization_forward(a, gamma, beta, running_mean, running_var)\n",
    "a_mean, a_sd=calcute_mean(out)\n",
    "print(f'After batch normalization mean close to (beta=0) and std near to (gamma=1)\\n mean:{a_mean} std:{a_sd}')\n",
    "\n",
    "gamma = np.asarray([1.0, 2.0, 3.0])\n",
    "beta = np.asarray([11.0, 12.0, 13.0])\n",
    "\n",
    "# # Perform batch normalization forward pass\n",
    "out, cache = batch_normalization_forward(a, gamma, beta, running_mean, running_var)\n",
    "\n",
    "# Now means should be close to beta and stds close to gamma\n",
    "a_mean, a_sd=calcute_mean(out)\n",
    "print(f'After batch normalization mean should close to beta=(',beta,')and std should close to gamam (gamma=', gamma,')\\n (mean:', a_mean, 'std:',a_sd, ') ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Batch Normalization (Back Propagation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class BatchNormalization:\n",
    "    def __init__(self, input_size, eps=1e-8, momentum=0.9) -> None:\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.running_mean = np.zeros(input_size)\n",
    "        self.running_var = np.zeros(input_size)\n",
    "        self.gamma = np.ones(input_size)\n",
    "        self.beta = np.zeros(input_size)\n",
    "        \n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, X, training=True):\n",
    "        if(training):\n",
    "            batch_mean=np.mean(X, axis=0)\n",
    "            batch_var=np.var(X, axis=0)\n",
    "            x_normalized=(X-batch_mean)/np.sqrt(batch_var+self.eps)\n",
    "            x_out=x_normalized*self.gamma+ self.beta\n",
    "            self.running_mean=self.momentum*batch_mean+(1-self.momentum)*batch_mean\n",
    "            self.running_var=self.momentum*batch_var+(1-self.momentum)*batch_var\n",
    "\n",
    "            self.cache=(x_normalized, batch_mean, batch_var)\n",
    "        else:\n",
    "            x_normalized=(X-self.running_mean)/np.sqrt(self.running_var+self.eps)\n",
    "            x_out= x_normalized*self.gamma+ self.beta\n",
    "        return x_out\n",
    "    \n",
    "    def back(self,d_out):\n",
    "        x_normalized, batch_mean, batch_var = self.cache\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:2\n",
      " array:\n",
      "[[0 0 3 0]\n",
      " [2 4 2 2]]\n",
      "R:2 M:4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(100)\n",
    "a= np.random.randint(5, size=(2, 4))\n",
    "print(f\"shape:{a.ndim}\\n array:\\n{a}\")\n",
    "\n",
    "N,M=a.shape # here N= batch size and M= input size\n",
    "print(f\"R:{N} M:{M}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
