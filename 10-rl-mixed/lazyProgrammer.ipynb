{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3#Dynamic Programming\n",
    "The term dynamic programming refers to a collection of algorithoms that can be used to compute optimal policies given a perfect model of the environment as a Markov Decision Process(MDP)\n",
    "1. Policy Evaluation(Prediction)\n",
    "2. Policy Improvement\n",
    "3. Policy Iteration\n",
    "4. Value Iteration\n",
    "5. Aynchronous DP\n",
    "6. Generalized Policy Iteration\n",
    "7. Efficiency in Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1#Policy Evaluation:\n",
    "The technique to compute value-function $v_{\\pi}$ or $q_{\\pi}$ for an arbitrary policy $\\pi$ is called `Policy Evaluation` according to DP.\n",
    "$$v_{\\pi}(s)=\\mathcal{ \\sum_{a\\in A} \\pi(a|s)(R^{a}_{s}+ \\gamma \\sum_{s'\\in S}P^{a}_{ss'}v_r(s'))}$$\n",
    "- Here everything is known except `v`\n",
    "- This equation is Linear.\n",
    "- If there is `n state` there will be `n no of v`.\n",
    "\n",
    "**Dp approach:**\n",
    "1. Initialize $v_0(s)=0 \\text{ or random for all states (0 for terminal state)}$\n",
    "2. $$v_{k+1}(s)=\\mathcal{ \\sum_{a\\in A} \\pi(a|s)(R^{a}_{s}+ \\gamma \\sum_{s'\\in S}P^{a}_{ss'}v_k(s'))}$$\n",
    "3. $$v^{k+1}(s)=\\mathcal{ R^{\\pi}+ \\gamma P^{\\pi}v^k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why does this works? $v_{\\pi}(s)= v_{\\infty}(s)$\n",
    "$$v_{k+1}(s)=\\mathcal{ \\sum_{a\\in A} \\pi(a|s)(R^{a}_{s}+ \\gamma \\sum_{s'\\in S}P^{a}_{ss'}v_k(s'))}$$\n",
    "- $v_{\\pi}(s)$ is a fixed point for this update what we are looking for. When update rule $v_{\\pi}(s)$ will no longer change is called a fixed point.\n",
    "\n",
    "#### When do we quit?\n",
    "- We approach true answer as $k\\to \\infty$\n",
    "- How: check how much $v_k(s)$ has changed from $k\\to {k+1}$\n",
    "  - $\\delta = \\mathcal {\\max_{s}|V_{k+1}(s)-v_k(s)|}$\n",
    "  - Exit when $\\delta \\lt {threshold}; {threshod=[10^{-3}, 10^{-5}, 10^{-8}]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2#Policy Improvement\n",
    "Computing value-function for a policy will help to find better policies. Suppose, we have calculated the value-function $V^{\\pi}$ for an arbitrary policy $\\pi$. For some state`S` we would like to know whether or not we should change the policy deterministically choose an action ***$\\mathcal{a\\neq} \\pi (s)$***\n",
    "\n",
    "#### Question: We already know how good it is to follow current policy from `S` to $V^r(s)$ would it better or worse to change to the new policy?\n",
    "One way to answer this question is to select `a` in `s` and thereafter following the `existing policy`, $\\pi$ and calculate the q function.\n",
    "$$\\mathcal {q^{r}(s,a)=\\mathbb{E}_{\\pi}[R_{t+1} + \\gamma V^{\\pi}(S_{t+1}) | S_t=s, A_t=a]  }$$\n",
    "$$\\mathcal {q^{r}(s,a)=\\sum_{s'\\in S}P^{a}_{ss'} [R^{a}_{ss'}+\\gamma v^{\\pi}(s')]}$$\n",
    "\n",
    "if $q^{r}(s,a)\\gt V^{\\pi}(s)$ if it is better to select `a` once in `s` and thereafter follow $\\pi$ than it would be to follow $\\pi$ all the time--then one would expect it to be better still to select `a` every time `s` is encountered, and that the new policy would in fact be a better one overall.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theoream:\n",
    "1. Consider a deterministic policy, $a=\\pi(s)$\n",
    "2. We can improve this policy by acting greedily\n",
    "   $$\\pi'(s)=\\argmax_{a\\in A}q_{\\pi}(s,a)$$\n",
    "3. This improve the value from any state `s` over one step,\n",
    "   $$q_{\\pi} (s, \\pi'(s))= \\max_{a\\in A} q_{\\pi}(s,a)\\geq q_{\\pi}(s, \\pi(s))=v_{\\pi}(s)$$\n",
    "4. It therefore, It improves the value function $v_{\\pi'}(s)\\geq v_{\\pi}(s)$\n",
    "$$v_{\\pi}(s)\\leq q_{\\pi} (s, \\pi'(s))= \\mathbb{E}_{\\pi'}[R_{t+1}+ \\gamma v_{\\pi}(S_{t+1})|S_t=s] $$\n",
    "$$=> v_{\\pi}(s)\\leq \\mathbb{E}_{\\pi'}[R_{t+1}+ \\gamma q_{\\pi}(S_{t+1}, \\pi'(S_{t+1}))|S_t=s] $$\n",
    "$$=> v_{\\pi}(s)\\leq \\mathbb{E}_{\\pi'}[R_{t+1}+ \\gamma R_{t+2}+\\gamma^{2} q_{\\pi}(S_{t+2}, \\pi'(S_{t+2}))|S_t=s] $$\n",
    "$$=> v_{\\pi}(s)\\leq \\mathbb{E}_{\\pi'}[R_{t+1}+ \\gamma R_{t+2}+\\cdots|S_t=s] = v_{\\pi'(s)}$$\n",
    "5. If improvements stop\n",
    "   $$q_{\\pi} (s, \\pi'(s))= \\max_{a\\in A} q_{\\pi}(s,a) = q_{\\pi}(s, \\pi(s))=v_{\\pi}(s)$$\n",
    "6. Then the bellman optimality equation has been satishfied\n",
    " $$ v_{\\pi}(s)= \\max_{a\\in A} q_{\\pi}(s,a)$$\n",
    "7. Therefore, $\\mathcal{v_{\\pi}(S)=v_{*}(s) \\text{ for all } s \\in S}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3#Policy Iteration\n",
    "Once a policy $\\pi$ has been improved using $v_r$ to yeild better policy $\\pi'$ we can then compute $v_{\\pi'}$ and improve it again to yeild an even better $\\pi''$. Therefore, we can obtain a sequence of monotonically improving policies and vlau-function.\n",
    "$$\\pi_0 \\to^{E} v_{\\pi_0}\\to^{I} \\pi_1 \\to^{E} v_{\\pi_1}\\to^{I} \\pi_2 \\to^{E} \\cdots \\to^{I} \\pi^{*}\\to^{E} v_{*}$$\n",
    "\n",
    "1. Each policy is guranteed to be a strict improvement over the previous one unless it is already optimal. Because a finite `MDP` has only a finite number of policies, this process must converse to an optimal policy and value-function in a finite number of iteration.\n",
    "2. This process is called policy iteration.\n",
    "3. each policy evaluation, itself an iterative computation, is started with the value function for the previous policy.\n",
    "4. This typically results in a great increase in the speed of convergence of policy evaluation (presumably\n",
    "because the value function changes little from one policy to the next)\n",
    "#### Algorithm:\n",
    "\n",
    "#### Drawbacks:\n",
    "One drawback to policy iteration is that each of its iterations involves policy evaluation, which may itself be a protracted iterative computation requiring multiple sweeps through the state set. If policy evaluation is done iteratively, then convergence exactly to occurs only in the limit.\n",
    "\n",
    "##### ***Must we wait for exact convergence or can we stop short of that?***\n",
    "We can truncate policy evaluation. In policy evaluation iterations beyond the first three have no effect on the corresponding `greedy policy`.\n",
    "- Optimal values are unique, but optimal policies are not unique.\n",
    "- What if we ran the value evaluation algorithm and kept switching back and forth between 2 or more optimal policies?\n",
    "  - The loop will never terminate.\n",
    "  - Sol: we can quit when the policy is stable or when the value is stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4#Value Iteration\n",
    "The policy evaluation step of policy iteration can be truncated in several ways without losing the convergence guarantees of policy iteration. One important special case is when policy evaluation is stopped after just one sweep (one backup of each state). This algorithm is called `value iteration`. It can be written as a particularly simple backup operation that combines the policy improvement and truncated policy evaluation steps:\n",
    "- Iterative application of Bellman optimality backup into an update rule.\n",
    "$$\\mathcal {v_{k+1}(s)= \\max_{a\\in A}(R^{a}_{s}+ \\gamma\\sum_{s'\\in S}P^{a}_{ss'}v_k(s'))}$$\n",
    "$$\\mathcal{v_{k+1}= \\max_{a\\in A}(R^{a}+ \\gamma P^{a}v_k)}$$\n",
    "- $v_1\\to v_2\\to \\cdots \\to v_*$\n",
    "- Using synchronous backup\n",
    "  - At each iteration k+1\n",
    "  - For all states $s\\in S$\n",
    "  - Update $v_{k+1}(s)\\text{ from } v_k(s')$\n",
    "- Converge to $v_*$\n",
    "- Unlike policy iteration there is no explicit policy\n",
    "- Intermediate value functions may not correspond to any policy\n",
    "  -   $$\\pi'(s)=\\argmax_{a\\in A}q_{\\pi}(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4#Monte Carlo Methods\n",
    "We begin by considering Monte Carlo methods for learning the state-value function for a given policy. Monte carlo is technique to estimate value-function from the average of the return of experiences to a state.\n",
    "1. Monte Carlo Policy Evaluation\n",
    "2. Monte Carlo Estimation of action-values\n",
    "3. Monte Cralo Control\n",
    "4. Monte Cralo Control without Exploring Star\n",
    "5. Off-Policy Prediction vai importance sampling\n",
    "6. Incremental Implementation\n",
    "7. Off-Policy Monte carlo Control\n",
    "8. Discounting-aware Importance Sampling\n",
    "9. Per-Decision Importance Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1#Monte Carlo Policy Evaluation\n",
    "Suppose we want to estimate the $V_{\\pi} \\text{ or } q_{\\pi}$, the value of a state `s` under given policy $\\pi$, given a set of `episodes` obtained by following $\\pi$ and passing through `s`, without environment dynamics or Bellman equation. Each occurrence of `state s` in an episode is called a `visit to s`. The ***every-visit*** MC method estimates $V_{\\pi}(s)$ as the average of returns $\\mathcal G$ following all the visits to `S` in a set of episodes.\n",
    "\n",
    "$$v_{\\pi}(s)=\\mathbb{E}[G_t|S_t=s] \\approx \\frac{1}{N}\\sum^{N}_{i=1}G_{i,s} $$\n",
    "\n",
    "***First Visit:*** Within a episode, the first time `s` is visited is called the `First Visit to S`. The `first-visit MC` method average just the returns following first visit to s. \n",
    "***Every-Visit:***\n",
    "\n",
    "- Both first visit MC and every-visit MC converge to $V_{\\pi}(s)$\n",
    "- Estimate for each `state s` are independent. The estimate for one state does not build upon the estimate of any other state.\n",
    "- MC methods do not bootstrap.\n",
    "- No need for $\\mathcal P^{a}_{ss'} \\text{ and } R^{a}_{ss'}$\n",
    "- Transition Probability and expected reward must be computed before dp can be applied, and such computations are often complex and error-prone.\n",
    "- Monte Carlo methods to work with sample episodes alone can be a significant advantage even when one has complete knowledge of the environment's dynamics.\n",
    "- MC sampled on the one episodes, whereas DP includes one-step transition.\n",
    "- `MC method has the ability to learn from actual experience and from simulated experience` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the value of a state not vissited by our policy?\n",
    "1. Do not compute any values from those states because we can not visit those states.\n",
    "2. Manually put the agent into different starting states, for example, in grid-world, do not start from the same position on every episode,  instead we can choose starting position at random to ensure that every state will have corresponing sample returns. That does not violate the policy.\n",
    "3. If a policy is probabilistic with non-zero probability for every actions for evry states then this would be not a problems, given enough time we will get a great number of samples.\n",
    "\n",
    "#### What if we encounter the same state more than once?\n",
    "1. Solution number one is to consider the return only for the first time the state was visited.\n",
    "2. Solution number one is to consider the return only for the first time the state was visited. \n",
    "3. It turns out that you can prove theoretically that these will both converge to the true answer.\n",
    "\n",
    "##### does this problem create an infinite loops?\n",
    "So now let's consider the problem where our policy leads to an infinite cycle. For example, suppose in one state the policy is to go left, but then in the state to the left, the policy is to go right. Clearly, this will just lead to going left and right forever.\n",
    "\n",
    "- The greater issue here is what if we have an episode that never ends in this case, Montecarlo methods do not apply because by definition of the Montecarlo method, we can only compute the value once we know the return, but we only know the return after the episode is terminated. \n",
    "If the episode does not terminate, then the return cannot be computed and Montecarlo methods cannot be employed. \n",
    "- Practically speaking, when it comes to our environment, we will declare our episode complete when it reaches a certain number of steps. For example, we consider a 20 steps or 100 steps to be the end of an episode if we haven't yet reached the terminal state.\n",
    "-  So even if there is an infinite cycle, the episode will still terminate. For example, in other environments like Cardpool and Mountain CA, which are part of opening IJM, the episodes end after you reach two hundred steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2#Monte Carlo Estimation of action-values\n",
    "If a model is not available, then it is particularly useful to estimate action values rather than state values. With a model, state values alone are sufficient to determine a policy; one simply looks ahead one step and chooses whichever action leads to the best combination of reward and next state, as we did in the chapter on DP.\n",
    "- Without a model, however, `state values` alone are `not sufficient to determine a policy`.\n",
    "- Thus, one of our primary goals for Monte Carlo methods is to estimate $Q_{*}$. To achieve this, we first consider another policy evaluation problem.\n",
    "- The every-visit MC method estimates the value of a state-action pair as the average of the returns that have followed visits to the state in which the action was selected.\n",
    "- The first-visit MC method averages the returns following the first time in each episode that the state was visited and the action was selected.\n",
    "\n",
    "#### Problem-1: Many relevant state-action pairs may never be visited.\n",
    "- If is a deterministic policy $\\pi$, then in following $\\pi$ one will observe returns only for one of the actions from each state.\n",
    "- With no returns to average, `the Monte Carlo estimates of the other actions will not improve with experience`.\n",
    "- This is a serious problem because the purpose of learning action values is to help in choosing among the actions available in each state. \n",
    "- To compare alternatives we need to estimate the value of all the actions from each state, not just the one we currently favor.\n",
    "- `This is the general problem of maintaining exploration.`\n",
    "\n",
    "##### Solution:\n",
    "- For policy evaluation to work for action values, we must assure `continual exploration`.\n",
    "- the first step of each episode starts at a state-action pair, and that every such pair has a `nonzero probability` of being selected as the start.\n",
    "- This guarantees that `all state-action pairs` will be visited an `infinite number of times` in the `limit of an infinite number of episodes`.\n",
    "- This process is called **`Exploring Stars`**\n",
    "- `But, ` this approach is not helpfull when agent learns directly from the interactions with an environment. The most common alternative approach to assuring that all state-action pairs are encountered is to consider only policies that `are stochastic with\n",
    "a nonzero probability of selecting all actions`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3#Monte Cralo Control\n",
    "$$\\pi_{0}\\to^{E} q_{r_{0}}\\to^{I}\\pi_{1}\\to^{E} q_{r_{1}}\\to^{I} r_2\\to^{E}\\cdots\\to^{I}r_{*}\\to^{E}q_{*}$$\n",
    "- episodes have exploring starts\n",
    "- policy evaluation could be done with an infinite number of episodes.\n",
    "- $\\pi(s)=\\argmax_{a\\in A}Q(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4#Monte Cralo Control without Exploring Star\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5#Off-Policy Prediction vai importance sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6#Incremental Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7#Off-Policy Monte carlo Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8#Discounting-aware Importance Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9#Per-Decision Importance Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5#Temporal Difference(TD)\n",
    "TD learning is a combination of Monte Carlo ideas and dynamic programming (DP) ideas. Like Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environment's dynamics. Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a final outcome (they bootstrap). For example `what if episodes never end`?\n",
    "- MC:\n",
    "  - episodes must be terminated, so G can be computed.\n",
    "- DP: use bootstraping\n",
    "  - $V(s)$ is updated using the current esmites of $V(s')$\n",
    "\n",
    "1. TD prediction\n",
    "2. Advantage of TD Prediction Methods\n",
    "3. Optimality of TD(0)\n",
    "4. Sarsa: On-policy TD control\n",
    "5. Q-Learning off-policy TD control\n",
    "6. Expected Sarsa\n",
    "7. Maximizing Bias and Double Learning\n",
    "8. Games, Afterstates and other speacial cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1#TD prediction $\\mathcal{TD(0)}$\n",
    "$\\alpha MC:$ $v(s_t)\\to v(s_t)+\\alpha[r_t-v(s_t)]$\n",
    "\n",
    "`DP:`$v_{\\pi}(s)= \\mathbb{E}_{\\pi}[r_{t+1}+\\gamma v_{\\pi}(S_{t+1})| S_t=s]$<br>\n",
    "\n",
    "- Temporal difference learning is simply to combine these two things together instead of trying to compute the average of all the samples.\n",
    "- This means we do not have to wait until the episode is over to make an update.\n",
    "- This can be helpful in cases where episodes are very long or even infinite.\n",
    "- the agent can learn as it goes as long as it has one reward, it can perform.\n",
    "- This update also recognize the use of bootstrapping effectively.\n",
    "- The target value, which is R plus gamma times V prime depends on the value function estimate at the next state $s'$.\n",
    "  - the part that we know for sure `r`\n",
    "  - unknown the future reward, so we have to guess\n",
    "-  no need to play an entire episode to collect a list of states and rewards.\n",
    "\n",
    "$$\\mathcal TD=v(s_t)+\\alpha[r_{t+1}+\\gamma v(s_{t+1})-v(s_t)]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4# Sarsa $(s,a,r,s',a')$: On-policy TD control\n",
    "- The first step is to learn an action-value function\n",
    "- consider transitions from `state-action(s,a) pair to state-action pair(s,a)`, and learn the value of state-action pairs.\n",
    "$$q(s_t,a_t)\\to q(s_t, a_t)+\\alpha[r_{t+1}+\\gamma q(s_{t+1}, a_{t+1})-q(s_t,a_t)]$$\n",
    "- epsilon -greedy or epsilon-soft policies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5#Q-Learning off-policy TD control\n",
    "$$q(s_t,a_t)\\to q(s_t, a_t)+\\alpha[r_{t+1}+\\gamma \\max_{a\\in A}q(s_{t+1}, a_{t+1})-q(s_t,a_t)]$$\n",
    "- Absylon Greedy is not an optimal policy. It helps us with expolaration but it also means that some percentage of the time we're just going to choose a suboptimal action randomly.\n",
    "- Q Learning gives us one way of avoiding this.\n",
    "- instead of using the actual next action in the target, we use the action we would have taken if we had chosen the current optimal action.\n",
    "- This means that the target will correspond to that suboptimal action with Q Learning you'll always use the maximum.\n",
    "- SARSA is called an `on-policy` method because the Q function we're learning is the Q function for the policy that we're actually using in the environment.\n",
    "  - once we complete training, this will be the policy that we consider to be the best policy for the agents\n",
    "- Q Learning is an off policy method.\n",
    "  - actions are dictated by an epsilon-gready policy but the Q function we are learning is for a purely greedy policy.\n",
    "  - `the max of a Q` We can differentiate between the two kinds of policies as follows\n",
    "    - The policy that we use to play the episode is called the `Behavior Policy`.The behavior policy dictates how we act in the environment. Behavior policy can be completely random, that is uniform, random, and you can still end up with an optimal target policy.\n",
    "\n",
    "    - the policy that we are learning is called the `target policy`.The target policy may not be the same as the one we are using to determine our actions during training. The target policy enables an agent how to maximize its rward.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6#n-step Bootsatrapping\n",
    "1. n-step TD Prediction\n",
    "2. n-step Sarsa\n",
    "3. n-steap off-policy learning\n",
    "4. per-decision Methods with control variates\n",
    "5. Off-policy learning without importance: the n-step Tree Backup Algorithm\n",
    "6. A-unifying Algorithm: n-step Q()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
