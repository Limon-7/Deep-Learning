{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Recurrent neural networks\n",
    "\n",
    "### 2. language modeling\n",
    "\n",
    "### 3. Image captioning\n",
    "\n",
    "### 4. soft attention\n",
    "\n",
    "### 5. LSTM\n",
    "\n",
    "### 6. GRU\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Recurrent neural networks:\n",
    "\n",
    "Recurrent neural networks, also known as RNNs, are a class of neural networks that allow previous outputs to be used as inputs while having hidden states.\n",
    "\n",
    "$$ h*t=fw(h*{t-1},x_t)$$\n",
    "**_Note:_** the `same function and same set of parameters` are used at every time step.\n",
    "\n",
    "##### 1.1 (Vanialla) Recurrent Neural Network:\n",
    "\n",
    "The network is just a single hidden state `h` where we use a recurrence formula that basically tells us how we should update our hidden state `h` as a function of previous hidden state $h_{t−1}$ and the current input $x_t$.\n",
    "\n",
    "$ h*t=fw(h_{t-1},x_t)$ <br>\n",
    "$ h_t=tanh(W_{hh}*h_{t-1}+W_{xh}*x_t) + b_h$<br>\n",
    "$y_t=W_{hy}*h_t + b_y$\n",
    "\n",
    "#### 1.2 RNN: Computational Graph:\n",
    "\n",
    "1. `h0` are initialized to zero.\n",
    "2. Gradient of W is the sum of all the `W` gradients that has been calculated!\n",
    "\n",
    "`1. One-to-Many:`\n",
    "\n",
    "![one-to-many](../images/one-to-many.png)\n",
    "\n",
    "`2. Many-to-one:`\n",
    "\n",
    "![many-to-one](../images/Many-to-one.png)\n",
    "\n",
    "`3. Many-to-Many:`\n",
    "\n",
    "![many-to-many](../images/many-to-many.png)\n",
    "\n",
    "`4. Sequence-to-Sequence:`\n",
    "\n",
    "![sequence-to-sequence](../images/sequence-to-sequence.png)\n",
    "\n",
    "1. Why We need RRN?\n",
    "\n",
    "Vanilla Neural Networks \"Feed neural networks\", input of fixed size goes through some hidden units and then go to output. We call it a one to one network. So the traditional Neural Network only **_depend on imput data and can not work with sequential data so this kind of network can not memorize previous data._**\n",
    "`Recurrent neural networks allow us to operate a sequences or variable length of input , output or both at a same time.`\n",
    "\n",
    "2. **_The pros and cons of a typical RNN architecture are summed up in the table below:_**\n",
    "\n",
    "| Advantages                                            |                                               drawbacks |\n",
    "| :---------------------------------------------------- | ------------------------------------------------------: |\n",
    "| Possibility of processing input of any length         |                                 Computation being slow. |\n",
    "| Model size not increasing with size of input          |   Difficulty of being getting information long time ago |\n",
    "| Computation takes into account historical information | can not consider any future input for the current state |\n",
    "| Weights are shared across time                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application and Types of RNN:\n",
    "\n",
    "1. **_Image Captioning:_** Image captioning is the process of `generating a textual description` or caption that accurately describes the content of an image. It combines computer vision techniques for understanding the visual content of an image. Generally use **`one-to-many`** model.\n",
    "\n",
    "2. **_Sentiment Classification:_** Sentiment classification is the task of automatically determining the sentiment expressed in a given piece of text. The goal is to classify the text into different sentiment categories, such as positive, negative or neutral. Another example, taking input as a sequence of video frames and produce a label what action was happening in that video. Generally use **_`many-to-one`_** model.\n",
    "\n",
    "3. **_Machine Translation:_** takes a sequence of words of a sentence in English, and then this RNN is asked to produce a sequence of words of a sentence in French. Another example, taking input as a sequence of video frames and produce a label for each label what action was happening in that frames. Generally use **_`many-to-many`_** model.\n",
    "\n",
    "![types-of-rnn](../images/RNN.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.76159416]\n",
      " [0.29131261]\n",
      " [0.09966799]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.76159416]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "h = [[1], [0.3], [0.1]] + [[0], [0], [1], [0]]\n",
    "print(np.tanh(h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Character Level Language Model:\n",
    "\n",
    "The main task of the character-level language model is to `predict the next character` given all previous characters in a sequence of data, i.e. `generates text character by character`. More formally, given a training sequence $(x_1, … , x_T)$, the RNN uses the sequence of its output vectors $(o_1, … , o_T)$ to obtain a sequence of predictive distributions $P(x^t/x^{t-1}) = softmax(o^t)$.\n",
    "\n",
    "**_`Algorithm:`_**\n",
    "\n",
    "1. We first build a vocabulary dictionary using all the unique letters of the names in the corpus as keys and the index of each letter starting from zero (since Python is a zero-indexed language) in an ascending order.\n",
    "2. Convert the input characters into one-hot vectors.\n",
    "3. Compute the hidden state layer.\n",
    "4. Compute the output layer and then pass it through softmax to get the results as probabilities.\n",
    "5. Feed the target character at time step (t) as the input character at time step (t + 1).\n",
    "6. This process is repeated untill all the letters are finished.\n",
    "\n",
    "**_`Terminologies:`_**\n",
    "\n",
    "1. **_Statistical Language Model:_** The general way of generating a sequence of text is to train a model to predict the next word/character given all previous words/characters. Such model is called a Statistical Language Model.\n",
    "\n",
    "2. **_`Sampling:`_** Sampling is what makes the text generated by the RNN at each time step an interesting/creative text. On each time step (t), the RNN output the `conditional probability distribution` of the next character given all the previous characters, ex.. $P(c_t/c_1, c_2, …, c_{t-1})$. If time step `t = 3` and we’re trying to predict the third character, the conditional probability distribution is: $P(c_3/c_1, c_2) = (0.2, 0.3, 0.4, 0.1)$. We have two extremes-\n",
    "\n",
    "   1. **_`Maximum Entropy:`_** The character will be picked randomly using `uniform probability distribution`; which means that all characters in the vocabulary dictionary are equally likely. Therefore, we’ll end up with maximum randomness in picking the next character and the generated text will not be either meaningful or sound real.\n",
    "\n",
    "   2. **_`Minimum entropy:`_** The character with the highest conditional probability will be picked on each time step. That means next character will be what the model estimates to be the right one based on the `training text and learned parameters`. As a result, the names generated will be both meaningful and sound real. However, it will also be repetitive and not as interesting since all the parameters were optimized to learn joint probability distribution in predicting the next character. **we increase randomness, text will lose local structure; however, as we decrease randomness, the generated text will sound more real and start to preserve its local structure.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Traing language Model:\n",
    "1. ***`Forward Pass:`***\n",
    "\n",
    "![](../images/character-language-model-training.png)\n",
    "\n",
    "1. Only the third prediction here is true. The loss needs to be optimized.\n",
    "2. We can train the network by feeding the whole words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Test language Model:\n",
    "1. ***`BackwardPass:`***\n",
    "\n",
    "![](../images/character-language-model-test.png)\n",
    "\n",
    "1. At test time we work with a character by character. The output character will be the next input with the other saved hidden activations.\n",
    "\n",
    "***Question: Why might we sample instead of taking the character with the largest score?***\n",
    "A sampling distribution refers to the probability distribution of a particular statistic that is calculated from multiple samples drawn from the same population. It provides insights into the behavior and properties of the statistic when repeatedly sampled from the population.\n",
    "\n",
    "1. `Promting diversity:`We introduce randomness and allow the model to explore different possibilities. This helps in generating diverse and varied outputs.\n",
    "2. `Avoiding local Optima:` Sampling enables the model to consider lower-scoring options, which might still be valid and interesting. This exploration can help the model escape local optima and generate more novel and diverse sequences.\n",
    "3. `Handling uncertainty:` In cases where the model is uncertain about the best choice, sampling allows for expressing that uncertainty. Rather than committing to a single option based on the highest score, sampling incorporates randomness and provides a range of potential outputs. This can be useful when the model encounters ambiguous or uncertain situations, allowing it to explore different interpretations.\n",
    "4. `Generating more natural outputs:` By sampling, the model can capture these natural variations, resulting in outputs that are perceived as more realistic and authentic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanila RNN problem:\n",
    "1. ***`Vanishing Gradient Problem:`***\n",
    "2. ***`Exploding Gradient Problem:`***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
