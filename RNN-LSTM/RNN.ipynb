{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Recurrent neural networks\n",
    "\n",
    "### 2. language modeling\n",
    "\n",
    "### 3. Image captioning\n",
    "\n",
    "### 4. soft attention\n",
    "\n",
    "### 5. LSTM\n",
    "\n",
    "### 6. GRU\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Recurrent neural networks:\n",
    "\n",
    "Recurrent neural networks, also known as RNNs, are a class of neural networks that allow previous outputs to be used as inputs while having hidden states.\n",
    "\n",
    "$$ h_t=fw(h_{t-1},x_t)$$\n",
    "**_Note:_** the `same function and same set of parameters` are used at every time step.\n",
    "\n",
    "##### 1.1 (Vanialla) Recurrent Neural Network:\n",
    "\n",
    "The network is just a single hidden state `h` where we use a recurrence formula that basically tells us how we should update our hidden state `h` as a function of previous hidden state $h_{t−1}$ and the current input $x_t$.\n",
    "\n",
    "![rnn-model](../images/rnn-model.png)\n",
    "\n",
    "$ h_t=fw(h_{t-1},x_t)$ <br>\n",
    "$ h_t=tanh(W_{hh}*h_{t-1}+W_{xh}*x_t) + b_h$<br>\n",
    "$y_t=W_{hy}*h_t + b_y$\n",
    "\n",
    "here,\n",
    "1. $h_t$: hidden state that is passed through time from $h_{t-1}$ to $h_t$\n",
    "2. $x_t$: raw input into the network\n",
    "3. $y_t$: output at each time step\n",
    "4. $W_x$: input weights\n",
    "5. $W_h$: hidden weights\n",
    "6. $W_y$: weights of the output between the hidden and input\n",
    "\n",
    "#### 1.2 RNN: Computational Graph:\n",
    "\n",
    "1. `h0` are initialized to zero.\n",
    "2. Gradient of W is the sum of all the `W` gradients that has been calculated!\n",
    "\n",
    "`1. One-to-Many:`\n",
    "\n",
    "![one-to-many](../images/one-to-many.png)\n",
    "\n",
    "`2. Many-to-one:`\n",
    "\n",
    "![many-to-one](../images/Many-to-one.png)\n",
    "\n",
    "`3. Many-to-Many:`\n",
    "\n",
    "![many-to-many](../images/many-to-many.png)\n",
    "\n",
    "`4. Sequence-to-Sequence:`\n",
    "\n",
    "![sequence-to-sequence](../images/sequence-to-sequence.png)\n",
    "\n",
    "1. Why We need RRN?\n",
    "\n",
    "Vanilla Neural Networks \"Feed neural networks\", input of fixed size goes through some hidden units and then go to output. We call it a one to one network. So the traditional Neural Network only **_depend on imput data and can not work with sequential data so this kind of network can not memorize previous data._**\n",
    "`Recurrent neural networks allow us to operate a sequences or variable length of input , output or both at a same time.`\n",
    "\n",
    "2. **_The pros and cons of a typical RNN architecture are summed up in the table below:_**\n",
    "\n",
    "| Advantages                                            |                                               drawbacks |\n",
    "| :---------------------------------------------------- | ------------------------------------------------------: |\n",
    "| Possibility of processing input of any length         |                                 Computation being slow. |\n",
    "| Model size not increasing with size of input          |   Difficulty of being getting information long time ago |\n",
    "| Computation takes into account historical information | can not consider any future input for the current state |\n",
    "| Weights are shared across time                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application and Types of RNN:\n",
    "\n",
    "1. **_Image Captioning:_** Image captioning is the process of `generating a textual description` or caption that accurately describes the content of an image. It combines computer vision techniques for understanding the visual content of an image. Generally use **`one-to-many`** model.\n",
    "\n",
    "2. **_Sentiment Classification:_** Sentiment classification is the task of automatically determining the sentiment expressed in a given piece of text. The goal is to classify the text into different sentiment categories, such as positive, negative or neutral. Another example, taking input as a sequence of video frames and produce a label what action was happening in that video. Generally use **_`many-to-one`_** model.\n",
    "\n",
    "3. **_Machine Translation:_** takes a sequence of words of a sentence in English, and then this RNN is asked to produce a sequence of words of a sentence in French. Another example, taking input as a sequence of video frames and produce a label for each label what action was happening in that frames. Generally use **_`many-to-many`_** model.\n",
    "\n",
    "![types-of-rnn](../images/RNN.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.76159416]\n",
      " [0.29131261]\n",
      " [0.09966799]\n",
      " [0.        ]\n",
      " [0.        ]\n",
      " [0.76159416]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "h = [[1], [0.3], [0.1]] + [[0], [0], [1], [0]]\n",
    "print(np.tanh(h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Character Level Language Model:\n",
    "\n",
    "The main task of the character-level language model is to `predict the next character` given all previous characters in a sequence of data, i.e. `generates text character by character`. More formally, given a training sequence $(x_1, … , x_T)$, the RNN uses the sequence of its output vectors $(o_1, … , o_T)$ to obtain a sequence of predictive distributions $P(x^t/x^{t-1}) = softmax(o^t)$.\n",
    "\n",
    "**_`Algorithm:`_**\n",
    "\n",
    "1. We first build a vocabulary dictionary using all the unique letters of the names in the corpus as keys and the index of each letter starting from zero (since Python is a zero-indexed language) in an ascending order.\n",
    "2. Convert the input characters into one-hot vectors.\n",
    "3. Compute the hidden state layer.\n",
    "4. Compute the output layer and then pass it through softmax to get the results as probabilities.\n",
    "5. Feed the target character at time step (t) as the input character at time step (t + 1).\n",
    "6. This process is repeated untill all the letters are finished.\n",
    "\n",
    "**_`Terminologies:`_**\n",
    "\n",
    "1. **_Statistical Language Model:_** The general way of generating a sequence of text is to train a model to predict the next word/character given all previous words/characters. Such model is called a Statistical Language Model.\n",
    "\n",
    "2. **_`Sampling:`_** Sampling is what makes the text generated by the RNN at each time step an interesting/creative text. On each time step (t), the RNN output the `conditional probability distribution` of the next character given all the previous characters, ex.. $P(c_t/c_1, c_2, …, c_{t-1})$. If time step `t = 3` and we’re trying to predict the third character, the conditional probability distribution is: $P(c_3/c_1, c_2) = (0.2, 0.3, 0.4, 0.1)$. We have two extremes-\n",
    "\n",
    "   1. **_`Maximum Entropy:`_** The character will be picked randomly using `uniform probability distribution`; which means that all characters in the vocabulary dictionary are equally likely. Therefore, we’ll end up with maximum randomness in picking the next character and the generated text will not be either meaningful or sound real.\n",
    "\n",
    "   2. **_`Minimum entropy:`_** The character with the highest conditional probability will be picked on each time step. That means next character will be what the model estimates to be the right one based on the `training text and learned parameters`. As a result, the names generated will be both meaningful and sound real. However, it will also be repetitive and not as interesting since all the parameters were optimized to learn joint probability distribution in predicting the next character. **we increase randomness, text will lose local structure; however, as we decrease randomness, the generated text will sound more real and start to preserve its local structure.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Traing language Model:\n",
    "1. ***`Forward Pass:`***\n",
    "\n",
    "![](../images/character-language-model-training.png)\n",
    "\n",
    "1. Only the third prediction here is true. The loss needs to be optimized.\n",
    "2. We can train the network by feeding the whole words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Test language Model:\n",
    "1. ***`BackwardPass:`***\n",
    "\n",
    "![](../images/character-language-model-test.png)\n",
    "\n",
    "1. At test time we work with a character by character. The output character will be the next input with the other saved hidden activations.\n",
    "\n",
    "***Question: Why might we sample instead of taking the character with the largest score?***\n",
    "A sampling distribution refers to the probability distribution of a particular statistic that is calculated from multiple samples drawn from the same population. It provides insights into the behavior and properties of the statistic when repeatedly sampled from the population.\n",
    "\n",
    "1. `Promting diversity:`We introduce randomness and allow the model to explore different possibilities. This helps in generating diverse and varied outputs.\n",
    "2. `Avoiding local Optima:` Sampling enables the model to consider lower-scoring options, which might still be valid and interesting. This exploration can help the model escape local optima and generate more novel and diverse sequences.\n",
    "3. `Handling uncertainty:` In cases where the model is uncertain about the best choice, sampling allows for expressing that uncertainty. Rather than committing to a single option based on the highest score, sampling incorporates randomness and provides a range of potential outputs. This can be useful when the model encounters ambiguous or uncertain situations, allowing it to explore different interpretations.\n",
    "4. `Generating more natural outputs:` By sampling, the model can capture these natural variations, resulting in outputs that are perceived as more realistic and authentic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Gradient through backpropagation in RNN:\n",
    "Backpropagation through time Forward through entire sequence to compute loss, then backward through entire sequence to compute gradient. But if we choose the whole sequence it will be so slow and take so much memory and will never converge.\n",
    "\n",
    "![bptt](../images/backpropagation-through-time.png)\n",
    "\n",
    "So `in practice` people are doing \"Truncated Backpropagation through time(TBTT)\" as we go on we Run forward and backward through chunks of the sequence instead of whole sequence. Then Carry hidden states forward in time forever, but only backpropagate for some smaller number of steps.\n",
    "\n",
    "![TBTT](../images/truncate-backpropagation-through-time-2.png)\n",
    "\n",
    "Truncated Backpropagation Through Time modifies BPTT by considering two main parameters: k1 and k2.\n",
    "1. `k1:` the number of timesteps for the forward pass between weight updates, not including the backward pass. This parameter influences the training time.\n",
    "2. `k2:` the number of timesteps for the backward pass. This parameter can be adjusted smaller to address vanishing gradients so that the gradients don’t need to span as far back. However, it should be large enough so that it can capture the temporal structure of the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanila RNN problem:\n",
    "An RNN block takes in input xt and previous hidden representation ht−1 and learn a transformation, which is then passed through tanh to produce the hidden representation ht for the next time step and output yt.\n",
    "$$h_t = tanh(W_{hh}h_{t-1} + W_{xh}x_t)$$\n",
    "For the back propagation, Let’s examine how the output at the very last timestep affects the weights at the very first time step. The partial derivative of $h_t$ with respect to h_{t−1} is written as:\n",
    "\n",
    "$$\\frac{\\partial h_t}{\\partial h_{t-1}} =  tanh^{'}(W_{hh}h_{t-1} + W_{xh}x_t)W_{hh}$$\n",
    "We update the weights $W_{hh}$ by getting the derivative of the loss at the very last time step $L_t$ with respect to $W_{hh}$\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial L_{t}}{\\partial W_{hh}} = \\frac{\\partial L_{t}}{\\partial h_{t}} \\frac{\\partial h_{t}}{\\partial h_{t-1} } \\dots \\frac{\\partial h_{1}}{\\partial W_{hh}} \\\\\n",
    "= \\frac{\\partial L_{t}}{\\partial h_{t}}(\\prod_{t=2}^{T} \\frac{\\partial h_{t}}{\\partial  h_{t-1}})\\frac{\\partial h_{1}}{\\partial W_{hh}} \\\\\n",
    "= \\frac{\\partial L_{t}}{\\partial h_{t}}(\\prod_{t=2}^{T}  tanh^{'}(W_{hh}h_{t-1} + W_{xh}x_t)W_{hh}^{T-1})\\frac{\\partial h_{1}}{\\partial W_{hh}} \\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "$tanh^{'}(W_{hh}h_{t-1} + W_{xh}x_t)$ will almost always be less than 1 because tanh is always between negative one and one. Thus, as `t` gets larger (i.e. longer timesteps), the gradient $(\\frac{\\partial L_{t}}{\\partial W})$ will descrease in value and get `close to zero`. This will lead to vanishing gradient problem, where gradients at future time steps rarely impact gradients at the very first time step. This is problematic when we model long sequence of inputs because the updates will be extremely slow.\n",
    "\n",
    " If we remove non-linearity (tanh) to solve the vanishing gradient problem, then we will be left with\n",
    " $$\\begin{aligned}\n",
    "\\frac{\\partial L_{t}}{\\partial W} = \\frac{\\partial L_{t}}{\\partial h_{t}}(\\prod_{t=2}^{T} W_{hh}^{T-1})\\frac{\\partial h_{1}}{\\partial W} \n",
    "\\end{aligned}$$\n",
    "1. ***`Vanishing Gradient Problem:`*** If the laregest singular value of $W_{hh}$ is smaller than 1, then we will have vanishing gradient problem as mentioned above which will significantly `slow down learning`.\n",
    "2. ***`Exploding Gradient Problem:`*** If the largest singular value of $W_{hh}$ is greater than 1, then the gradients will blow up and the model will get very large gradients coming back from future time steps. Exploding gradient often leads to getting gradients that are NaNs.\n",
    "\n",
    "we can treat the exploding gradient problem through ***gradient clipping***, which is clipping large gradient values to a maximum threshold. However, since vanishing gradient problem still exists in cases where largest singular value of $W_{hh}$ matrix is less than one, LSTM was designed to avoid this problem.\n",
    "\n",
    "![valina-rnn-problem-back-prob](../images/valina_grad_problem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
