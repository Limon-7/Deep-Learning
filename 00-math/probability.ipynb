{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability For Deep Learning\n",
    "<a id='header_cell'></a>\n",
    "Serial|Topic|Status|\n",
    ":--|--:|--:|\n",
    "01|[Event](#event_cell)||\n",
    "02|[Probability](#probability_cell)||\n",
    "03|[Bayes Theorem](#bayes_cell)||\n",
    "04|[Probability Distribution Function](#p_dfc_cell)|\n",
    "05|[Expected value](#ev_cell)||\n",
    "06| [KL Divergence](#kl_cell)||\n",
    "07| [Data Likelihood](#d_likelihood_cell)||"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1. Event](#header_cell):\n",
    "<a id='event_cell'></a>\n",
    "There are three type of events.\n",
    "1. Independent event: Independent events are those events whose occurrence is not dependent on any other event.A and B are two events associated with the same random experiment, then A and B are known as independent events if $P(A ∩ B) = P(B) .P(A) = P(A\\mid B)=\\frac{P(A\\cap B)}{P(B)}$<br>\n",
    "    * For example, if two coins are flipped, then the chance of both being heads is <br>$\\tfrac{1}{2}\\times\\tfrac{1}{2} = \\tfrac{1}{4}$.<br>\n",
    "    * A random card is selected from a statndard 52-card deck. What is probability the card is `a king`, given that the card is `read`?<br>\n",
    "        Sample Space: 52\n",
    "        P(K): card is a King = 4/52=1/13<br>\n",
    "        P(R): card is read =26/52<br>\n",
    "        intuition: card is a King and card is read = 2/26=1/13 because red abd king =2 and sample space will be = 26 <br>\n",
    "        $P(K\\mid R)=\\frac {P(K\\cap R)}{P(R)}=\\frac{\\frac 2 52}{\\frac 26 52}= 1/13$<br>\n",
    "        ***`Note: here the probabilty of P(K) and P(K|R) are same. K and R events are independent if `$P(K\\mid R) = P(K)$***\n",
    "\n",
    "2. Mutually Exclusive Event: Two events A and B are said to be mutually exclusive events if they cannot occur at the same time. Mutually exclusive events never have an outcome in common. $P(A ∩ B) = 0$\n",
    "    - For example, To find the probability of drawing a red card(heart or diamond) or a club? $26/52 + 13/52 = 39/52 or 3/4.$\n",
    "    - The probability of drawing a red and a club in two drawings without replacement is $ 26/52 × 13/51 × 2 =13/51$\n",
    "\n",
    "3. Dependent Event: Dependent events are those which depend upon what happened before.\n",
    "When two events, A and B, are dependent, the probability of occurrence of A and B is:\n",
    "$P(A and B)=P(A\\cap B) = P(A) · P(B|A)$\n",
    "    - One card is drawn at random from a 52 card deck. What is the probability that the card drawn is  red and a king? $26/52 * 25/51 * 2$\n",
    "    - One card is drawn at random from a 52 card deck. What is the probability that the card drawn is either red or a king? $26/52 + 4/52 – 2/52 = 28/52 = 7/13$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [2. Probability:](#header_cell)\n",
    "<a id='probability_cell'></a>\n",
    "Probability is the branch of mathematics concerning numerical descriptions of how likely an event is to occur, or how likely it is that a proposition is true. The probability of an event is a number between 0 and 1, where, roughly speaking, 0 indicates impossibility of the event and 1 indicates certainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ***$P(x)$:*** the probabilty of an event x.\n",
    "\n",
    "2. ***${\\displaystyle P(A\\cap B)}$(intersection or joint probability):*** If two **independent events A and B** occur on a single performance of an experiment, this is called the intersection or joint probability of A and B, denoted as  ${\\displaystyle P(A\\cap B).}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ***Conditional probability:*** Conditional probability is the probability of some event A, given the occurrence of some other event B. Conditional probability is written \n",
    "$P(A\\mid B)$, and is read \"the probability of A, given B\". It is define-\n",
    "${\\displaystyle P(A\\mid B)={\\frac {P(A\\cap B)}{P(B)}}\\,}$\n",
    "    - What is the probability `at least one of the two conins flips is tails`, given that the `first flip is heads`?<br>\n",
    "        Sample Space:`HH TT HT TH`<br>\n",
    "        P(E): at least one of the two conins flips is tails = 3/4<br>\n",
    "        P(F): first flip is heads = 1/2<br>\n",
    "        intuition: 1/2<br>\n",
    "        Using Formula: $P(E\\mid F)=\\frac {P(E\\cap F)}{P(F)}=\\frac{\\frac 1 4}{\\frac 1 2}= 1/2$\n",
    "    - A random card is selected from a statndard 52-card deck. What is probability the card is `a king`, given that the card is `read`?<br>\n",
    "        Sample Space: 52\n",
    "        P(K): card is a King = 4/52=1/13<br>\n",
    "        P(R): card is read =26/52<br>\n",
    "        intuition: card is a King and card is read = 2/26=1/13 because red abd king =2 and sample space will be = 26 <br>\n",
    "        $P(K\\mid R)=\\frac {P(K\\cap R)}{P(R)}=\\frac{\\frac 2 52}{\\frac 26 52}= 1/13$<br>\n",
    "        ***`Note: here the probabilty of P(K) and P(K|R) are same. K and R events are independent if `$P(K\\mid R) = P(K)$***\n",
    "\n",
    "    - For Exam1 and Exam2, `40% of students pass Exam1` and `10% of students pass the both exams`. What percent of students pass Exam1 also pass Exam2?<br>\n",
    "        $P(E1)$: 40%<br>\n",
    "        $P(E2\\cap E1)$: 10%<br>\n",
    "        $P(E2\\mid E1)=\\frac {P(E2\\cap E1)}{P(E1)}=\\frac {10\\%} {40\\%}=\\frac 1 4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ***Law of Total Probabilty***: We have a sample space that can be partitioned into three events $B1, B2, B3$. And say we have another event in this sample space called $A$. Then, whenever $A$ occurs, it must be true that $B1$ also occurs, or $B2$ also occurs, or $B3$ also occurs, by definition of a partition. It also follows from the definition of partition that $B1, B2, B3$ are ***`disjoint`***, no more than one of them can occur at the same time.\n",
    "\n",
    "    Therefore, we can find the probability of A by adding the probability that A and B1 occur, the probability that A and B2 occur, and the probability that A and B3 occur. That is, P( A ) = P( A intersect B1 ) + P( A intersect B2 ) + P( A intersect B3 ). This is the law of total probability for a partition into three sets, but it is also true for any partition into a finite number of sets, or a countably infinite number of sets!<br>\n",
    "    $P(A)= P(A\\cap B1)+P(A\\cap B2)+ P(A\\cap B3)= P(A\\mid B1)\\cdot P(B1)+ P(A\\mid B2)\\cdot P(B2)+ P(A\\mid B3)\\cdot P(B3)$\n",
    "    - Bag A has 2 red balls and 6 green balls, Bag B has 3 red balls and 2 green balls, Bag C has 1 red balls and 4 green balls? A ball is randomly seleced, what is the probability that the ball is read?<br>\n",
    "    Intuition: 6/15=2/5\n",
    "    Total Bag: 3\n",
    "    $P(R)= P(R\\cap A)+P(R\\cap A)+ P(R\\cap A)= P(R\\mid A)\\cdot P(A)+ P(R\\mid B)\\cdot P(B)+ P(R\\mid C)\\cdot P(C)={\\frac {2} {5}} \\cdot {\\frac {1}{3}}+ {\\frac {3} {5}} \\cdot {\\frac {1}{3}}+{\\frac {1} {5}} \\cdot {\\frac {1}{3}}= \\frac{2}{5}$<br>\n",
    "\n",
    "    If $\\{B_1, B_2, B_3,...B_n\\}$ is a finite sample space and A is an event in the same sample space, then\n",
    "    $$ P(A) =\\sum_{i}^{n}P(A\\cap B_i)= \\sum_{i}^{n}P(A\\mid B_i)\\cdot P(B_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary of probability:\n",
    "\n",
    "Event| Probability|\n",
    ":--|--:|\n",
    "A|${\\displaystyle P(A)\\in [0,1]}$|\n",
    "not A|${\\displaystyle P(A^{\\complement })=1-P(A)\\,}$|\n",
    "A or B| ${\\displaystyle P\\left(A{\\hbox{ or }}B\\right)=P(A\\cup B)=P\\left(A\\right)+P\\left(B\\right)-P\\left(A \\cap B\\right).}$ <br> $ P(A\\cup B) = P(A) + P(B)$ if A and B are independent|\n",
    "A and B|$P(A \\cap B) = P(A\\mid B)P(B) = P(B\\mid A)P(A)$ <br> $P(A \\cap B) = P(A)P(B)$ if A and B are independent|\n",
    "A given B|$P(A \\mid B) = \\frac {P(A\\cap B)}{P(B)}=\\frac {P(B\\mid A)P(A)}{P(B)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [3. Bayes Theorem:](#header_cell)\n",
    "<a id='bayes_cell'></a>\n",
    "Bayes' Theorem tells us how to calculate the probability of event A happening after we've observed event B, by considering the probability of observing event B given that event A has happened, and then adjusting that probability based on the prior probability of event A.\n",
    "\n",
    "$P(A\\mid B) = \\frac{P(B\\mid A)P(A)}{P(B)}$ where, A, B are events and $P(B)\\not = 0$\n",
    "- ${\\displaystyle P(A\\vert B)}$ is a conditional probability: the probability of event the probability of event \n",
    "A occurring given that B is true. It is also called the ***posterior*** probability of A given B.\n",
    "- ${\\displaystyle P(B\\vert A)}$ is also a conditional probability: the probability of event B occuring give that A is true. It can also be interpreted as the likelihood of A given a fixed B because ${\\displaystyle P(B\\vert A)=L(A\\vert B)}$.\n",
    "- P(A) and P(B) are the probabilities of observing A and B respectively without any given conditions; P(A) is known as the **prior probability** and P(B) is called **marginal probability.**\n",
    "\n",
    "If $A_1, A_2, A_3...A_n$ is a partion of sample space then the bayes theoream will be following,\n",
    "$$P(A_i\\mid B) = \\frac{P(B\\mid A_i)P(A_i)}{P(\\sum_{i}^{n}P(B\\mid A_i)\\cdot P(A_i))}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that a laboratory test on a blood sample yields one of two results, `positive or negative`. It is found that `95% of people with a particular disease` produce a positive result. But `2% of people without the disease` will also `produce a positive` (a false positive). Suppose `1% of the population actually has the disease`. What is the probability that a person choosen random from the poppulation will have the disease, given that the person's blood yields a `positive result`?<br>\n",
    "$P(pos\\mid D)=95\\% =.95$<br>\n",
    "$P(pos\\mid WD)=2\\%=.02$<br>\n",
    "$P(D)=1\\%=.01$<br>\n",
    "$P(D\\mid pos)= \\frac {P(pos\\mid D)\\cdot P(D)}{P(pos)}=\\frac{.95 \\cdot .01}{P(pos\\mid D)\\cdot P(D)+ P(pos\\mid WD)\\cdot P(1-D)}= \\frac {.95\\cdot.01}{.95\\cdot .01 + .02\\cdot (1-.01)}=0.32$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4. Probability Distribution Function:](#header_cell)\n",
    "<a id='p_dfc_cell'></a>\n",
    "In probability theory and statistics, a probability distribution is the mathematical function that gives the probabilities of occurrence of different possible outcomes for an experiment.\n",
    "$$F_X(x)=P(X≤x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ***`The Probability Mass Function (PMF)`***: The Probability Mass Function (PMF) is a concept in probability and statistics that describes the probability distribution of a discrete random variable. It gives the probability that a discrete random variable X takes on a specific value x.\n",
    " * $P(X=x)≥0$: The probability of any specific value is always  non-negative.\n",
    " * $\\sum_x P(X=x)=1$: The sum of probabilities for all possible values of X is equal to 1.\n",
    "\n",
    "    Consider a random variable X representing the outcome of rolling a fair six-sided die. The possible values of X are 1, 2, 3, 4, 5, and 6.\n",
    "    $$P(X=x)=\\frac 1 6$$\n",
    "    The PMF of X is:\n",
    "    ${ P_{X=x}={\\begin{cases} \n",
    "    {\\frac{1}{6}} &: for\\; x=1\\\\\n",
    "    {\\frac{1}{6}} &: for\\; x=2\\\\\n",
    "    {\\frac{1}{6}} &: for\\; x=3\\\\\n",
    "    {\\frac{1}{6}} &: for\\; 1x=4\\\\\n",
    "    {\\frac{1}{6}} &: for\\; x=5\\\\\n",
    "    {\\frac{1}{6}} &: for\\; x=6\\\\\n",
    "    {0} &: for\\;other\\; value\\; of\\; x\\\\\n",
    "    \\end{cases}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ***`Cumulative distribution function (CDF):`***\n",
    "    It provides the probability that a random variable X is less than or equal to a certain value x. The CDF is denoted as F(x), where x is the value at which you want to evaluate the cumulative probability.\n",
    "    $$F_{X}(x)=P(X≤x)$$\n",
    "    - $0\\leq F_X(x)\\leq 1$<br>\n",
    "    - ${ \\lim _{x\\to -\\infty }F_{X}(x)=0,\\quad \\lim _{x\\to +\\infty }F_{X}(x)=1}$\n",
    "\n",
    "    Consider a random variable X representing the outcome of rolling a fair six-sided die. The possible values of X are 1, 2, 3, 4, 5, and 6.\n",
    "    For each possible value of x, we can calculate the cumulative probability-\n",
    "    - $F(1)=P(X\\leq 1)=\\frac {1}{6}$\n",
    "    - $F(2)=P(X\\leq 2)=\\frac {2}{6}$\n",
    "    - $F(3)=P(X\\leq 3)=\\frac {3}{6}$\n",
    "    - $F(4)=P(X\\leq 4)=\\frac {4}{6}$\n",
    "    - $F(5)=P(X\\leq 5)=\\frac {5}{6}$\n",
    "    - $F(6)=P(X\\leq 6)=1$\n",
    "\n",
    "    So the CDF of X:\n",
    " ${ P_{X}(x)={\\begin{cases}\n",
    " 0&: x<1\\\\ \n",
    " {\\frac{1}{6}} &: if\\; 1\\leq x<2\\\\\n",
    " {\\frac{1}{6}} &: if\\; 1\\leq x<2\\\\\n",
    " {\\frac{2}{6}} &: if\\; 2\\leq x<3\\\\\n",
    " {\\frac{3}{6}} &: if\\; 3\\leq x<4\\\\\n",
    " {\\frac{4}{6}} &: if\\; 4\\leq x<5\\\\\n",
    " {\\frac{5}{6}} &: if\\; 5\\leq x<6\\\\\n",
    " {1} &: if\\; x\\geq 6\\\\\n",
    " \\end{cases}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ***`Probability density function:`*** The Probability Density Function (PDF) is used to describe the probability distribution of a continuous random variable.\n",
    "    $$\\int_{-\\infty}^{\\infty} f(x)dx=1$$\n",
    "    The PDF of X is given by the normal distribution formula-\n",
    "    $$f(x)=\\frac {1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{x-\\mu^2}{2\\mu^2}}$$\n",
    "    $$P(a\\leq X\\leq b)=\\int_{a}^{b}\\frac {1}{\\sigma \\sqrt{2\\pi}}e^{-\\frac{x-\\mu^2}{2\\mu^2}} dx$$\n",
    "    * X taking on a single value in a continuous distribution is zero.\n",
    "    \n",
    "    Let's say that X follows a normal distribution with mean μ=170 cm and standard deviation σ=10 cm. For example, to find the probability that an individual's height is between 160 cm and 180 cm, \n",
    "$$P(160\\leq 180)= \\int_{160}^{180} \\frac {1}{10\\cdot 1 \\sqrt{2\\pi}}\\cdot e^{-\\frac{(x-170)^2}{2\\cdot 10^2}}dx $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - The probability density function (PDF) of a multivariate normal distribution with k -\n",
    "         $$f(x)=\\frac {1}{\\lvert\\Sigma \\rvert^{\\frac{1}{2}} {2\\pi}^{\\frac{k}{2}}}\\exp^{(-\\frac{1}{2}{(x-\\mu)^T}\\Sigma^{-1}(x-\\mu))}$$\n",
    "\n",
    "      Mean Vector: $\\mu=\\begin{bmatrix} 2\\\\ 3\\end{bmatrix}$ <br>\n",
    "      Covariance matrix: $\\Sigma=\\begin{bmatrix} 1&.05\\\\0.5 & 2 \\end{bmatrix}$<br>\n",
    "      let's calculate the PDF for a specific point, $x=\\begin{bmatrix}2.5\\\\4\\end{bmatrix}$<br>\n",
    "      $x-\\mu=\\begin{bmatrix} 2.5-2\\\\4-3\\end{bmatrix}=\\begin{bmatrix} .5\\\\1\\end{bmatrix}$<br>\n",
    "      ${(x-\\mu)^T}\\Sigma^{-1}(x-\\mu)=\\begin{bmatrix} .5&1\\end{bmatrix} {\\begin{bmatrix} 1&.05\\\\0.5 & 2 \\end{bmatrix}}^{-1}{\\begin{bmatrix} .5\\\\1\\end{bmatrix}}=0.4843$<br>\n",
    "      So the probabilty density at<br>\n",
    "      $f(x)= \\frac {1}{\\lvert \\Sigma \\rvert^{\\frac{1}{2}} {2\\pi}^{\\frac{k}{2}}}\\exp^{(-\\frac{1}{2}\\cdot 0.4843^2)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mahalanobis distance: The Mahalanobis distance is a measure of the distance between a point and a distribution in a multivariate space.<br>\n",
    "$D^2={(x-\\mu)^T}\\Sigma^{-1}(x-\\mu)$<br>\n",
    "  * x is k-dimensional vector representing the point's values in each dimension\n",
    "  * μ is a k-dimensional vector representing the mean of the distribution.\n",
    "  * $\\Sigma$ is a $k \\times  k$ covariance matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5. Expected value:](#header_cell)\n",
    "<a id='ev_cell'></a>\n",
    "In probability theory, the expected value (also called expectation, expectancy, expectation operator, mathematical expectation, mean, average, or first moment) is a generalization of the weighted average. Informally, the expected value is the arithmetic mean of a large number of independently selected outcomes of a random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Exepected value of a random discrete variable:\n",
    "    $$E(x)=\\sum_{x\\in s} x.P(X=x)$$\n",
    "    Let X represent the outcome of a roll of a fair six-sided die. More specifically, X will be the number of pips showing on the top face of the die after the toss. The possible values for X are 1, 2, 3, 4, 5, and 6, all of which are equally likely with a probability of 1 / 6 . <br>\n",
    "    The expectation of X is $$E(x)=\\sum_{x\\in s} x.P(X=x)= 1\\cdot {\\frac {1}{6}}+2\\cdot {\\frac {1}{6}}+3\\cdot {\\frac {1}{6}}+4\\cdot {\\frac {1}{6}}+5\\cdot {\\frac {1}{6}}+6\\cdot {\\frac {1}{6}}=3.5$$\n",
    "\n",
    "    \n",
    "    The roulette game consists of a small ball and a wheel with 38 numbered pockets around the edge. As the wheel is spun, the ball bounces around randomly until it settles down in one of the pockets. Suppose random variable X represents the (monetary) outcome of a \\$1 bet on a single number (\"straight up\" bet). If the bet wins \n",
    "    (which happens with probability $\\frac{1}{38}$ in American roulette), the payoff is \\$35; otherwise the player loses the bet.\n",
    "    $${\\displaystyle \\operatorname {E} [\\,{\\text{gain from }}\\$1{\\text{ bet}}\\,]=-\\$1\\cdot {\\frac {37}{38}}+\\$35\\cdot {\\frac {1}{38}}=-\\${\\frac {1}{19}}.}$$\n",
    "    That is, the expected value to be won from a \\$1 bet is −\\$ $\\frac {1} {19}$\n",
    ". Thus, in 190 bets, the net loss will probably be about $10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Exepected value of a Exponential Distribution:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Exponential Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [6. Kullback-Leibler (KL) Divergence:](#header_cell)\n",
    "<a id='kl_cell'></a>\n",
    "Kullback-Leibler (KL) Divergence, also known as relative entropy, is a measure used in information theory and statistics to quantify the difference between two probability distributions.<br>\n",
    "two probability distributions P and Q is defined as:\n",
    "$$D_{KL}(P\\parallel Q)=\\sum_{x}P(x) log(\\frac{P(x)}{Q(x)})$$\n",
    "or in continuous value:\n",
    "$$D_{KL}(P\\parallel Q)=\\int P(x) log(\\frac{P(x)}{Q(x)})$$\n",
    "**Note:** KL Divergence is not symmetric, meaning $D_{KL}(P\\parallel Q)\\neq D_{KL}(Q\\parallel P)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say you have two distributions, P and Q, over the outcomes of a fair six-sided die. The distributions are as follows:\n",
    "$P(1)=P(2)=P(3)=P(4)=P(5)=P(6)=\\frac 1 6$<br>\n",
    "$Q(1)=\\frac {1}{12}\\; ,Q(2)=\\frac {1}{6}\\; ,Q(3)=\\frac {1}{6}\\; ,Q(4)=\\frac {1}{3}\\; ,Q(5)=\\frac {1}{12}\\; ,Q(6)=\\frac {1}{12}\\;$<br>\n",
    "So, $D_{KL}(P\\parallel Q)=\\int P(x) log(\\frac{P(x)}{Q(x)})=\\frac{1}{6}log(\\frac{\\frac{1}{6}}{\\frac{1}{12}})+\n",
    "\\frac{1}{6}log(\\frac{\\frac{1}{6}}{\\frac{1}{6}})+\n",
    "\\frac{1}{6}log(\\frac{\\frac{1}{6}}{\\frac{1}{6}})+\n",
    "\\frac{1}{6}log(\\frac{\\frac{1}{6}}{\\frac{1}{3}})+\n",
    "\\frac{1}{6}log(\\frac{\\frac{1}{6}}{\\frac{1}{12}})+\n",
    "\\frac{1}{6}log(\\frac{\\frac{1}{6}}{\\frac{1}{12}})\\approx 0.2075$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Kullback-Leibler (KL) Divergence between two multivariate Gaussian distributions that have the same dimension k.***\n",
    "\n",
    "Let's say we have two multivariate Gaussian distributions, P and Q, both with dimension k<br>\n",
    "$P(x)=\\mathcal{N}(\\mu_P,\\Sigma_P)$<br>\n",
    "$Q(x)=\\mathcal{N}(\\mu_Q,\\Sigma_Q)$<br>\n",
    "$$D_{KL}(P\\parallel Q)=\\frac{1}{2}\\cdot(tr(\\Sigma_Q^{-1}\\Sigma_P)+\n",
    "(\\mu_Q-\\mu_P)^{T}\\Sigma_Q^{-1}(\\mu_Q-\\mu_P)-k+\\ln(\\frac {det(\\Sigma_Q)}{det(\\Sigma_P)}))$$\n",
    "\n",
    "Where<br>\n",
    "- tr: trace of a matrix. $tr=\\begin {bmatrix} 1&1&4\\\\2&6&7\\\\0&4&5 \\end {bmatrix}=1+6+5=12$<br>\n",
    "- det: determinant of that matrix. $det=\\begin {bmatrix} 1&2\\\\3&4 \\end {bmatrix}=1*4-2*6=-2$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [7. Data Likelihood:](#header_cell)\n",
    "<a id='d_likelihood_cell'></a>\n",
    "Data likelihood refers to the probability of observing a given set of data points under a statistical model. It quantifies how well the model explains or predicts the observed data.\n",
    "\n",
    "$$P_{\\theta}(x) = \\int P_{\\theta}(z)P_{\\theta}(x |z)dz$$\n",
    "Where,\n",
    "- x is the observed data.\n",
    "- $\\theta$ is the model parameter\n",
    "- $P_{\\theta}(x |z)$ is the likelihood of observing x given z\n",
    "- $P(z)$ is the prior distribution.\n",
    "\n",
    "Mathematically, if we have a statistical model parameterized by θ and a set of observed data points $D = {x_1, x_2, ..., x_n}$, the data likelihood L(θ | D) is given by:\n",
    "\n",
    "$L(θ | D) = P(D | θ) = P(x_1 | θ) * P(x_2 | θ) * ... * P(x_n | θ)$<br>\n",
    "\n",
    "To estimate the parameter θ that maximizes the likelihood of observing the given data, we could differentiate the likelihood function with respect to θ, set the derivative to zero, and solve for θ. This approach, known as maximum likelihood estimation (MLE), is a common method for estimating model parameters based on observed data.\n",
    "\n",
    "1. ***Intractable data likelihood:*** An intractable data likelihood refers to a situation in which the likelihood function of a statistical model cannot be easily computed or evaluated directly due to the complexity of the model or the data. This often arises in Bayesian statistics, where the likelihood function is a crucial component for **parameter estimation and inference**.\n",
    "    $$P_{\\theta}(x) = \\int P_{\\theta}(z)P_{\\theta}(x |z)dz$$\n",
    "    In this example, because of the integration involved in calculating P(X), it's not straightforward to compute the likelihood directly, and hence, it becomes intractable.\n",
    "\n",
    "    ***Approximate Methods:*** Using techniques like Variational Inference or Expectation-Maximization (EM) to approximate the ***posterior distribution*** and estimate the parameters indirectly.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
