{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hesse normal form:\n",
    "[Hesse normal form](https://en.wikipedia.org/wiki/Hesse_normal_form) is primarily used in analytical geometry or co-ordinate geometry for calculating distances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculas:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivates:\n",
    "1. Instantaneous rate of change (Physics)\n",
    "2. Slope of a line at a specific point (Geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain rule:\n",
    "The chain rule is a formula for calculating the derivatives of composite functions. Composite functions are functions composed of functions inside other function(s).\n",
    "$$f(x)=h(g(x))$$\n",
    "$$\\frac{df}{dx}=\\frac{dh}{dg}\\cdot \\frac{dg}{dx}$$\n",
    "\n",
    "Example:\n",
    "$\\displaystyle MSE=J(w,b)=\\frac{1}{N}\\sum_{i=1}^{n}(y_i-(wx_i+b)^2)$<br>\n",
    "let, $g=(y_i-(wx_i+b))$<br>\n",
    "outer function: $\\displaystyle \\frac {dg^2}{dg}=2g=2(y_i-(wx_i+b))$<br>\n",
    "inner fuction: $\\displaystyle \\frac {d((y_i-(wx_i+b)))}{dw}=-x_i$<br>\n",
    "inner fuction: $\\displaystyle \\frac {d((y_i-(wx_i+b)))}{db}=-1$<br><br>\n",
    "$\\displaystyle J'(w,b)=\\begin{bmatrix} \\frac{df}{dw} \\\\ \\\\ \\frac{df}{db}\\end{bmatrix}\n",
    "= \\displaystyle {\\begin{bmatrix} \\frac{1}{N}\\sum_{i=1}^{n}-2x_i\\cdot (y_i-(wx_i+b)) \\\\ \\\\ \\frac{1}{N}\\sum_{i=1}^{n}-2\\cdot (y_i-(wx_i+b))\\end{bmatrix}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax Function:\n",
    "The softmax function, also known as softargmax or normalized exponential function, converts a vector of K real numbers into a probability distribution of K possible outcomes. The softmax function ensures that the output probabilities sum up to 1.\n",
    "\n",
    "$f_j(z) = \\frac{e^{z_j}}{\\sum_k e^{z_k}}$\n",
    "\n",
    "Here,\n",
    "- $z$ is the input vector\n",
    "- $k$ The number of classes in the multi-class classifier.\n",
    "\n",
    "Example:\n",
    "$z = \\left[ \\begin{array}{rr} 8  \\\\ 5 \\\\ 0 \\end{array}\\right] \\hspace{1cm} $<br>\n",
    "\n",
    "Calculation:\n",
    "\n",
    "$e^{z_1}=e^8= 2981$<br>\n",
    "$e^{z_2}=e^5= 148.4$<br>\n",
    "$e^{z_3}=e^0= 1.0$<br>\n",
    "$\\sum_k e^{z_k}= e^8+e^5+e^0=3130.4$ <br>\n",
    "$f_1(z_1) = \\frac{e^{z_1}}{\\sum_k e^{z_k}}$ $=2981/3130.4=0.953$<br>\n",
    "$f_2(z_2) = \\frac{e^{z_2}}{\\sum_k e^{z_k}}$ $=148.4/3130.4=0.0474$<br>\n",
    "$f_3(z_3) = \\frac{e^{z_3}}{\\sum_k e^{z_k}}$ $=1/3130.4=0.0003$<br>\n",
    "\n",
    "It is informative to check that we have three output values which are all valid probabilities, that is they lie between 0 and 1, and they sum to 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy:\n",
    "Entropy, in the context of decision trees and machine learning, is a concept used to measure the impurity or disorder of a set of data points with respect to their class labels. It is commonly used as a criterion to make decisions about how to split a dataset when constructing a decision tree.\n",
    "\n",
    "$$Entropy\\;(S)=-P_1\\log_2(p_1)-P_2\\log_2(p_2)-\\cdot \\cdot \\cdot-P_n\\log_2(p_n)=\\displaystyle -\\sum_{1}^{n}{P_n\\cdot \\log_2(p_n)}$$\n",
    "where,\n",
    "1. S is the data point\n",
    "2. $p_1, p_2, ... p_n$ are the proportions of the data points\n",
    "3. n is the distinct class lavel  in the dataset\n",
    "\n",
    "Example: Consider a dataset with 100 samples that are classified into two classes: \"Positive\" and \"Negative\". Let's say there are 70 \"Positive\" samples and 30 \"Negative\" samples.<br>\n",
    "\n",
    "$P_{+}=70/100=.7 \\; and\\; p_{-}=30/100$<br>\n",
    "$Entropy\\;(S)=−0.7log_2​(0.7)−0.3log2​(0.3)≈0.88$<br>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
