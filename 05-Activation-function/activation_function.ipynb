{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid activation function\n",
    "Advantage:\n",
    "1. Squashes numbers to range [0,1]\n",
    "2. Historically popular since they have nice interpretation as a saturating “firing rate” of a neuron\n",
    "\n",
    "Disadvantage:\n",
    "1. Saturated neurons “kill” the gradients\n",
    "2. Sigmoid outputs are not zero-centered\n",
    "3. exp() is a bit compute expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmooid_function(x):\n",
    "    sig=1/(1+math.exp(-x)) #1/(1+np.exp(-x))\n",
    "    # print(\"Sigmoid Function:\", sig)\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999546021312976"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmooid_function(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh activation function\n",
    "Advantage:\n",
    "1. Squashes numbers to range [-1,1]\n",
    "2. zero centered (nice)\n",
    "\n",
    "Disadvantage:\n",
    "1. still kills gradients when saturated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_function(x):\n",
    "    thf= math.tanh(x)\n",
    "    # print(thf)\n",
    "    return thf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh_function(-1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relu activation function\n",
    "`max(0.0, x)`\n",
    "\n",
    "Advantage:\n",
    "- Does not saturate (in +region)\n",
    "- Very computationally efficient\n",
    "- Converges much faster than sigmoid/tanh in practice (e.g. 6x)\n",
    "- Actually more biologically plausible than sigmoid.\n",
    "\n",
    "Disadvantage:\n",
    "- Not zero-centered output\n",
    "- An annoyance'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_function(x):\n",
    "\trelu=max(0.0, x)\n",
    "\t# print(\"Relu Function:\", relu)\n",
    "\treturn relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu_function(-10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky Relu function:\n",
    "    - Does not saturate\n",
    "    - Computationally efficient\n",
    "    - Converges much faster than sigmoid/tanh in practice! (e.g. 6x)\n",
    "    - will not “die”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu_function(x):\n",
    "    if x>0:\n",
    "        return x\n",
    "    else:\n",
    "        return .01*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaky_relu_function(-100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expotential Relu Function:\n",
    "    - All benefits of ReLU\n",
    "    - Closer to zero mean outputs\n",
    "    - Negative saturation regime compared with Leaky ReLU adds some robustness to noise\n",
    "    - Computation requires exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_relu_function(x):\n",
    "    '''\n",
    "    '''\n",
    "    if x>0:\n",
    "        return x\n",
    "    else:\n",
    "        return .01*(np.exp(x)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.009999546000702375"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_relu_function(-10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Point:\n",
    "    - Use ReLU. Be careful with your learning rates\n",
    "    - Try out Leaky ReLU / Maxout / ELU\n",
    "    - Try out tanh but don’t expect much\n",
    "    - Don’t use sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
