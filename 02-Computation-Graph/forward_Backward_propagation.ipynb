{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer activation:\n",
      "[[0.5        0.5        0.5        0.5       ]\n",
      " [0.86617546 0.27343225 0.72113296 0.46223277]\n",
      " [0.85371646 0.59872543 0.72685773 0.9038621 ]\n",
      " [0.97420925 0.3595954  0.87311946 0.88988196]] \n",
      " Output:\n",
      "[[0.72168765]\n",
      " [0.6897504 ]\n",
      " [0.82876192]\n",
      " [0.81259969]]\n",
      "Hidden layer delta:\n",
      "[[ 0.00385266 -0.01532565 -0.00537645 -0.05428096]\n",
      " [-0.00148476  0.00695431  0.00218849  0.02380024]\n",
      " [-0.00078319  0.00340418  0.00114631  0.01080831]\n",
      " [ 0.00355223 -0.01719516 -0.00517822 -0.05191785]]\n",
      " Output:\n",
      "[[-0.15882788]\n",
      " [ 0.06902228]\n",
      " [ 0.03622499]\n",
      " [-0.17298694]]\n"
     ]
    }
   ],
   "source": [
    "class Propagation:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        np.random.seed(00)\n",
    "        self.W1= np.random.randn(input_size, hidden_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        return self.sigmoid(x) * (1 - self.sigmoid(x))\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        self.hidden_layer_input = np.dot(X, self.W1) + self.b1\n",
    "        self.hidden_layer_activation = self.sigmoid(self.hidden_layer_input) # added activation for non-linearity\n",
    "        self.output_layer_input = np.dot(self.hidden_layer_activation, self.W2) + self.b2\n",
    "        self.output = self.sigmoid(self.output_layer_input) # calculate the output\n",
    "        return self.hidden_layer_activation, self.output\n",
    "    \n",
    "    # Define the backpropagation function\n",
    "    def backpropagation(self, X, y):\n",
    "        self.error = y - self.output\n",
    "        self.output_delta = self.error * self.sigmoid_derivative(self.output)\n",
    "        self.hidden_layer_error = np.dot(self.output_delta, self.W2.T)\n",
    "        self.hidden_layer_delta = self.hidden_layer_error * self.sigmoid_derivative(self.hidden_layer_activation)\n",
    "        return self.hidden_layer_delta, self.output_delta\n",
    "\n",
    "# Example usage\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "input_size = X.shape[1]\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "\n",
    "# Create an instance of the NeuralNetwork class\n",
    "p = Propagation(input_size, hidden_size, output_size)\n",
    "\n",
    "# Forward propagation\n",
    "hidden_layer_activation, output = p.forward_propagation(X)\n",
    "\n",
    "# Backpropagation\n",
    "hidden_layer_delta, output_delta = p.backpropagation(X, y)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Hidden layer activation:\\n{hidden_layer_activation} \\n Output:\\n{output}\")\n",
    "print(f\"Hidden layer delta:\\n{hidden_layer_delta}\\n Output:\\n{output_delta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x)*(1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(X, weights, biases):\n",
    "    layer_outputs=[X]\n",
    "    for i in range(len(weights)):\n",
    "        layer_input=np.dot(layer_outputs[-1], weights[i])+biases[i] # y= wx+b\n",
    "        layer_activation= sigmoid(layer_input) # added activation ex=> sigmoid activation function\n",
    "        layer_outputs.append(layer_activation)\n",
    "    return layer_outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for multiple neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward propagation: 2W^T*X\n",
    "def back_propagation(X, y, layer_outputs, weights):\n",
    "    errors=[y-layer_outputs[-1]]\n",
    "    deltas=[errors[-1]*sigmoid_derivative(layer_outputs[-1])]\n",
    "    for i in range(len(weights)-1,0,-1):\n",
    "        error=np.dot(deltas[-1], np.transpose(weights[i]))\n",
    "        errors.append(error)\n",
    "        delta=errors[-1]*sigmoid_derivative(layer_outputs[i])\n",
    "        deltas.append(delta)\n",
    "\n",
    "    return deltas[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the training function\n",
    "def train(X,y, layer_sizes, num_epochs, learning_rate):\n",
    "    # Initialize the weights and biases randomly\n",
    "    np.random.seed(0)\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        weights.append(np.random.randn(layer_sizes[i], layer_sizes[i+1]))\n",
    "        biases.append(np.zeros((1, layer_sizes[i+1])))\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        # Forward propagation\n",
    "        layer_outputs = forward_propagation(X, weights, biases)\n",
    "\n",
    "        # Backpropagation\n",
    "        deltas = back_propagation(X, y, layer_outputs, weights)\n",
    "        \n",
    "        # Update the weights and biases\n",
    "        for i in range(len(weights)):\n",
    "            weights[i] += learning_rate * layer_outputs[i].T.dot(deltas[i])\n",
    "            biases[i] += learning_rate * np.sum(deltas[i], axis=0)\n",
    "        \n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [[0.46067261]\n",
      " [0.51741939]\n",
      " [0.49491105]\n",
      " [0.51403026]]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "layer_sizes = [2, 4, 4, 1]  # Input size, hidden layer sizes, output size\n",
    "num_epochs = 10000\n",
    "learning_rate = 0.1\n",
    "\n",
    "weights, biases = train(X, y, layer_sizes, num_epochs, learning_rate)\n",
    "\n",
    "# Make predictions\n",
    "layer_outputs = forward_propagation(X, weights, biases)\n",
    "predictions = layer_outputs[-1]\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "layer_outputs = [X]\n",
    "layer_outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
