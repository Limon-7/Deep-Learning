{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1#Intro Of Reinfocement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### History:\n",
    "The `History` is the sequence of observations, actions or rewards.\n",
    "$$H_t=O_1, A_1, R_1, \\cdot \\cdot \\cdot, O_t, A_{t-1}, R_t$$\n",
    "1. All obserable variables up to time `t`\n",
    "2. What happens next depend on history:\n",
    "   1. The agent select action\n",
    "   2. The environment selects observations/rewards.\n",
    "#### State:\n",
    "State is the information used to determine what happens next. Generally, state is a function of the history. $S_t= f(H_{t})$\n",
    "\n",
    "#### Environment State $S^{e}_t$:\n",
    "The environment State is environment's private representation.\n",
    "1. Contain all necessary informations that determine what happens next`(observations, rewards)` from the environment perspective.\n",
    "2. Generally not visible to agent. For example, robots are walking on a road, it does not know the road's building process.\n",
    "3. If $S^{e}_t$ visible, it may contain irrelevant informations.\n",
    "\n",
    "#### Agent State $S^{a}_t$:\n",
    "The agent state is the agent's internal reprentation.\n",
    "1. We store some information which determines what action the agent will pick.\n",
    "2. It is our decision what information we will store and this informations use by RL algorithm\n",
    "3. It can be any function of history and the `Agent` has the full control over this function. $S^{a}_t=f(H_t)$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information State`(Markov State)`:\n",
    "An information state contains all information from the history. The Markov Property basically says that the probability of next state($S_{t+1}\\mid S_t$) conditioned current state is the same of probability of the next state if showed all the previous states ($S_{t+1}\\mid S_1,\\cdot \\cdot \\cdot, S_t$).\n",
    "\n",
    "1. A state $S_t$ is Merkov if and only if $\\mathbb {P}[S_{t+1} \\mid S_t] = \\mathbb {P}[S_{t+1} | S_1, ..., S_t]$\n",
    "2. Future is `independent` for the past, given the `present`. $H_{1: t}\\to S_t \\to H_{t+1:\\infty}$\n",
    "3. Once the state is know, the history may be thrown away.\n",
    "4. The state is sufficient of the future.\n",
    "5. $S^{e}_{t} \\text { and } H_t$ is Markov.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully Obserable Environements:\n",
    "Agent directly observes Environent State. Formally, this is a `Markov decision process(MDP)`\n",
    "$$O_t=S^{a}_t=S^{e}_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partially Obserable Environment\n",
    "Agent indirectly observes environment.Formally, this is a `partially obserable Markov decision process(POMDP)` $S^{a}_t\\neq S^{e}_t$.\n",
    "\n",
    "- A robot with camera vision isnâ€™t told its absolute location\n",
    "- A trading agent only observes current prices\n",
    "- A poker playing agent only observes public cards.\n",
    "1. Agent must constract its own state reprentation.\n",
    "   1. Keep track complete histoy. $S^{a}_t=H_t$\n",
    "   2. Beliefs of environmental state. $S^{a}_t=(\\mathbb{P}[S^{e}_t=s_1],\\cdot \\cdot \\cdot, [S^{e}_t=s_n])$. This whole `vector` of probabilites defines the state.\n",
    "   3. Recurrent Neural Network. $S^{a}_t=\\sigma(S^{a}_{t-1}W_s+O_t W_0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Major Components of RL Agent:\n",
    "1. Policy\n",
    "2. Value Function\n",
    "3. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.Policy $\\pi$\n",
    "A strategy used by agent to determine the next action based on the current state. On the other a policy is rule used by an agent to decide what action to take. $a_t=\\mu (s_t)$ or $a_t=\\pi (.\\mid s_t)$\n",
    "1. A policy fully defines the behaviors of an agent\n",
    "2. MDP polices depend on the current state(not the history)\n",
    "3. Policies are stationary(time-independent)\n",
    "$$A_t\\sim \\pi(.|S_t), \\forall t>0 $$\n",
    "4. Given an MDP $\\mathcal {M} = \\langle \\mathcal{S,A,P,R,\\gamma} \\rangle $ and a policy $\\pi$\n",
    "   - The state sequence $S_1, S_2, \\cdots$ is a Markov process $\\langle \\mathcal{S,P^{\\pi}} \\rangle $\n",
    "   -  The state and Reward sequence $S_1,R_2, S_2, \\cdots$ is a Markov process $\\langle \\mathcal{S,P^{\\pi}, R^{\\pi}, \\gamma} \\rangle $\n",
    "   -  where\n",
    "$$\\mathcal {P^{\\pi}_{ss'}}=\\mathcal{\\sum_{a \\in A} \\pi(a|s)P^{a}_{ss'}}$$\n",
    "$$\\mathcal {R^{\\pi}_{s}}=\\mathcal{\\sum_{a \\in A} \\pi(a|s)R^{a}_{s}}$$\n",
    "\n",
    "###### Deterministic Policy: $\\pi(s)$\n",
    "maps each state to a single action with certaity. $\\pi: S \\to A$ and $s\\in S \\text{ and } a\\in A$. \n",
    "\n",
    "###### Stochastic Policy: $\\pi(a\\mid s)$\n",
    "maps each state to a probability distribution over actions.\n",
    "$$\\pi(a\\mid s)=\\mathbb P[A_t=a\\mid S_t=s]$$\n",
    "   1. Categorial Policy: Categorical Policy is used in descrete action spaces like the way for a classifier, so there will be a final linear layer that will give us logits for each action, followed by a `softmax` to convert the logit into probabilities. Log-Likelihood the last layer of probabilites as $p_{\\theta}(s)$. The log-likelihood for an action `a` can then optinin by the indexing into the vector. $\\log \\pi_{\\theta}(a\\mid s)=\\log [P_{\\theta}(s)]_a$\n",
    "   1. Gaussian Policy: A multivariate Gaussian distribution(normal distribution) is described by a mean vector $(\\mu)$ and covarience matrix $(\\sum)$. `In Gaussian distribution matrix has entries on the diagoanl, therby we can reopresent it by a vector.`\n",
    "      1. First way: There is a single vector of log standard deviations, $\\log \\sigma$ which is not a function of state, rather a standalone parameter.\n",
    "      2. Second way: There is a neural network that maps from states to log standard deviations, $\\log \\sigma_{\\theta}(s)$. It may optionally share some layers with the mean network.\n",
    "      3. **Sampling:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.Value function(On policy Value function) \n",
    "The value function`(State Value function)` representing the expected culumatiive reward that an agent can acheive starting from a given `state(S)` following a particular policy($\\pi$) from `state(S)`.\n",
    "\n",
    "$$V^{\\pi}(s) = \\mathbb{E}[\\sum_{t\\geq 0} \\gamma^{t}r_t \\mid s_o=s,\\pi ]$$\n",
    "$$V^{\\pi}(s) = \\mathbb{E_{\\pi}}[R_{t+1} + \\gamma R_{t+2} +\\gamma^{2}R_{t+3}+ \\cdot \\cdot \\mid s_o=s,\\pi ]$$\n",
    "where,\n",
    "- $\\mathbb{E}$ denotes the expected value under the policy $\\pi$\n",
    "- $\\sum_{t\\geq 0} \\gamma^{t}r_t$ is the total discounted reward from tim step `t`\n",
    "- $\\gamma$ is the discounted factor, $0\\leq \\gamma \\le1$\n",
    "- $r_t$ is the reward received after taking an action at time step `t`\n",
    "- $s_0$ is the state at time `t`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3#Model\n",
    "Model isn't the Environment itself, but it predicts what the environment will do next to help make a plan. It can be done by two ways-\n",
    "1. Transitions: $\\mathcal P $ predicts the next state or predicts the dynamics of the environment. For example, a helicapter shifts the right angle and wind comes from the same directions, the helicapter will move towards the oppsotion of winds, this is dynamic environment. So, model should be dynamics. $\\mathcal P^{a}_{ss^{\\prime}}=\\mathbb P[S^{\\prime}=s^{\\prime}\\mid S=s, A=a]$\n",
    "   \n",
    "2. Reward $\\mathcal R$: Predict the next-immediate return. For example, the helicapter will get rewarded 1 points for staying alive. $\\mathcal R^{a}_{s}=\\mathbb E[R \\mid S=s, A=a]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorizing RL agents:\n",
    "1. Value Based:\n",
    "   1. No policy\n",
    "   2. Value Function\n",
    "2. Policy Based\n",
    "   1. Policy\n",
    "   2. No value Function\n",
    "3. Actor Critic\n",
    "   1. Policy\n",
    "   2. Valu Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning and Planning\n",
    "There are two fundamental Problem:\n",
    "\n",
    "1. ***Reinforcement Learning:***\n",
    "   - The environment is `initially unknown`.\n",
    "   - The agent interacts with the environment and figure out the best way to behave with this environment and maximize the reward. For example- `Atari` game.\n",
    "\n",
    "\n",
    "2. ***Planning:***\n",
    "   - A model of the `environment is known`.\n",
    "   - The agent performs computations with its model(without external iteractions). For example, we tell all rules of the certain games to Agent.\n",
    "   - So agent will take times for computation for the best reward.\n",
    "\n",
    "***Note:*** To work with `RL` first learn how environment is works, then do planning. These two things are linked together but they are seperated by problem setup.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expolration and Exploitation\n",
    "**Exploration:** Expolaration finds more information about the environment.\n",
    "\n",
    "**Exploitation:** Exploitation exploits known information to maximize reward.\n",
    "\n",
    "**Example:** `Resturant Selection`\n",
    "1. `Expoloration:` Try a new resturant.\n",
    "2. `Exploitation:` Go to your favourite resturant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2#Markov Decision Process`(MDPs)`\n",
    "Markove decision process formally describe an environment for reinforcement learning.\n",
    "- Environment is fully observable- all relevent informations of the environment are presented to `Agent`\n",
    "- The current state completely characterises the process.\n",
    "- all problems can be formalized as `MDPs`.\n",
    "  - Optimal control deals with continuos MDPs- how an octopas will swim in fluid.\n",
    "  - Partially obserable problems can be converted into MDPs\n",
    "  - `Bandits(Expoloration Expolitation dilemma in RL-> agents will get set of actions and get some reward for that action)` are MDPs with one state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Property:\n",
    "A state $\\mathcal S_t$ is Merkov if and only if $$\\mathbb {P}[S_{t+1} \\mid S_t] = \\mathbb {P}[S_{t+1} | S_1, ..., S_t]$$\n",
    "\n",
    "1. Future is `independent` for the past, given the `present`. $H_{1: t}\\to S_t \\to H_{t+1:\\infty}$\n",
    "2. Once the state is know, the history may be thrown away.\n",
    "3. The state is sufficient of the future.\n",
    "4. $S^{e}_{t} \\text { and } H_t$ is Markov."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Transition Matrix $\\mathcal P$\n",
    "$$\\mathcal {P_{{ss^{\\prime}}}}= \\mathbb{P}[S_{t+1}=s^{\\prime}\\mid S_t=s]$$\n",
    "$$\\mathcal{p} = \\begin{bmatrix} \\mathcal{p_{11}} & \\ldots& \\mathcal{p_{1n}}\\\\\n",
    "\\vdots & & \\\\\n",
    "\\mathcal{p_{n1}}& \\ldots& \\mathcal{p_{nn}}\n",
    "\\end{bmatrix} $$\n",
    "`where each row of matrix sums to 1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Process $\\langle \\mathcal{S,P, R, \\gamma} \\rangle$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return $\\mathcal {G_t}$:\n",
    "Total discounted reward from time-step t is called Return.\n",
    "$$\\mathcal{G_t}=R_{t+1}+ \\gamma R_{t+2} + \\gamma^{2} R_{t+3}+\\cdots = \\sum^{\\infty}_{k=0} \\gamma^{k}R_{t+k+1} $$\n",
    "\n",
    "- This values immediate reward above delyed reward.\n",
    "  - $\\gamma \\text{ close to 0 leads to }$ `myopic` evaluation.\n",
    "  -  $\\gamma \\text{ close to 1 leads to }$ `far-sighted` evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for t=1, k=T-2\n",
    "$$\\mathcal{G_1}=R_{2}+ \\gamma R_{3} + \\gamma^{2} R_{4}+\\cdots = \\sum^{\\infty}_{T\\geq 2} \\gamma^{T-2}R_{T} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why discount?\n",
    "Most Markov reward and decision processs are discounted.\n",
    "1. Mathematically convenient to discount rewards\n",
    "2. Avoids infinite returns in cyclic Markov processes\n",
    "3. Uncertainty about the future may not be fully represented\n",
    "4. If the reward is financial, immediate rewards may earn more interest than delayed rewards\n",
    "5. Animal/human behaviour shows preference for immediate reward\n",
    "6. It is sometimes possible to use undiscounted Markov reward\n",
    "processes (i.e. **$\\gamma = 1$**), e.g. if all sequences terminate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Function `V(s)`:\n",
    "The value function `V(s)` gives the long-term values of state `s`. The state value function V(s) of an MRP is the **expected return $(\\mathbb{E})$** starting from state s.\n",
    "$$\\mathcal v(s)=\\mathbb {E}[G_t| S_t=s]$$\n",
    "$$\\mathcal{G_1}=R_{2}+ \\gamma R_{3} + \\gamma^{2} R_{4}+\\cdots = \\sum^{\\infty}_{T\\geq 2} \\gamma^{T-2}R_{T} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation for MRPs:\n",
    "The Bellman equation is a fundamental concept in Reinforcement Learning that provides a recursive decomposition of the value function. It states that the value of a state under a particular policy $(\\pi)$ can be decomposed into the `immediate reward` $R_{t+1}$  `plus the discounted value of a subsequent state` $\\gamma \\mathcal{v(S_{t+1})}$.\n",
    "\n",
    "$$\\mathcal v(s)=\\mathbb {E}[G_t| S_t=s]$$\n",
    "\n",
    "$$=\\mathbb {E}[R_{t+1}+ \\gamma R_{t+2} + \\gamma^{2} R_{t+3}+\\cdots | S_t=s ]$$\n",
    "\n",
    "$$=\\mathbb {E}[R_{t+1}+ \\gamma (R_{t+2} + \\gamma R_{t+3}+\\cdots) | S_t=s ]$$\n",
    "$$=\\mathbb {E}[R_{t+1}+ \\gamma \\mathcal{G_{t+1}} | S_t=s ]$$\n",
    "$$=\\mathbb {E}[R_{t+1}+ \\gamma \\mathcal{v_{(S_{t+1})}} | S_t=s ]$$\n",
    "\n",
    "***`Bellman Equation Matrix Form:`***\n",
    "$$\\begin{bmatrix} v(1)\\\\\\vdots\\\\v(n)\\end{bmatrix} =\n",
    "\\begin{bmatrix} R_1\\\\\\vdots\\\\R_n\\end{bmatrix}\n",
    "+\n",
    "\\gamma \\begin{bmatrix} \\mathcal{p_{11}} & \\ldots& \\mathcal{p_{1n}}\\\\\n",
    "\\vdots & & \\\\\n",
    "\\mathcal{p_{n1}}& \\ldots& \\mathcal{p_{nn}}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} v(1)\\\\\\vdots\\\\v(n)\\end{bmatrix}\n",
    " $$\n",
    " - It is linear equation.\n",
    " - It can be solved directly:\n",
    "  $$\\mathcal{v}=\\mathcal{R+\\gamma Pv}$$\n",
    "  $$\\mathcal{R}=\\mathcal{(1-\\gamma P)v}$$\n",
    "  $$\\mathcal{v}=\\mathcal{(1-\\gamma P)^{-1}R}$$\n",
    "- Computational complexity is $O(n^3)\\text{ for n states}$\n",
    "- Dirrect solution only possible for small MRPs\n",
    "- Iterative methods for large MRPs\n",
    "  - Dynamic Programming\n",
    "  - Monte-Carlo evaluation\n",
    "  - Temporal-Difference learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process $\\mathcal (S, A, R, P, \\gamma)$\n",
    "A `Markov decision process(MDP)` is `Markov Reward Process` with decisions. It is an environment in which all states are Markov.\n",
    "\n",
    "- `S:` set of possible state\n",
    "\n",
    "- `A:` a finite set of possible action\n",
    "  \n",
    "- `P:` state transition probability matrix $\\mathcal (P)$ : distribution over next state given `(state, action)` pair $\\mathcal P(s^{\\prime}\\mid s,a)$  or $\\mathcal {P^{a}_{ss^{\\prime}}} = \\mathbb P [S_{t+1}=s'| S_t=s, A_t=a]$ probablility from `state s` to **$s^{\\prime}$**\n",
    "  - $\\mathcal P(s^{\\prime}\\mid s,a)$ or ${\\mathcal P^{a}_{ss^{\\prime}}} = \\mathbb P [S_{t+1}=s'| S_t=s, A_t=a]$\n",
    "  \n",
    "- **`Reward: `** $\\mathcal R$ is reward function, $\\mathcal {R^{a}_{s}}=\\mathbb E [R_{t+1}| S_t = s, A_t=a]$ \n",
    "- $\\gamma$: discounted factor; $\\gamma \\in [0,1]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy: $\\pi$\n",
    "A strategy used by agent to determine the next action based on the current state. On the other a policy is rule used by an agent to decide what action to take. $a_t=\\mu (s_t)$ or $a_t=\\pi (.\\mid s_t)$\n",
    "1. A policy fully defines the behaviors of an agent\n",
    "2. MDP polices depend on the current state(not the history)\n",
    "3. Policies are stationary(time-independent)\n",
    "$$A_t\\sim \\pi(.|S_t), \\forall t>0 $$\n",
    "4. Given an MDP $\\mathcal {M} = \\langle \\mathcal{S,A,P,R,\\gamma} \\rangle $ and a policy $\\pi$\n",
    "   - The state sequence $S_1, S_2, \\cdots$ is a Markov process $\\langle \\mathcal{S,P^{\\pi}} \\rangle $\n",
    "   -  The state and Reward sequence $S_1,R_2, S_2, \\cdots$ is a Markov process $\\langle \\mathcal{S,P^{\\pi}, R^{\\pi}, \\gamma} \\rangle $\n",
    "   -  where\n",
    "$$\\mathcal {P^{\\pi}_{ss'}}=\\mathcal{\\sum_{a \\in A} \\pi(a|s)P^{a}_{ss'}}$$\n",
    "$$\\mathcal {R^{\\pi}_{s}}=\\mathcal{\\sum_{a \\in A} \\pi(a|s)R^{a}_{s}}$$\n",
    "- Transition Probability will be average of all probabity of that state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Function(State-value function) in MDPs $(v_{\\pi}(s))$:\n",
    "The state-value function $v_{\\pi}(s)$ of an `MDP` is the expected return starting from `state s` and then following policy $\\pi$\n",
    "$$\\mathcal v_{\\pi}(s)=\\mathbb {E_{\\pi}}[G_t| S_t=s]$$\n",
    "\n",
    "**Bellman Expectation equation for value function in MDPs**\n",
    "$$v_{\\pi}(s)=\\mathbb {E_{\\pi}}[R_{t+1}+ \\gamma \\mathcal{v_{\\pi}(S_{t+1})} | S_t=s ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action-value function in MDPs $(q_{\\pi}(s,a))$\n",
    "The action-value function $(q_{\\pi}(s,a))$ is the expected return starting from state s, taking action s, and then folowing the policy $\\pi$\n",
    "$$\\mathcal q_{\\pi}(s,a)=\\mathbb {E_{\\pi}}[G_t| S_t=s, A_t=a]$$\n",
    "\n",
    "**Bellman Expectation equation for action-value function in MDPs**\n",
    "$$q_{\\pi}(s,a)=\\mathbb {E_{\\pi}}[R_{t+1}+ \\gamma \\mathcal{q_{\\pi}(S_{t+1},A_{t+1})} | S_t=s, A_t=a ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relationship between $v_{\\pi}(s) \\text{ vs } q_{\\pi}(s,a)$ :\n",
    "From s to subsequent state by taking some action the action value\n",
    "$$v_{\\pi}(s)=\\sum_{a\\in A}\\pi(a|s)q_r(s,a)$$\n",
    "$$q_{\\pi}(s,a)=\\mathcal {R^{a}_{s}+ \\gamma \\sum_{s'\\in S}P^{a}_{ss'}v_r(s')}$$\n",
    "\n",
    "$$v_{\\pi}(s)=\\mathcal{ \\sum_{a\\in A} \\pi(a|s)(R^{a}_{s}+ \\gamma \\sum_{s'\\in S}P^{a}_{ss'}v_r(s'))}$$\n",
    "$$\\gamma \\sum_{s'\\in S}P^{a}_{ss'}v_r(s') =\\gamma {(\n",
    "    P^{a}_{s's'_1}v_r(s'_1)+ P^{a}_{s'_1 s'_2}v_r(s'_2) \n",
    "    )}$$\n",
    "\n",
    "$$q_{\\pi}(s,a)=\\mathcal {R^{a}_{s}+ \\gamma \\sum_{s'\\in S}P^{a}_{ss'}\\sum_{a'\\in A}\\pi(a'|s')q_r(s',a')}$$\n",
    "\n",
    "\n",
    "**Bellman Equation Matrix Form:**\n",
    "$$v_r=\\mathcal{R^{\\pi} +\\gamma P^r v_r}$$\n",
    "$$v_r=\\mathcal{(\\gamma P^r )^{-1} R^{\\pi} }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal  State-Value function( $v_{*}(s)$ ) and Action-Value function( $q_{*}(s,a)$ ):\n",
    "***State-Value function( $v_{*}(s)$ ):*** The optimal state-value function is the maximum value function over all policies.\n",
    "$$\\mathcal{v_{*}(s)=\\max_{\\pi}v_r(s)}$$\n",
    "\n",
    "***Action-Value function( $q_{*}(s,a)$ ):*** The optimal action-value function is the maximum action-value function over all policies.\n",
    "$$\\mathcal{q_{*}(s,a)=\\max_{\\pi}q_r(s,a)}$$\n",
    "- Optimal valu function specifies the best possible perfomance in the MDP.\n",
    "- An MDP is solved when we get optimal value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Policy:\n",
    "$$\\pi\\geq\\pi^{\\prime} \\text{ if } v_{\\pi}(s)\\geq v_{\\pi^{\\prime}}(s), \\forall s$$\n",
    "For any Markov Decision Process\n",
    "- There exists an optimal policy $\\pi_*$ that is better than or equal to all other policies,$\\pi_* \\geq \\pi, \\forall \\pi$.\n",
    "- All optimal policies achieve the optimal value function, $v_{Ï€_âˆ—} (s) = v_{*}(s)$\n",
    "- All optimal policies achieve the optimal action-value function, $q_{Ï€_âˆ—} (s,a) = q_{*}(s,a)$\n",
    "\n",
    "***Find an optimal policy:*** An optimal policy can be found by maximising over $q_{*}(s,a)$\n",
    "\n",
    "$$\\pi_{*}(a|s)= \\begin{cases}\n",
    "1 &\\quad \\text{if } \\mathcal{a=\\argmax_{a\\in A} q_{*}(s,a)}\\\\\n",
    "0 &\\quad \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "There is always a deterministic optimal policy for any `MDP`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bellman Optimality Equation\n",
    "***Optimal Value Function: $v_{*}(s)$***\n",
    "$$v_{*}(s)=\\max_{a} q_{*}(s,a)$$\n",
    "$$\\mathcal {q_{*}(s,a)=R^{a}_{s}+\\gamma\\sum_{s'\\in S}P^{a}_{ss'}v_{*}(s')}$$\n",
    "\n",
    "$$\\mathcal {v_{*}(s)=\\max_{a}R^{a}_{s}+\\gamma\\sum_{s'\\in S}P^{a}_{ss'}v_{*}(s')}$$\n",
    "\n",
    "***Optimal Value Function: $q_{*}(s,a)$***\n",
    "$$\\mathcal {q_{*}(s,a)=R^{a}_{s}+\\gamma\\sum_{s'\\in S}P^{a}_{ss'}\\max_{a'} q_{*}(s',a')}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ways to solve Bellman Optimality equation:\n",
    "Bellman Optiality equation is non-linear.\n",
    "- No close form solution\n",
    "- Iterative Solution Methods:\n",
    "  - Value Iteration\n",
    "  - Policy Learning\n",
    "  - Q-learning\n",
    "  - Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extention to MDPs\n",
    "1. Infinite and continuous MDPs\n",
    "2. Partially obserable MDPs(POMDPs)\n",
    "   1. Belief State\n",
    "   2. Reduction of POMDPs\n",
    "3. undiscounted, average reward MDPs\n",
    "   1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3#Dynamic Programming:\n",
    "The term dynamic programming refers to a collection of algorithoms that can be used to compute optimal policies given a perfect model of the environment as a Markov Decision Process(MDP)\n",
    "1. Policy Evaluation(Prediction)\n",
    "2. Policy Improvement\n",
    "3. Policy Iteration\n",
    "4. Value Iteration\n",
    "5. Synchronous  DP Algorithms\n",
    "6. Asynchronous DP\n",
    "7. Generalized Policy Iteration\n",
    "8. Efficiency in Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1#Policy Evaluation:\n",
    "The technique to compute value-function $v_{\\pi}$ or $q_{\\pi}$ for an arbitrary policy $\\pi$ is called `Policy Evaluation` according to DP.\n",
    "$$v_{\\pi}(s)=\\mathcal{ \\sum_{a\\in A} \\pi(a|s)(R^{a}_{s}+ \\gamma \\sum_{s'\\in S}P^{a}_{ss'}v_r(s'))}$$\n",
    "- Here everything is known except `v`\n",
    "- This equation is Linear.\n",
    "- If there is `n state` there will be `n no of v`.\n",
    "\n",
    "**Dp approach:**\n",
    "1. Initialize $v_0(s)=0 \\text{ or random for all states (0 for terminal state)}$\n",
    "2. $$v_{k+1}(s)=\\mathcal{ \\sum_{a\\in A} \\pi(a|s)(R^{a}_{s}+ \\gamma \\sum_{s'\\in S}P^{a}_{ss'}v_k(s'))}$$\n",
    "3. $$v^{k+1}(s)=\\mathcal{ R^{\\pi}+ \\gamma P^{\\pi}v^k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why does this works? $v_{\\pi}(s)= v_{\\infty}(s)$\n",
    "$$v_{k+1}(s)=\\mathcal{ \\sum_{a\\in A} \\pi(a|s)(R^{a}_{s}+ \\gamma \\sum_{s'\\in S}P^{a}_{ss'}v_k(s'))}$$\n",
    "- $v_{\\pi}(s)$ is a fixed point for this update what we are looking for. When update rule $v_{\\pi}(s)$ will no longer change is called a fixed point.\n",
    "\n",
    "#### When do we quit?\n",
    "- We approach true answer as $k\\to \\infty$\n",
    "- How: check how much $v_k(s)$ has changed from $k\\to {k+1}$\n",
    "  - $\\delta = \\mathcal {\\max_{s}|V_{k+1}(s)-v_k(s)|}$\n",
    "  - Exit when $\\delta \\lt {threshold}; {threshod=[10^{-3}, 10^{-5}, 10^{-8}]}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2#Policy Improvement\n",
    "Computing value-function for a policy will help to find better policies. Suppose, we have calculated the value-function $V^{\\pi}$ for an arbitrary policy $\\pi$. For some state`S` we would like to know whether or not we should change the policy deterministically choose an action ***$\\mathcal{a\\neq} \\pi (s)$***\n",
    "\n",
    "#### Question: We already know how good it is to follow current policy from `S` to $V^r(s)$ would it better or worse to change to the new policy?\n",
    "One way to answer this question is to select `a` in `s` and thereafter following the `existing policy`, $\\pi$ and calculate the q function.\n",
    "$$\\mathcal {q^{r}(s,a)=\\mathbb{E}_{\\pi}[R_{t+1} + \\gamma V^{\\pi}(S_{t+1}) | S_t=s, A_t=a]  }$$\n",
    "$$\\mathcal {q^{r}(s,a)=\\sum_{s'\\in S}P^{a}_{ss'} [R^{a}_{ss'}+\\gamma v^{\\pi}(s')]}$$\n",
    "\n",
    "if $q^{r}(s,a)\\gt V^{\\pi}(s)$ if it is better to select `a` once in `s` and thereafter follow $\\pi$ than it would be to follow $\\pi$ all the time--then one would expect it to be better still to select `a` every time `s` is encountered, and that the new policy would in fact be a better one overall. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theoream:\n",
    "1. Consider a deterministic policy, $a=\\pi(s)$\n",
    "2. We can improve this policy by acting greedily\n",
    "   $$\\pi'(s)=\\argmax_{a\\in A}q_{\\pi}(s,a)$$\n",
    "3. This improve the value from any state `s` over one step,\n",
    "   $$q_{\\pi} (s, \\pi'(s))= \\max_{a\\in A} q_{\\pi}(s,a)\\geq q_{\\pi}(s, \\pi(s))=v_{\\pi}(s)$$\n",
    "4. It therefore, It improves the value function $v_{\\pi'}(s)\\geq v_{\\pi}(s)$\n",
    "$$v_{\\pi}(s)\\leq q_{\\pi} (s, \\pi'(s))= \\mathbb{E}_{\\pi'}[R_{t+1}+ \\gamma v_{\\pi}(S_{t+1})|S_t=s] $$\n",
    "$$=> v_{\\pi}(s)\\leq \\mathbb{E}_{\\pi'}[R_{t+1}+ \\gamma q_{\\pi}(S_{t+1}, \\pi'(S_{t+1}))|S_t=s] $$\n",
    "$$=> v_{\\pi}(s)\\leq \\mathbb{E}_{\\pi'}[R_{t+1}+ \\gamma R_{t+2}+\\gamma^{2} q_{\\pi}(S_{t+2}, \\pi'(S_{t+2}))|S_t=s] $$\n",
    "$$=> v_{\\pi}(s)\\leq \\mathbb{E}_{\\pi'}[R_{t+1}+ \\gamma R_{t+2}+\\cdots|S_t=s] = v_{\\pi'(s)}$$\n",
    "5. If improvements stop\n",
    "   $$q_{\\pi} (s, \\pi'(s))= \\max_{a\\in A} q_{\\pi}(s,a) = q_{\\pi}(s, \\pi(s))=v_{\\pi}(s)$$\n",
    "6. Then the bellman optimality equation has been satishfied\n",
    " $$ v_{\\pi}(s)= \\max_{a\\in A} q_{\\pi}(s,a)$$\n",
    "7. Therefore, $\\mathcal{v_{\\pi}(S)=v_{*}(s) \\text{ for all } s \\in S}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3#Policy Iteration\n",
    "Once a policy $\\pi$ has been improved using $v_r$ to yeild better policy $\\pi'$ we can then compute $v_{\\pi'}$ and improve it again to yeild an even better $\\pi''$. Therefore, we can obtain a sequence of monotonically improving policies and vlau-function.\n",
    "$$\\pi_0 \\to^{E} v_{\\pi_0}\\to^{I} \\pi_1 \\to^{E} v_{\\pi_1}\\to^{I} \\pi_2 \\to^{E} \\cdots \\to^{I} \\pi^{*}\\to^{E} v_{*}$$\n",
    "\n",
    "1. Each policy is guranteed to be a strict improvement over the previous one unless it is already optimal. Because a finite `MDP` has only a finite number of policies, this process must converse to an optimal policy and value-function in a finite number of iteration.\n",
    "2. This process is called policy iteration.\n",
    "3. each policy evaluation, itself an iterative computation, is started with the value function for the previous policy.\n",
    "4. This typically results in a great increase in the speed of convergence of policy evaluation (presumably\n",
    "because the value function changes little from one policy to the next)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Theorem: Principle of Optimality\n",
    "An optimal policy can be subdivided into two components:\n",
    "1. An optimal first action $A_*$\n",
    "2. Followed by an optimal policy from successor state $S'$\n",
    "\n",
    "A policy $\\pi(a|s)$ achieves the optimal value from state `s`, $ v_{\\pi}(s)=v_{*}(s)$ if an only if\n",
    "1. for any state $s'$ reachable from `s`\n",
    "2. $\\pi$ achieves the optimal value from state $s', v_{\\pi}(s')=v_{*}(s')$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drawbacks:\n",
    "One drawback to policy iteration is that each of its iterations involves policy evaluation, which may itself be a protracted iterative computation requiring multiple sweeps through the state set. If policy evaluation is done iteratively, then convergence exactly to occurs only in the limit.\n",
    "\n",
    "##### ***Must we wait for exact convergence or can we stop short of that?***\n",
    "We can truncate policy evaluation. In policy evaluation iterations beyond the first three have no effect on the corresponding `greedy policy`.\n",
    "\n",
    "***`KeyPoint:`***\n",
    "- Optimal values are unique, but optimal policies are not unique.\n",
    "- What if we ran the value evaluation algorithm and kept switching back and forth between 2 or more optimal policies?\n",
    "  - The loop will never terminate.\n",
    "  - Sol: we can quit when the policy is stable or when the value is stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4#Value Iteration\n",
    "The policy evaluation step of policy iteration can be truncated in several ways without losing the convergence guarantees of policy iteration. One important special case is when policy evaluation is stopped after just one sweep (one backup of each state). This algorithm is called `value iteration`. It can be written as a particularly simple backup operation that combines the policy improvement and truncated policy evaluation steps:\n",
    "- Iterative application of Bellman optimality backup into an update rule.\n",
    "$$\\mathcal {v_{k+1}(s)= \\max_{a\\in A}(R^{a}_{s}+ \\gamma\\sum_{s'\\in S}P^{a}_{ss'}v_k(s'))}$$\n",
    "$$\\mathcal{v_{k+1}= \\max_{a\\in A}(R^{a}+ \\gamma P^{a}v_k)}$$\n",
    "- $v_1\\to v_2\\to \\cdots \\to v_*$\n",
    "- Using synchronous backup\n",
    "  - At each iteration k+1\n",
    "  - For all states $s\\in S$\n",
    "  - Update $v_{k+1}(s)\\text{ from } v_k(s')$\n",
    "- Converge to $v_*$\n",
    "- Unlike policy iteration there is no explicit policy\n",
    "- Intermediate value functions may not correspond to any policy\n",
    "  -   $$\\pi'(s)=\\argmax_{a\\in A}q_{\\pi}(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5#Synchronous  DP Algorithms:\n",
    "|Problem|Bellman Equation|Algorithm| Time Complexity| Space Complexity|\n",
    "|:--|:--|:--|:--|:--|\n",
    "Prediction|Bellman Expectation Equation|Iterative Policy Evaluation|||\n",
    "Control|Bellman Expectation Equation + Greedy Policy Improvement|Policy Iteration|||\n",
    "Control|Bellman Optimality Equation|Value Iteration|||\n",
    "\n",
    "\n",
    "1. Algorithm are based on state-value function\n",
    "2. Complexity $O(mn^2)$ per iteration, `for m action and n states`\n",
    "3. Could also apply to action-value function $q_{\\pi}(s,a) \\text{ or } q_{*}(s,a)$\n",
    "4. Complexity $O(m^2n^2)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6#Asynchronous DP:\n",
    "A major drawback to the DP methods that we have discussed so far is that they involve operations over the entire state set of the MDP, that is, they require sweeps of the state set. If the state set is very large, then even a single sweep can be prohibitively expensive. For example, the game of backgammon has over $10^20$ states. Even if we could perform the value iteration backup on a million states per second, it would take over a thousand years to complete a single sweep.\n",
    "\n",
    "Asynchronous DP algorithms are in-place iterative DP algorithms that are not organized in terms of systematic sweeps of the state set. These algorithms back up the values of states in any order whatsoever, using whatever values of other states happen to be available. The values of some states may be backed up several times before the values of others are backed up once. To converge correctly, however, an asynchronous algorithm must continue to backup the values of all the states: it can't ignore any state after some point in the computation. Asynchronous DP algorithms allow great flexibility in selecting states to which backup operations are applied.\n",
    "\n",
    "There are Three simple ideas for async dP:\n",
    "1. In-Place dp\n",
    "2. Prioritised sweeping\n",
    "3. Real-time dp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4#Monte Carlo Methods\n",
    "1. Monte Carlo Policy Evaluation\n",
    "2. Monte Carlo Estimation of action-values\n",
    "3. Monte Cralo Control\n",
    "4. Monte Cralo Control without Exploring Star\n",
    "5. Off-Policy Prediction vai importance sampling\n",
    "6. Incremental Implementation\n",
    "7. Off-Policy Monte carlo Control\n",
    "8. Discounting-aware Importance Sampling\n",
    "9. Per-Decision Importance Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1#Monte Carlo Policy Evaluation\n",
    "Suppose we want to estimate the $V_{\\pi} \\text{ or } q_{\\pi}$, the value of a state `s` under given policy $\\pi$, given a set of `episodes` obtained by following $\\pi$ and passing through `s`, without environment dynamics or Bellman equation. Each occurrence of `state s` in an episode is called a `visit to s`. The ***every-visit*** MC method estimates $V_{\\pi}(s)$ as the average of returns $\\mathcal G$ following all the visits to `S` in a set of episodes.\n",
    "\n",
    "$$v_{\\pi}(s)=\\mathbb{E}[G_t|S_t=s] \\approx \\frac{1}{N}\\sum^{N}_{i=1}G_{i,s} $$\n",
    "\n",
    "***First Visit:*** Within a episode, the first time `s` is visited is called the `First Visit to S`. The `first-visit MC` method average just the returns following first visit to s. \n",
    "***Every-Visit:***\n",
    "\n",
    "- Both first visit MC and every-visit MC converge to $V_{\\pi}(s)$\n",
    "- Estimate for each `state s` are independent. The estimate for one state does not build upon the estimate of any other state.\n",
    "- MC methods do not bootstrap.\n",
    "- No need for $\\mathcal P^{a}_{ss'} \\text{ and } R^{a}_{ss'}$\n",
    "- Transition Probability and expected reward must be computed before dp can be applied, and such computations are often complex and error-prone.\n",
    "- Monte Carlo methods to work with sample episodes alone can be a significant advantage even when one has complete knowledge of the environment's dynamics.\n",
    "- MC sampled on the one episodes, whereas DP includes one-step transition.\n",
    "- `MC method has the ability to learn from actual experience and from simulated experience` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the value of a state not vissited by our policy?\n",
    "1. Do not compute any values from those states because we can not visit those states.\n",
    "2. Manually put the agent into different starting states, for example, in grid-world, do not start from the same position on every episode,  instead we can choose starting position at random to ensure that every state will have corresponing sample returns. That does not violate the policy.\n",
    "3. If a policy is probabilistic with non-zero probability for every actions for evry states then this would be not a problems, given enough time we will get a great number of samples.\n",
    "\n",
    "#### Q-1: What if we encounter the same state more than once?\n",
    "1. Solution number one is to consider the return only for the first time the state was visited.\n",
    "2. Solution number one is to consider the return only for the first time the state was visited. \n",
    "3. It turns out that you can prove theoretically that these will both converge to the true answer.\n",
    "\n",
    "##### Q-2: Does this problem create an infinite loops?\n",
    "So now let's consider the problem where our policy leads to an infinite cycle. For example, suppose in one state the policy is to go left, but then in the state to the left, the policy is to go right. Clearly, this will just lead to going left and right forever.\n",
    "\n",
    "- The greater issue here is what if we have an episode that never ends in this case, Montecarlo methods do not apply because by definition of the Montecarlo method, we can only compute the value once we know the return, but we only know the return after the episode is terminated. \n",
    "If the episode does not terminate, then the return cannot be computed and Montecarlo methods cannot be employed. \n",
    "- Practically speaking, when it comes to our environment, we will declare our episode complete when it reaches a certain number of steps. For example, we consider a 20 steps or 100 steps to be the end of an episode if we haven't yet reached the terminal state.\n",
    "-  So even if there is an infinite cycle, the episode will still terminate. For example, in other environments like Cardpool and Mountain CA, which are part of opening IJM, the episodes end after you reach two hundred steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2#Monte Carlo Estimation of action-values\n",
    "If a model is not available, then it is particularly useful to estimate action values rather than state values. With a model, state values alone are sufficient to determine a policy; one simply looks ahead one step and chooses whichever action leads to the best combination of reward and next state, as we did in the chapter on DP.\n",
    "- Without a model, however, `state values` alone are `not sufficient to determine a policy`.\n",
    "- Thus, one of our primary goals for Monte Carlo methods is to estimate $Q_{*}$. To achieve this, we first consider another policy evaluation problem.\n",
    "- The every-visit MC method estimates the value of a state-action pair as the average of the returns that have followed visits to the state in which the action was selected.\n",
    "- The first-visit MC method averages the returns following the first time in each episode that the state was visited and the action was selected.\n",
    "\n",
    "#### Problem-1: Many relevant state-action pairs may never be visited.\n",
    "- If is a deterministic policy $\\pi$, then in following $\\pi$ one will observe returns only for one of the actions from each state.\n",
    "- With no returns to average, `the Monte Carlo estimates of the other actions will not improve with experience`.\n",
    "- This is a serious problem because the purpose of learning action values is to help in choosing among the actions available in each state. \n",
    "- To compare alternatives we need to estimate the value of all the actions from each state, not just the one we currently favor.\n",
    "- `This is the general problem of maintaining exploration.`\n",
    "\n",
    "##### Solution:\n",
    "- For policy evaluation to work for action values, we must assure `continual exploration`.\n",
    "- the first step of each episode starts at a state-action pair, and that every such pair has a `nonzero probability` of being selected as the start.\n",
    "- This guarantees that `all state-action pairs` will be visited an `infinite number of times` in the `limit of an infinite number of episodes`.\n",
    "- This process is called **`Exploring Stars`**\n",
    "- `But, ` this approach is not helpfull when agent learns directly from the interactions with an environment. The most common alternative approach to assuring that all state-action pairs are encountered is to consider only policies that `are stochastic with\n",
    "a nonzero probability of selecting all actions`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3#Monte Cralo Control\n",
    "$$\\pi_{0}\\to^{E} q_{r_{0}}\\to^{I}\\pi_{1}\\to^{E} q_{r_{1}}\\to^{I} r_2\\to^{E}\\cdots\\to^{I}r_{*}\\to^{E}q_{*}$$\n",
    "- episodes have exploring starts\n",
    "- policy evaluation could be done with an infinite number of episodes.\n",
    "- $\\pi(s)=\\argmax_{a\\in A}Q(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5#Temporal Difference(TD)\n",
    "1. TD prediction\n",
    "2. Advantage of TD Prediction Methods\n",
    "3. Optimality of TD(0)\n",
    "4. Sarsa: On-policy TD control\n",
    "5. Q-Learning off-policy TD control\n",
    "6. Expected Sarsa\n",
    "7. Maximizing Bias and Double Learning\n",
    "8. Games, Afterstates and other speacial cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6#n-step Bootsatrapping\n",
    "1. n-step TD Prediction\n",
    "2. n-step Sarsa\n",
    "3. n-steap off-policy learning\n",
    "4. per-decision Methods with control variates\n",
    "5. Off-policy learning without importance: the n-step Tree Backup Algorithm\n",
    "6. A-unifying Algorithm: n-step Q()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning(RL)\n",
    "Reinforcement learning is a type of machine learning where an agent learns to take decisions by performing actions in  an environment to maximize cumulative reward.\n",
    "\n",
    "#### Terms:\n",
    "1. **Agent:** The learner or decision maker.\n",
    "2. **Environment:** The external systems with which the agent interacts.\n",
    "3. **State`(s)`:** A reprentation of the current situation of the agent.\n",
    "4. **Action`(a)`:** A set of all possible moves the agent can take.\n",
    "5. **Action Space:** The set of all valid actions in a given environment is often called `Action Space`.\n",
    "   1. ***Descrete Action Space:*** where only a finite number of moves are available for agents. EX. `Atari and GO`\n",
    "   2. ***Continuous Action Space:*** In continous action space actions are real-valued matrix- ex. `Robot`. \n",
    "6. **Reward`(r)`:** Feedback from the environment based on the action taken by agent. $r_t = R(s_t, a_t, s_{t+1}) \\text{ or } r_t = R(s_t), \\text{ or state-action pair } r_t = R(s_t,a_t).$\n",
    "7. **Return:** `Finite Horizontal undiscounted return` $\\to R(\\tau) = \\sum_{t=0}^T r_t.$ `Infinite Horizontal discounted return` $$\\to \\gamma \\in (0,1): R(\\tau) = \\sum_{t=0}^{\\infty} \\gamma^t r_t.$$ \n",
    "\n",
    "8. **Value Function`(V)`:** The expected long-term return with discount, as oppsed to short-term reward.\n",
    "9.  **Q-Value`(Q)`:** The expected utility of taking a given action in a given state and following a particular policy thereafter. \n",
    "10. **Trajectories($\\tau$):** A trajectory $\\tau$ is a sequence of states and actions in the world. $\\tau = (s_0, a_0, s_1, a_1, ...).$\n",
    "`The very first state of the world`, $s_0$, is randomly sampled from the start-state distribution, sometimes denoted by $\\rho_0:s_0 \\sim \\rho_0(\\cdot).$ State transitions are governend by the natural laws of the environment and depend only on the most recent action, $a_t$.\n",
    "    1. Deterministic: $s_{t+1} = f(s_t, a_t)$\n",
    "    2. Stochastic: $s_{t+1} \\sim P(\\cdot|s_t, a_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation:\n",
    "The Bellman equation is a fundamental concept in Reinforcement Learning that provides a recursive decomposition of the value function. it states that the value of a state under a particular policy $(\\pi)$ can be decomposed into the immediate reward  plus the discounted value of a subsequent state.\n",
    "\n",
    "***Bellman Equation for Value Function:***\n",
    "$$V^{\\pi}(s) = \\mathbb{E}[\\sum_{t\\geq 0} r+ \\gamma^{t}r_t \\mid s_o=s,\\pi ]$$\n",
    "\n",
    "***Bellman Equation for Q-value function:***\n",
    "$$Q^{\\pi}(s,a) = \\mathbb{E}[\\sum_{t\\geq 0} r+ \\gamma^{t}r_t \\mid s_o=s,a_0=a, \\pi ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Example:***\n",
    "```\n",
    "[(0,0),(0,1)]\n",
    "[(1,0),(1,1)]\n",
    "```\n",
    "- Start State:`(1,0)`\n",
    "- Goal State:`(0,1)`\n",
    "- Action: `Up, Right`\n",
    "- `r`: +1 for reaching the goal and -.1  for reach each move\n",
    "- $\\gamma$=.9\n",
    "- $\\pi$: always move up if possible otherwise move right\n",
    "\n",
    "***Value Function:*** $V^{\\pi}((1,0)) = \\mathbb{E}[\\sum_{t\\geq 0} r+ \\gamma^{t}V^{\\pi}((0,0)) \\mid s_o=(1,0),\\pi ]$\n",
    "1. $V^{\\pi}((0,0)) = [-.1 + .9\\times 1]= 0.81$\n",
    "2. $V^{\\pi}((1,0)) = [-.1 + .9\\times .81]= 0.629$\n",
    "\n",
    "The expected cumulative reward starting from state(1,0) = .0629\n",
    "\n",
    "***Q-Function:*** $Q^{\\pi}((1,0),Up) = \\mathbb{E}[\\sum_{t\\geq 0} r+ \\gamma^{t}Q^{\\pi}(s_t, a_t) \\mid s_o=(1,0),a_0=Up, \\pi ]$\n",
    "\n",
    "$Q^{\\pi}((1,0),Up) =  -.1 + \\gamma^{t}Q^{\\pi}((0,0), Right)\n",
    "   =-.1 + .9\\times 1 \\text{     }[\\text{assueme }  Q^{\\pi}((0,0), Right)=1; \\text{since it dirrectly reachs the goal}]$\n",
    "\n",
    "$Q^{\\pi}((1,0),Up) =  -.1+.9=.8$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Q-Value Function:\n",
    "The optimal Q-value function $Q^{*}$ is the maximum expected cumulative reward achieveable from a given (state, action) pair.\n",
    "$$Q^{*}(s,a) = max_{\\pi}\\mathbb{E}[\\sum_{t\\geq 0} \\gamma^{t}r_t \\mid s_o=s,a_0=a, \\pi ]$$ \n",
    "\n",
    "\n",
    "The bellman equation:\n",
    "$$Q^{*}(s,a) = \\mathbb{E}_{s^{\\prime}\\sim \\infty }[ r+ \\gamma \\max_{a^{\\prime}} Q^{*}(s^{\\prime},a^{\\prime}) \\mid s, a ]$$\n",
    "If the optimal state-action values for the next time-step $Q^{*}(s^{\\prime},a^{\\prime})$ are known, then the optimal strategy is to take the action that maximizes the expected value of $r+ \\gamma Q^{*}(s^{\\prime},a^{\\prime})$\n",
    "\n",
    "The optimal policy $\\pi^{*}$ corresponds to taking the best action in any state as specified by $Q^{*}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving Optimal Policy:\n",
    "Value Iteration algorithm--> Use bellman equation as an iterative update where $Q_{i}$ will converge to $Q^{*}$ as $i \\to \\infty$\n",
    "$$Q_{i+1}(s,a) = \\mathbb{E}[ r+ \\gamma \\max_{a^{\\prime}} Q^{*}(s^{\\prime},a^{\\prime}) \\mid s,a ]$$\n",
    "\n",
    "The problem with approach is not scalable. Must compute $Q(s,a)$ for every state-action pair. If the state is big , computationally infeasible to compute for entire state space.\n",
    "\n",
    "***`To solve the above problem, use a function estimator to estimate Q(s,a)` for example Neural network.*** We already learnt that if we have some complex function that we don't know but want to estimate a neural network is a good way to estimate it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning:\n",
    "Use a function approximator to estimate the action value function.\n",
    "$$ Q(s,a,\\theta) \\approx Q^{*}(s,a)$$\n",
    "- $\\theta$ is the function parameter(weights).\n",
    "\n",
    "$$Q^{*}(s,a) = \\mathbb{E}_{s^{\\prime}\\sim \\infty }[ r+ \\gamma \\max_{a^{\\prime}} Q^{*}(s^{\\prime},a^{\\prime}) \\mid s, a ] \\text{ want to find a Q-function that satishfies Bellman equation.} $$\n",
    "\n",
    "***Forward Pass:***<br>\n",
    "***`Loss function:`*** $L_i(\\theta_{i})=  \\mathbb{E}_{s,a\\sim p(.)}[(y_i - Q(s,a,\\theta_i))^2]$<br>\n",
    "where, $y_i=Q^{*}(s,a, \\theta_{i}) = \\mathbb{E}_{s^{\\prime}\\sim \\infty }[ r+ \\gamma \\max_{a^{\\prime}} Q(s^{\\prime},a^{\\prime}, \\theta_{i-1}) \\mid s, a ]$<br>\n",
    "This function will  iteratively try to make the Q-value close to the target value $(y_i)$, if Q-function corresponds to optimal $Q^*$ and optimal policy $r^*$.\n",
    "\n",
    "***Backward Pass:*** Gradient will update with respect to Q function parameters $\\theta$ <br>\n",
    "$$\\bigtriangledown_{\\theta_{i}}L_i (\\theta_{i}) = \\mathbb{E}_{s,a\\sim p(.), s^{\\prime}\\sim \\infty }[ r+ \\gamma \\max_{a^{\\prime}} Q(s^{\\prime},a^{\\prime}, \\theta_{i-1}) - Q(s,a, \\theta_{i}) \\bigtriangledown_{\\theta_{i}} Q(s,a,\\theta_i)  ]$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
