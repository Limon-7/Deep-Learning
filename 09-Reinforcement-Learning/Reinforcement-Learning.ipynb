{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1#Intro Of Reinfocement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### History:\n",
    "The `History` is the sequence of observations, actions or rewards.\n",
    "$$H_t=O_1, A_1, R_1, \\cdot \\cdot \\cdot, O_t, A_{t-1}, R_t$$\n",
    "1. All obserable variables up to time `t`\n",
    "2. What happens next depend on history:\n",
    "   1. The agent select action\n",
    "   2. The environment selects observations/rewards.\n",
    "#### State:\n",
    "State is the information used to determine what happens next. Generally, state is a function of the history. $S_t= f(H_{t})$\n",
    "\n",
    "#### Environment State $S^{e}_t$:\n",
    "The environment State is environment's private representation.\n",
    "1. Contain all necessary informations that determine what happens next`(observations, rewards)` from the environment perspective.\n",
    "2. Generally not visible to agent. For example, robots are walking on a road, it does not know the road's building process.\n",
    "3. If $S^{e}_t$ visible, it may contain irrelevant informations.\n",
    "\n",
    "#### Agent State $S^{a}_t$:\n",
    "The agent state is the agent's internal reprentation.\n",
    "1. We store some information which determines what action the agent will pick.\n",
    "2. It is our decision what information we will store and this informations use by RL algorithm\n",
    "3. It can be any function of history and the `Agent` has the full control over this function. $S^{a}_t=f(H_t)$\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information State`(Markov State)`:\n",
    "An information state contains all information from the history. The Markov Property basically says that the probability of next state($S_{t+1}\\mid S_t$) conditioned current state is the same of probability of the next state if showed all the previous states ($S_{t+1}\\mid S_1,\\cdot \\cdot \\cdot, S_t$).\n",
    "\n",
    "1. A state $S_t$ is Merkov if and only if $\\mathbb {P}[S_{t+1} \\mid S_t] = \\mathbb {P}[S_{t+1} | S_1, ..., S_t]$\n",
    "2. Future is `independent` for the past, given the `present`. $H_{1: t}\\to S_t \\to H_{t+1:\\infty}$\n",
    "3. Once the state is know, the history may be thrown away.\n",
    "4. The state is sufficient of the future.\n",
    "5. $S^{e}_{t} \\text { and } H_t$ is Markov.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fully Obserable Environements:\n",
    "Agent directly observes Environent State. Formally, this is a `Markov decision process(MDP)`\n",
    "$$O_t=S^{a}_t=S^{e}_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Partially Obserable Environment\n",
    "Agent indirectly observes environment.Formally, this is a `partially obserable Markov decision process(POMDP)` $S^{a}_t\\neq S^{e}_t$.\n",
    "\n",
    "- A robot with camera vision isnâ€™t told its absolute location\n",
    "- A trading agent only observes current prices\n",
    "- A poker playing agent only observes public cards.\n",
    "1. Agent must constract its own state reprentation.\n",
    "   1. Keep track complete histoy. $S^{a}_t=H_t$\n",
    "   2. Beliefs of environmental state. $S^{a}_t=(\\mathbb{P}[S^{e}_t=s_1],\\cdot \\cdot \\cdot, [S^{e}_t=s_n])$. This whole `vector` of probabilites defines the state.\n",
    "   3. Recurrent Neural Network. $S^{a}_t=\\sigma(S^{a}_{t-1}W_s+O_t W_0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Major Components of RL Agent:\n",
    "1. Policy\n",
    "2. Value Function\n",
    "3. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.Policy $\\pi$\n",
    "A strategy used by agent to determine the next action based on the current state. On the other a policy is rule used by an agent to decide what action to take. $a_t=\\mu (s_t)$ or $a_t=\\pi (.\\mid s_t)$\n",
    "1. A policy fully defines the behaviors of an agent\n",
    "2. MDP polices depend on the current state(not the history)\n",
    "3. Policies are stationary(time-independent)\n",
    "$$A_t\\sim \\pi(.|S_t), \\forall t>0 $$\n",
    "4. Given an MDP $\\mathcal {M} = \\langle \\mathcal{S,A,P,R,\\gamma} \\rangle $ and a policy $\\pi$\n",
    "   - The state sequence $S_1, S_2, \\cdots$ is a Markov process $\\langle \\mathcal{S,P^{\\pi}} \\rangle $\n",
    "   -  The state and Reward sequence $S_1,R_2, S_2, \\cdots$ is a Markov process $\\langle \\mathcal{S,P^{\\pi}, R^{\\pi}, \\gamma} \\rangle $\n",
    "   -  where\n",
    "$$\\mathcal {P^{\\pi}_{ss'}}=\\mathcal{\\sum_{a \\in A} \\pi(a|s)P^{a}_{ss'}}$$\n",
    "$$\\mathcal {R^{\\pi}_{s}}=\\mathcal{\\sum_{a \\in A} \\pi(a|s)R^{a}_{s}}$$\n",
    "\n",
    "###### Deterministic Policy: $\\pi(s)$\n",
    "maps each state to a single action with certaity. $\\pi: S \\to A$ and $s\\in S \\text{ and } a\\in A$. \n",
    "\n",
    "###### Stochastic Policy: $\\pi(a\\mid s)$\n",
    "maps each state to a probability distribution over actions.\n",
    "$$\\pi(a\\mid s)=\\mathbb P[A_t=a\\mid S_t=s]$$\n",
    "   1. Categorial Policy: Categorical Policy is used in descrete action spaces like the way for a classifier, so there will be a final linear layer that will give us logits for each action, followed by a `softmax` to convert the logit into probabilities. Log-Likelihood the last layer of probabilites as $p_{\\theta}(s)$. The log-likelihood for an action `a` can then optinin by the indexing into the vector. $\\log \\pi_{\\theta}(a\\mid s)=\\log [P_{\\theta}(s)]_a$\n",
    "   1. Gaussian Policy: A multivariate Gaussian distribution(normal distribution) is described by a mean vector $(\\mu)$ and covarience matrix $(\\sum)$. `In Gaussian distribution matrix has entries on the diagoanl, therby we can reopresent it by a vector.`\n",
    "      1. First way: There is a single vector of log standard deviations, $\\log \\sigma$ which is not a function of state, rather a standalone parameter.\n",
    "      2. Second way: There is a neural network that maps from states to log standard deviations, $\\log \\sigma_{\\theta}(s)$. It may optionally share some layers with the mean network.\n",
    "      3. **Sampling:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.Value function(On policy Value function) \n",
    "The value function`(State Value function)` representing the expected culumatiive reward that an agent can acheive starting from a given `state(S)` following a particular policy($\\pi$) from `state(S)`.\n",
    "\n",
    "$$V^{\\pi}(s) = \\mathbb{E}[\\sum_{t\\geq 0} \\gamma^{t}r_t \\mid s_o=s,\\pi ]$$\n",
    "$$V^{\\pi}(s) = \\mathbb{E_{\\pi}}[R_{t+1} + \\gamma R_{t+2} +\\gamma^{2}R_{t+3}+ \\cdot \\cdot \\mid s_o=s,\\pi ]$$\n",
    "where,\n",
    "- $\\mathbb{E}$ denotes the expected value under the policy $\\pi$\n",
    "- $\\sum_{t\\geq 0} \\gamma^{t}r_t$ is the total discounted reward from tim step `t`\n",
    "- $\\gamma$ is the discounted factor, $0\\leq \\gamma \\le1$\n",
    "- $r_t$ is the reward received after taking an action at time step `t`\n",
    "- $s_0$ is the state at time `t`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3#Model\n",
    "Model isn't the Environment itself, but it predicts what the environment will do next to help make a plan. It can be done by two ways-\n",
    "1. Transitions: $\\mathcal P $ predicts the next state or predicts the dynamics of the environment. For example, a helicapter shifts the right angle and wind comes from the same directions, the helicapter will move towards the oppsotion of winds, this is dynamic environment. So, model should be dynamics. $\\mathcal P^{a}_{ss^{\\prime}}=\\mathbb P[S^{\\prime}=s^{\\prime}\\mid S=s, A=a]$\n",
    "   \n",
    "2. Reward $\\mathcal R$: Predict the next-immediate return. For example, the helicapter will get rewarded 1 points for staying alive. $\\mathcal R^{a}_{s}=\\mathbb E[R \\mid S=s, A=a]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorizing RL agents:\n",
    "1. Value Based:\n",
    "   1. No policy\n",
    "   2. Value Function\n",
    "2. Policy Based\n",
    "   1. Policy\n",
    "   2. No value Function\n",
    "3. Actor Critic\n",
    "   1. Policy\n",
    "   2. Valu Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning and Planning\n",
    "There are two fundamental Problem:\n",
    "\n",
    "1. ***Reinforcement Learning:***\n",
    "   - The environment is `initially unknown`.\n",
    "   - The agent interacts with the environment and figure out the best way to behave with this environment and maximize the reward. For example- `Atari` game.\n",
    "\n",
    "\n",
    "2. ***Planning:***\n",
    "   - A model of the `environment is known`.\n",
    "   - The agent performs computations with its model(without external iteractions). For example, we tell all rules of the certain games to Agent.\n",
    "   - So agent will take times for computation for the best reward.\n",
    "\n",
    "***Note:*** To work with `RL` first learn how environment is works, then do planning. These two things are linked together but they are seperated by problem setup.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expolration and Exploitation\n",
    "**Exploration:** Expolaration finds more information about the environment.\n",
    "\n",
    "**Exploitation:** Exploitation exploits known information to maximize reward.\n",
    "\n",
    "**Example:** `Resturant Selection`\n",
    "1. `Expoloration:` Try a new resturant.\n",
    "2. `Exploitation:` Go to your favourite resturant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2#Markov Decision Process`(MDPs)`\n",
    "Markove decision process formally describe an environment for reinforcement learning.\n",
    "- Environment is fully observable- all relevent informations of the environment are presented to `Agent`\n",
    "- The current state completely characterises the process.\n",
    "- all problems can be formalized as `MDPs`.\n",
    "  - Optimal control deals with continuos MDPs- how an octopas will swim in fluid.\n",
    "  - Partially obserable problems can be converted into MDPs\n",
    "  - `Bandits(Expoloration Expolitation dilemma in RL-> agents will get set of actions and get some reward for that action)` are MDPs with one state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Property:\n",
    "A state $\\mathcal S_t$ is Merkov if and only if $$\\mathbb {P}[S_{t+1} \\mid S_t] = \\mathbb {P}[S_{t+1} | S_1, ..., S_t]$$\n",
    "\n",
    "1. Future is `independent` for the past, given the `present`. $H_{1: t}\\to S_t \\to H_{t+1:\\infty}$\n",
    "2. Once the state is know, the history may be thrown away.\n",
    "3. The state is sufficient of the future.\n",
    "4. $S^{e}_{t} \\text { and } H_t$ is Markov."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Transition Matrix $\\mathcal P$\n",
    "$$\\mathcal {P_{{ss^{\\prime}}}}= \\mathbb{P}[S_{t+1}=s^{\\prime}\\mid S_t=s]$$\n",
    "$$\\mathcal{p} = \\begin{bmatrix} \\mathcal{p_{11}} & \\ldots& \\mathcal{p_{1n}}\\\\\n",
    "\\vdots & & \\\\\n",
    "\\mathcal{p_{n1}}& \\ldots& \\mathcal{p_{nn}}\n",
    "\\end{bmatrix} $$\n",
    "`where each row of matrix sums to 1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Process $\\langle \\mathcal{S,P, R, \\gamma} \\rangle$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return $\\mathcal {G_t}$:\n",
    "Total discounted reward from time-step t is called Return.\n",
    "$$\\mathcal{G_t}=R_{t+1}+ \\gamma R_{t+2} + \\gamma^{2} R_{t+3}+\\cdots = \\sum^{\\infty}_{k=0} \\gamma^{k}R_{t+k+1} $$\n",
    "\n",
    "- This values immediate reward above delyed reward.\n",
    "  - $\\gamma \\text{ close to 0 leads to }$ `myopic` evaluation.\n",
    "  -  $\\gamma \\text{ close to 1 leads to }$ `far-sighted` evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for t=1, k=T-2\n",
    "$$\\mathcal{G_1}=R_{2}+ \\gamma R_{3} + \\gamma^{2} R_{4}+\\cdots = \\sum^{\\infty}_{T\\geq 2} \\gamma^{T-2}R_{T} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why discount?\n",
    "Most Markov reward and decision processs are discounted.\n",
    "1. Mathematically convenient to discount rewards\n",
    "2. Avoids infinite returns in cyclic Markov processes\n",
    "3. Uncertainty about the future may not be fully represented\n",
    "4. If the reward is financial, immediate rewards may earn more interest than delayed rewards\n",
    "5. Animal/human behaviour shows preference for immediate reward\n",
    "6. It is sometimes possible to use undiscounted Markov reward\n",
    "processes (i.e. **$\\gamma = 1$**), e.g. if all sequences terminate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Function `V(s)`:\n",
    "The value function `V(s)` gives the long-term values of state `s`. The state value function V(s) of an MRP is the **expected return $(\\mathbb{E})$** starting from state s.\n",
    "$$\\mathcal v(s)=\\mathbb {E}[G_t| S_t=s]$$\n",
    "$$\\mathcal{G_1}=R_{2}+ \\gamma R_{3} + \\gamma^{2} R_{4}+\\cdots = \\sum^{\\infty}_{T\\geq 2} \\gamma^{T-2}R_{T} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation for MRPs:\n",
    "The Bellman equation is a fundamental concept in Reinforcement Learning that provides a recursive decomposition of the value function. It states that the value of a state under a particular policy $(\\pi)$ can be decomposed into the `immediate reward` $R_{t+1}$  `plus the discounted value of a subsequent state` $\\gamma \\mathcal{v(S_{t+1})}$.\n",
    "\n",
    "$$\\mathcal v(s)=\\mathbb {E}[G_t| S_t=s]$$\n",
    "\n",
    "$$=\\mathbb {E}[R_{t+1}+ \\gamma R_{t+2} + \\gamma^{2} R_{t+3}+\\cdots | S_t=s ]$$\n",
    "\n",
    "$$=\\mathbb {E}[R_{t+1}+ \\gamma (R_{t+2} + \\gamma R_{t+3}+\\cdots) | S_t=s ]$$\n",
    "$$=\\mathbb {E}[R_{t+1}+ \\gamma \\mathcal{G_{t+1}} | S_t=s ]$$\n",
    "$$=\\mathbb {E}[R_{t+1}+ \\gamma \\mathcal{v_{(S_{t+1})}} | S_t=s ]$$\n",
    "\n",
    "***`Bellman Equation Matrix Form:`***\n",
    "$$\\begin{bmatrix} v(1)\\\\\\vdots\\\\v(n)\\end{bmatrix} =\n",
    "\\begin{bmatrix} R_1\\\\\\vdots\\\\R_n\\end{bmatrix}\n",
    "+\n",
    "\\gamma \\begin{bmatrix} \\mathcal{p_{11}} & \\ldots& \\mathcal{p_{1n}}\\\\\n",
    "\\vdots & & \\\\\n",
    "\\mathcal{p_{n1}}& \\ldots& \\mathcal{p_{nn}}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} v(1)\\\\\\vdots\\\\v(n)\\end{bmatrix}\n",
    " $$\n",
    " - It is linear equation.\n",
    " - It can be solved directly:\n",
    "  $$\\mathcal{v}=\\mathcal{R+\\gamma Pv}$$\n",
    "  $$\\mathcal{R}=\\mathcal{(1-\\gamma P)v}$$\n",
    "  $$\\mathcal{v}=\\mathcal{(1-\\gamma P)^{-1}R}$$\n",
    "- Computational complexity is $O(n^3)\\text{ for n states}$\n",
    "- Dirrect solution only possible for small MRPs\n",
    "- Iterative methods for large MRPs\n",
    "  - Dynamic Programming\n",
    "  - Monte-Carlo evaluation\n",
    "  - Temporal-Difference learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Process $\\mathcal (S, A, R, P, \\gamma)$\n",
    "A `Markov decision process(MDP)` is `Markov Reward Process` with decisions. It is an environment in which all states are Markov.\n",
    "\n",
    "- `S:` set of possible state\n",
    "\n",
    "- `A:` a finite set of possible action\n",
    "  \n",
    "- `P:` state transition probability matrix $\\mathcal (P)$ : distribution over next state given `(state, action)` pair $\\mathcal P(s^{\\prime}\\mid s,a)$  or $\\mathcal {P^{a}_{ss^{\\prime}}} = \\mathbb P [S_{t+1}=s'| S_t=s, A_t=a]$ probablility from `state s` to **$s^{\\prime}$**\n",
    "  - $\\mathcal P(s^{\\prime}\\mid s,a)$ or ${\\mathcal P^{a}_{ss^{\\prime}}} = \\mathbb P [S_{t+1}=s'| S_t=s, A_t=a]$\n",
    "  \n",
    "- **`Reward: `** $\\mathcal R$ is reward function, $\\mathcal {R^{a}_{s}}=\\mathbb E [R_{t+1}| S_t = s, A_t=a]$ \n",
    "- $\\gamma$: discounted factor; $\\gamma \\in [0,1]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy: $\\pi$\n",
    "A strategy used by agent to determine the next action based on the current state. On the other a policy is rule used by an agent to decide what action to take. $a_t=\\mu (s_t)$ or $a_t=\\pi (.\\mid s_t)$\n",
    "1. A policy fully defines the behaviors of an agent\n",
    "2. MDP polices depend on the current state(not the history)\n",
    "3. Policies are stationary(time-independent)\n",
    "$$A_t\\sim \\pi(.|S_t), \\forall t>0 $$\n",
    "4. Given an MDP $\\mathcal {M} = \\langle \\mathcal{S,A,P,R,\\gamma} \\rangle $ and a policy $\\pi$\n",
    "   - The state sequence $S_1, S_2, \\cdots$ is a Markov process $\\langle \\mathcal{S,P^{\\pi}} \\rangle $\n",
    "   -  The state and Reward sequence $S_1,R_2, S_2, \\cdots$ is a Markov process $\\langle \\mathcal{S,P^{\\pi}, R^{\\pi}, \\gamma} \\rangle $\n",
    "   -  where\n",
    "$$\\mathcal {P^{\\pi}_{ss'}}=\\mathcal{\\sum_{a \\in A} \\pi(a|s)P^{a}_{ss'}}$$\n",
    "$$\\mathcal {R^{\\pi}_{s}}=\\mathcal{\\sum_{a \\in A} \\pi(a|s)R^{a}_{s}}$$\n",
    "- Transition Probability will be average of all probabity of that state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Function(State-value function) in MDPs $(v_{\\pi}(s))$:\n",
    "The state-value function $v_{\\pi}(s)$ of an `MDP` is the expected return starting from `state s` and then following policy $\\pi$\n",
    "$$\\mathcal v_{\\pi}(s)=\\mathbb {E_{\\pi}}[G_t| S_t=s]$$\n",
    "\n",
    "**Bellman Expectation equation for value function in MDPs**\n",
    "$$v_{\\pi}(s)=\\mathbb {E_{\\pi}}[R_{t+1}+ \\gamma \\mathcal{v_{\\pi}(S_{t+1})} | S_t=s ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action-value function in MDPs $(q_{\\pi}(s,a))$\n",
    "The action-value function $(q_{\\pi}(s,a))$ is the expected return starting from state s, taking action s, and then folowing the policy $\\pi$\n",
    "$$\\mathcal q_{\\pi}(s,a)=\\mathbb {E_{\\pi}}[G_t| S_t=s, A_t=a]$$\n",
    "\n",
    "**Bellman Expectation equation for action-value function in MDPs**\n",
    "$$q_{\\pi}(s,a)=\\mathbb {E_{\\pi}}[R_{t+1}+ \\gamma \\mathcal{q_{\\pi}(S_{t+1},A_{t+1})} | S_t=s, A_t=a ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relationship between $v_{\\pi}(s) \\text{ vs } q_{\\pi}(s,a)$ :\n",
    "From s to subsequent state by taking some action the action value\n",
    "$$v_{\\pi}(s)=\\sum_{a\\in A}\\pi(a|s)q_r(s,a)$$\n",
    "$$q_{\\pi}(s,a)=\\mathcal {R^{a}_{s}+ \\gamma \\sum_{s'\\in S}P^{a}_{ss'}v_r(s')}$$\n",
    "\n",
    "$$v_{\\pi}(s)=\\mathcal{ \\sum_{a\\in A} \\pi(a|s)(R^{a}_{s}+ \\gamma \\sum_{s'\\in S}P^{a}_{ss'}v_r(s'))}$$\n",
    "$$\\gamma \\sum_{s'\\in S}P^{a}_{ss'}v_r(s') =\\gamma {(\n",
    "    P^{a}_{s's'_1}v_r(s'_1)+ P^{a}_{s'_1 s'_2}v_r(s'_2) \n",
    "    )}$$\n",
    "\n",
    "$$q_{\\pi}(s,a)=\\mathcal {R^{a}_{s}+ \\gamma \\sum_{s'\\in S}P^{a}_{ss'}\\sum_{a'\\in A}\\pi(a'|s')q_r(s',a')}$$\n",
    "\n",
    "\n",
    "**Bellman Equation Matrix Form:**\n",
    "$$v_r=\\mathcal{R^{\\pi} +\\gamma P^r v_r}$$\n",
    "$$v_r=\\mathcal{(\\gamma P^r )^{-1} R^{\\pi} }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimal  State-Value function( $v_{*}(s)$ ) and Action-Value function( $q_{*}(s,a)$ ):\n",
    "***State-Value function( $v_{*}(s)$ ):*** The optimal state-value function is the maximum value function over all policies.\n",
    "$$\\mathcal{v_{*}(s)=\\max_{\\pi}v_r(s)}$$\n",
    "\n",
    "***Action-Value function( $q_{*}(s,a)$ ):*** The optimal action-value function is the maximum action-value function over all policies.\n",
    "$$\\mathcal{q_{*}(s,a)=\\max_{\\pi}q_r(s,a)}$$\n",
    "- Optimal valu function specifies the best possible perfomance in the MDP.\n",
    "- An MDP is solved when we get optimal value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Policy:\n",
    "$$\\pi\\geq\\pi^{\\prime} \\text{ if } v_{\\pi}(s)\\geq v_{\\pi^{\\prime}}(s), \\forall s$$\n",
    "For any Markov Decision Process\n",
    "- There exists an optimal policy $\\pi_*$ that is better than or equal to all other policies,$\\pi_* \\geq \\pi, \\forall \\pi$.\n",
    "- All optimal policies achieve the optimal value function, $v_{Ï€_âˆ—} (s) = v_{*}(s)$\n",
    "- All optimal policies achieve the optimal action-value function, $q_{Ï€_âˆ—} (s,a) = q_{*}(s,a)$\n",
    "\n",
    "***Find an optimal policy:*** An optimal policy can be found by maximising over $q_{*}(s,a)$\n",
    "\n",
    "$$\\pi_{*}(a|s)= \\begin{cases}\n",
    "1 &\\quad \\text{if } \\mathcal{a=\\argmax_{a\\in A} q_{*}(s,a)}\\\\\n",
    "0 &\\quad \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "There is always a deterministic optimal policy for any `MDP`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bellman Optimality Equation\n",
    "***Optimal Value Function: $v_{*}(s)$***\n",
    "$$v_{*}(s)=\\max_{a} q_{*}(s,a)$$\n",
    "$$\\mathcal {q_{*}(s,a)=R^{a}_{s}+\\gamma\\sum_{s'\\in S}P^{a}_{ss'}v_{*}(s')}$$\n",
    "\n",
    "$$\\mathcal {v_{*}(s)=\\max_{a}R^{a}_{s}+\\gamma\\sum_{s'\\in S}P^{a}_{ss'}v_{*}(s')}$$\n",
    "\n",
    "***Optimal Value Function: $q_{*}(s,a)$***\n",
    "$$\\mathcal {q_{*}(s,a)=R^{a}_{s}+\\gamma\\sum_{s'\\in S}P^{a}_{ss'}\\max_{a'} q_{*}(s',a')}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ways to solve Bellman Optimality equation:\n",
    "Bellman Optiality equation is non-linear.\n",
    "- No close form solution\n",
    "- Iterative Solution Methods:\n",
    "  - Value Iteration\n",
    "  - Policy Learning\n",
    "  - Q-learning\n",
    "  - Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extention to MDPs\n",
    "1. Infinite and continuous MDPs\n",
    "2. Partially obserable MDPs(POMDPs)\n",
    "   1. Belief State\n",
    "   2. Reduction of POMDPs\n",
    "3. undiscounted, average reward MDPs\n",
    "   1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3#Dynamic Programming:\n",
    "The term dynamic programming refers to a collection of algorithoms that can be used to compute optimal policies given a perfect model of the environment as a Markov Decision Process(MDP)\n",
    "1. Policy Evaluation(Prediction)\n",
    "2. Policy Improvement\n",
    "3. Policy Iteration\n",
    "4. Value Iteration\n",
    "5. Aynchronous DP\n",
    "6. Generalized Policy Iteration\n",
    "7. Efficiency in Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4#Monte Carlo Methods\n",
    "1. Monte Carlo Prediction\n",
    "2. Monte Carlo Estimation of action-values\n",
    "3. Monte Cralo Control\n",
    "4. Monte Cralo Control without Exploring Star\n",
    "5. Off-Policy Prediction vai importance sampling\n",
    "6. Incremental Implementation\n",
    "7. Off-Policy Monte carlo Control\n",
    "8. Discounting-aware Importance Sampling\n",
    "9. Per-Decision Importance Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5#Temporal Difference(TD)\n",
    "1. TD prediction\n",
    "2. Advantage of TD Prediction Methods\n",
    "3. Optimality of TD(0)\n",
    "4. Sarsa: On-policy TD control\n",
    "5. Q-Learning off-policy TD control\n",
    "6. Expected Sarsa\n",
    "7. Maximizing Bias and Double Learning\n",
    "8. Games, Afterstates and other speacial cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6#n-step Bootsatrapping\n",
    "1. n-step TD Prediction\n",
    "2. n-step Sarsa\n",
    "3. n-steap off-policy learning\n",
    "4. per-decision Methods with control variates\n",
    "5. Off-policy learning without importance: the n-step Tree Backup Algorithm\n",
    "6. A-unifying Algorithm: n-step Q()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning(RL)\n",
    "Reinforcement learning is a type of machine learning where an agent learns to take decisions by performing actions in  an environment to maximize cumulative reward.\n",
    "\n",
    "#### Terms:\n",
    "1. **Agent:** The learner or decision maker.\n",
    "2. **Environment:** The external systems with which the agent interacts.\n",
    "3. **State`(s)`:** A reprentation of the current situation of the agent.\n",
    "4. **Action`(a)`:** A set of all possible moves the agent can take.\n",
    "5. **Action Space:** The set of all valid actions in a given environment is often called `Action Space`.\n",
    "   1. ***Descrete Action Space:*** where only a finite number of moves are available for agents. EX. `Atari and GO`\n",
    "   2. ***Continuous Action Space:*** In continous action space actions are real-valued matrix- ex. `Robot`. \n",
    "6. **Reward`(r)`:** Feedback from the environment based on the action taken by agent. $r_t = R(s_t, a_t, s_{t+1}) \\text{ or } r_t = R(s_t), \\text{ or state-action pair } r_t = R(s_t,a_t).$\n",
    "7. **Return:** `Finite Horizontal undiscounted return` $\\to R(\\tau) = \\sum_{t=0}^T r_t.$ `Infinite Horizontal discounted return` $$\\to \\gamma \\in (0,1): R(\\tau) = \\sum_{t=0}^{\\infty} \\gamma^t r_t.$$ \n",
    "\n",
    "8. **Value Function`(V)`:** The expected long-term return with discount, as oppsed to short-term reward.\n",
    "9.  **Q-Value`(Q)`:** The expected utility of taking a given action in a given state and following a particular policy thereafter. \n",
    "10. **Trajectories($\\tau$):** A trajectory $\\tau$ is a sequence of states and actions in the world. $\\tau = (s_0, a_0, s_1, a_1, ...).$\n",
    "`The very first state of the world`, $s_0$, is randomly sampled from the start-state distribution, sometimes denoted by $\\rho_0:s_0 \\sim \\rho_0(\\cdot).$ State transitions are governend by the natural laws of the environment and depend only on the most recent action, $a_t$.\n",
    "    1. Deterministic: $s_{t+1} = f(s_t, a_t)$\n",
    "    2. Stochastic: $s_{t+1} \\sim P(\\cdot|s_t, a_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bellman Equation:\n",
    "The Bellman equation is a fundamental concept in Reinforcement Learning that provides a recursive decomposition of the value function. it states that the value of a state under a particular policy $(\\pi)$ can be decomposed into the immediate reward  plus the discounted value of a subsequent state.\n",
    "\n",
    "***Bellman Equation for Value Function:***\n",
    "$$V^{\\pi}(s) = \\mathbb{E}[\\sum_{t\\geq 0} r+ \\gamma^{t}r_t \\mid s_o=s,\\pi ]$$\n",
    "\n",
    "***Bellman Equation for Q-value function:***\n",
    "$$Q^{\\pi}(s,a) = \\mathbb{E}[\\sum_{t\\geq 0} r+ \\gamma^{t}r_t \\mid s_o=s,a_0=a, \\pi ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Example:***\n",
    "```\n",
    "[(0,0),(0,1)]\n",
    "[(1,0),(1,1)]\n",
    "```\n",
    "- Start State:`(1,0)`\n",
    "- Goal State:`(0,1)`\n",
    "- Action: `Up, Right`\n",
    "- `r`: +1 for reaching the goal and -.1  for reach each move\n",
    "- $\\gamma$=.9\n",
    "- $\\pi$: always move up if possible otherwise move right\n",
    "\n",
    "***Value Function:*** $V^{\\pi}((1,0)) = \\mathbb{E}[\\sum_{t\\geq 0} r+ \\gamma^{t}V^{\\pi}((0,0)) \\mid s_o=(1,0),\\pi ]$\n",
    "1. $V^{\\pi}((0,0)) = [-.1 + .9\\times 1]= 0.81$\n",
    "2. $V^{\\pi}((1,0)) = [-.1 + .9\\times .81]= 0.629$\n",
    "\n",
    "The expected cumulative reward starting from state(1,0) = .0629\n",
    "\n",
    "***Q-Function:*** $Q^{\\pi}((1,0),Up) = \\mathbb{E}[\\sum_{t\\geq 0} r+ \\gamma^{t}Q^{\\pi}(s_t, a_t) \\mid s_o=(1,0),a_0=Up, \\pi ]$\n",
    "\n",
    "$Q^{\\pi}((1,0),Up) =  -.1 + \\gamma^{t}Q^{\\pi}((0,0), Right)\n",
    "   =-.1 + .9\\times 1 \\text{     }[\\text{assueme }  Q^{\\pi}((0,0), Right)=1; \\text{since it dirrectly reachs the goal}]$\n",
    "\n",
    "$Q^{\\pi}((1,0),Up) =  -.1+.9=.8$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Q-Value Function:\n",
    "The optimal Q-value function $Q^{*}$ is the maximum expected cumulative reward achieveable from a given (state, action) pair.\n",
    "$$Q^{*}(s,a) = max_{\\pi}\\mathbb{E}[\\sum_{t\\geq 0} \\gamma^{t}r_t \\mid s_o=s,a_0=a, \\pi ]$$ \n",
    "\n",
    "\n",
    "The bellman equation:\n",
    "$$Q^{*}(s,a) = \\mathbb{E}_{s^{\\prime}\\sim \\infty }[ r+ \\gamma \\max_{a^{\\prime}} Q^{*}(s^{\\prime},a^{\\prime}) \\mid s, a ]$$\n",
    "If the optimal state-action values for the next time-step $Q^{*}(s^{\\prime},a^{\\prime})$ are known, then the optimal strategy is to take the action that maximizes the expected value of $r+ \\gamma Q^{*}(s^{\\prime},a^{\\prime})$\n",
    "\n",
    "The optimal policy $\\pi^{*}$ corresponds to taking the best action in any state as specified by $Q^{*}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving Optimal Policy:\n",
    "Value Iteration algorithm--> Use bellman equation as an iterative update where $Q_{i}$ will converge to $Q^{*}$ as $i \\to \\infty$\n",
    "$$Q_{i+1}(s,a) = \\mathbb{E}[ r+ \\gamma \\max_{a^{\\prime}} Q^{*}(s^{\\prime},a^{\\prime}) \\mid s,a ]$$\n",
    "\n",
    "The problem with approach is not scalable. Must compute $Q(s,a)$ for every state-action pair. If the state is big , computationally infeasible to compute for entire state space.\n",
    "\n",
    "***`To solve the above problem, use a function estimator to estimate Q(s,a)` for example Neural network.*** We already learnt that if we have some complex function that we don't know but want to estimate a neural network is a good way to estimate it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning:\n",
    "Use a function approximator to estimate the action value function.\n",
    "$$ Q(s,a,\\theta) \\approx Q^{*}(s,a)$$\n",
    "- $\\theta$ is the function parameter(weights).\n",
    "\n",
    "$$Q^{*}(s,a) = \\mathbb{E}_{s^{\\prime}\\sim \\infty }[ r+ \\gamma \\max_{a^{\\prime}} Q^{*}(s^{\\prime},a^{\\prime}) \\mid s, a ] \\text{ want to find a Q-function that satishfies Bellman equation.} $$\n",
    "\n",
    "***Forward Pass:***<br>\n",
    "***`Loss function:`*** $L_i(\\theta_{i})=  \\mathbb{E}_{s,a\\sim p(.)}[(y_i - Q(s,a,\\theta_i))^2]$<br>\n",
    "where, $y_i=Q^{*}(s,a, \\theta_{i}) = \\mathbb{E}_{s^{\\prime}\\sim \\infty }[ r+ \\gamma \\max_{a^{\\prime}} Q(s^{\\prime},a^{\\prime}, \\theta_{i-1}) \\mid s, a ]$<br>\n",
    "This function will  iteratively try to make the Q-value close to the target value $(y_i)$, if Q-function corresponds to optimal $Q^*$ and optimal policy $r^*$.\n",
    "\n",
    "***Backward Pass:*** Gradient will update with respect to Q function parameters $\\theta$ <br>\n",
    "$$\\bigtriangledown_{\\theta_{i}}L_i (\\theta_{i}) = \\mathbb{E}_{s,a\\sim p(.), s^{\\prime}\\sim \\infty }[ r+ \\gamma \\max_{a^{\\prime}} Q(s^{\\prime},a^{\\prime}, \\theta_{i-1}) - Q(s,a, \\theta_{i}) \\bigtriangledown_{\\theta_{i}} Q(s,a,\\theta_i)  ]$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
