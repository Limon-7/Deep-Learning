{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization:\n",
    "Regularization is one of the ways to improve our model to work on unseen data by ignoring the less important features. Regularization minimizes the validation loss and tries to improve the accuracy of the model. It avoids overfitting by adding a penalty to the model with high variance, thereby shrinking the beta coefficients to zero.\n",
    "1. L2 Regularization\n",
    "2. L1 Regularization\n",
    "3. Elastic Net Regularization\n",
    "4. Dropout Regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. L2 Regularization:\n",
    "\n",
    "It adjusts models with overfitting or underfitting by adding a penalty equivalent to the sum of the squares of the magnitudes of the coefficients. Ridge regression never reaches zero.\n",
    "\n",
    "    $Loss = L = 1/N * Sum(Li(f(X[i],W),Y[i])) +  λ * Σ|θ^2|$\n",
    "     0<λ<infinity\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. L1 Regularization:\n",
    "\n",
    "Modifies overfitted or under-fitted models by adding a penalty equivalent to the sum of the absolute values ​​of the coefficients. Lasso regression also performs coefficient minimization, but instead of squaring the magnitudes of the coefficients, it takes the actual values ​​of the coefficients. This means that the sum of the coefficients can also be 0 because there are negative coefficients. So, Lasso regression helps to reduce the overfitting in the model as well as feature selection.\n",
    "\n",
    "    $Loss = L = 1/N * Sum(Li(f(X[i],W),Y[i])) +  λ * Σ|θ|.// 0<lambda<infinite$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Elastic Net Regularization:\n",
    "\n",
    "Elastic Net combines L1 and L2 regularization by adding a mixture of both penalty terms to the objective function. It offers a balance between the feature selection capability of L1 regularization and the coefficient shrinkage of L2 regularization.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Dropout Regularization:\n",
    "\n",
    "The Dropout algorithm is a regularization technique commonly used in deep learning models, particularly in neural networks. It helps to prevent overfitting by randomly dropping out or deactivating a certain percentage of neurons or connections during the training phase. This dropout process forces the network to learn more robust and generalizable features. While training, dropout is implemented by only keeping a neuron active with some probability p\n",
    " (a hyperparameter generally .2 to .5 ), or setting it to zero otherwise.\n",
    "\n",
    "1. During training:\n",
    "\n",
    "    1. For each training example, during the forward pass, randomly set a fraction (dropout rate) of the neurons or connections to zero. This means these neurons or connections will be temporarily ignored and have no contribution to the subsequent layers' computations.\n",
    "    2. Perform the forward pass as usual, computing the output of the model.\n",
    "    During the backward pass, only update the weights of the active neurons or connections (the ones that were not dropped out).\n",
    "\n",
    "2. During testing or inference:\n",
    "\n",
    "    1. No dropout is applied. Instead, the full network is used to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.63092928 3.67980665]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np;\n",
    "\"\"\" \n",
    "Inverted Dropout: Recommended implementation example.\n",
    "We drop and scale at train time and don't do anything at test time.\n",
    "\"\"\"\n",
    "\n",
    "p = 0.5 # probability of keeping a unit active. higher = less dropout\n",
    "W1= np.random.rand(2)\n",
    "W2= np.random.rand(2)\n",
    "W3= np.random.rand(2)\n",
    "b1= np.random.rand(2)\n",
    "b2= np.random.rand(2)\n",
    "b3= np.random.rand(2)\n",
    "\n",
    "x= np.random.rand(2)\n",
    "\n",
    "def train_step(X):\n",
    "  # forward pass for example 3-layer neural network\n",
    "  H1 = np.maximum(0, np.dot(W1, X) + b1)\n",
    "  U1 = (np.random.rand(*H1.shape) < p) / p # first dropout mask. Notice /p!\n",
    "  H1 *= U1 # drop!\n",
    "  H2 = np.maximum(0, np.dot(W2, H1) + b2)\n",
    "  U2 = (np.random.rand(*H2.shape) < p) / p # second dropout mask. Notice /p!\n",
    "  H2 *= U2 # drop!\n",
    "  out = np.dot(W3, H2) + b3\n",
    "  return out\n",
    "  # backward pass: compute gradients... (not shown)\n",
    "  # perform parameter update... (not shown)\n",
    "  \n",
    "def predict(X):\n",
    "  # ensembled forward pass\n",
    "  H1 = np.maximum(0, np.dot(W1, X) + b1) # no scaling necessary\n",
    "  H2 = np.maximum(0, np.dot(W2, H1) + b2)\n",
    "  out = np.dot(W3, H2) + b3\n",
    "\n",
    "print(train_step(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
