{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid activation function\n",
    "Advantage:\n",
    "1. Squashes numbers to range [0,1]\n",
    "2. Historically popular since they have nice interpretation as a saturating “firing rate” of a neuron\n",
    "\n",
    "Disadvantage:\n",
    "1. Saturated neurons “kill” the gradients\n",
    "    - Gradients are in most cases near 0 (Big values/small values), that kills the updates if the graph/network are large.\n",
    "2. Sigmoid outputs are not zero-centered\n",
    "    - Didn't produce zero-mean data.\n",
    "3. exp() is a bit compute expensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmooid_function(x):\n",
    "    sig=1/(1+math.exp(-x)) #1/(1+np.exp(-x))\n",
    "    # print(\"Sigmoid Function:\", sig)\n",
    "    return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999546021312976"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmooid_function(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh activation function\n",
    "Advantage:\n",
    "1. Squashes numbers to range [-1,1]\n",
    "2. zero centered (nice)\n",
    "\n",
    "Disadvantage:\n",
    "1. still kills gradients when saturated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_function(x):\n",
    "    thf= math.tanh(x)\n",
    "    # print(thf)\n",
    "    return thf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh_function(-1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relu activation function\n",
    "`max(0.0, x)`\n",
    "\n",
    "Advantage:\n",
    "- Does not saturate (in +region)\n",
    "- Does not kill the gradient.\n",
    "    - Only small values that are killed. Killed the gradient in the half.\n",
    "- Very computationally efficient\n",
    "- Converges much faster than sigmoid/tanh in practice (e.g. 6x)\n",
    "- Actually more biologically plausible than sigmoid.\n",
    "\n",
    "Disadvantage:\n",
    "- Not zero-centered output\n",
    "- If weights aren't initialized good, maybe 75% of the neurons will be dead and thats a waste computation. But its still works. This is an active area of research to optimize this.\n",
    "    - To solve the issue mentioned above, people might initialize all the biases by 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_function(x):\n",
    "\trelu=max(0.0, x)\n",
    "\t# print(\"Relu Function:\", relu)\n",
    "\treturn relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu_function(-10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky Relu function:\n",
    "    - Does not saturate\n",
    "    - Computationally efficient\n",
    "    - Converges much faster than sigmoid/tanh in practice! (e.g. 6x)\n",
    "    - will not “die”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu_function(x):\n",
    "    if x>0:\n",
    "        return x\n",
    "    else:\n",
    "        return .01*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaky_relu_function(-100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expotential Relu Function:\n",
    "    - All benefits of ReLU\n",
    "    - Closer to zero mean outputs\n",
    "    - Negative saturation regime compared with Leaky ReLU adds some robustness to noise\n",
    "    - Computation requires exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_relu_function(x):\n",
    "    '''\n",
    "    '''\n",
    "    if x>0:\n",
    "        return x\n",
    "    else:\n",
    "        return .01*(np.exp(x)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.009999546000702375"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_relu_function(-10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maxout activation:\n",
    "- maxout(x) = max(w1.T*x + b1, w2.T*x + b2)\n",
    "- Generalizes RELU and Leaky RELU\n",
    "- Doesn't die!\n",
    "- Problems:\n",
    "    - doubles the number of parameters per neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxout_function(x, weights, biases):\n",
    "    linear_outputs=np.dot(x, weights)+biases\n",
    "    max_output= np.max(linear_outputs, axis=1)\n",
    "    return max_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [3.  6.6]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3], [4, 5, 6]])  # Input\n",
    "weights = np.array([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])  # Weights\n",
    "biases = np.array([0.1, 0.2])  # Biases\n",
    "\n",
    "output = maxout_function(x, weights, biases)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Point:\n",
    "    - Use ReLU. Be careful with your learning rates\n",
    "    - Try out Leaky ReLU / Maxout / ELU\n",
    "    - Try out tanh but don’t expect much\n",
    "    - Don’t use sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
