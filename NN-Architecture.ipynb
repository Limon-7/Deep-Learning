{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### McCulloach & Pitt's neuron Model:\n",
    "<p align=\"center\"><img width=\"90%\" src=\"./images/percepton-1.png\" /></p>\n",
    "    \n",
    "1. $f(x)= \\begin{cases} 1  & \\quad \\text{if } \\sum w_i x_i \\geq t\\\\ o & \\quad { otherwise}\\end{cases}$\n",
    "\n",
    "<p align=\"center\"><img width=\"70%\" src=\"./images/percepton-2.png\" /></p>\n",
    "\n",
    "1. $\\sum$: Net Input Function or Wieghted Sum\n",
    "2. Activation Function or Threshold\n",
    "    * Threshold: Threshold determinds whether the neurons are activated or not. Threshold functions normally introduces non-linear properties into ANN by calculating the weighted sum and adding direction.\n",
    "\n",
    "$$\\color{green} \\text{Mathematical formulation of a biological neuron, could solve AND, OR, NOT problems, but can not solve XOR problem.}$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***NN***(Neural Network)\n",
    "A neural network is a type of machine learning algorithm that is designed to recognize patterns in data. It is modeled after the structure of the human brain, which consists of interconnected nodes called neurons. A neural network consists of multiple layers of interconnected nodes, called artificial neurons or perceptrons. Each neuron receives input from the previous layer and uses a mathematical function to generate an output, which is then passed on to the next layer.\n",
    "\n",
    "![nn](./images/nn.png)\n",
    "\n",
    "Fo example linear classification we computed scores for different visual categories given the image using the formula $s=Wx$, where $W$ was a matrix and x was an input column vector containing all pixel data of the image. CIFAR-10, $x$ is a $[3072x1]$ column vector, and $W$ is a $[10x3072]$ matrix, so that the output scores is a vector of $10$ class scores.\n",
    "An example neural network would instead compute as $s = W_2 \\max(0, W_1 x)$.<br>\n",
    "here $\\max(0, )$ is the non linearity activation function that is applied elementwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma(\\sum_{i=1}^{m}x_iw_i +b)=\\sigma(X^Tw +b)=\\hat y\\;; \\text{ use dot product}$$\n",
    "\n",
    "Percepton Algorithm:\n",
    "\n",
    "let $D=(\\langle\\textbf X^{[1]}, y^{[1]} \\rangle, \\langle\\textbf X^{[1]}, y^{[1]} \\rangle, \\cdots \\langle\\textbf X^{[n]}, y^{[n]} \\rangle) \\in(\\mathbb R^m\\times \\{0,1\\})^n$\n",
    "1. Initialize w:=$0^m\\; \\text{\\color{green} (where weight incl. bias)}$\n",
    "2. for every training epoch:\n",
    "    * for every $\\langle\\textbf X^{[1]}, y^{[1]} \\rangle\\in D$:\n",
    "        * $\\displaystyle \\hat y^{[i]}:=\\sigma(\\textbf X^{[i]T}\\textbf w)$\n",
    "        * $err:=(y^{[i]}-\\hat y^{[i]})$\n",
    "        * $\\textbf w:=\\textbf w + err\\times \\textbf X^{[i]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9996687197985136"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "class Neuron(object):\n",
    "  def __init__(self, weights):\n",
    "    self.weights = weights\n",
    "    self.bias = 0 \n",
    "  \n",
    "  def forward(self, inputs):\n",
    "    \"\"\" assume inputs and weights are 1-D numpy arrays and bias is a number \"\"\"\n",
    "    cell_body_sum = np.sum(inputs.dot( self.weights)) + self.bias # dot.product\n",
    "    # cell_body_sum = np.sum(self.weights.dot(inputs)) + self.bias # dot.product\n",
    "    # cell_body_sum = np.sum(inputs.T.dot(self.weights)) + self.bias # dot.product\n",
    "\n",
    "    firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum.sum())) # sigmoid activation function\n",
    "    return firing_rate\n",
    "\n",
    "np.random.seed(100)\n",
    "inputs=np.random.rand(1,9)\n",
    "w = np.random.rand(9,4)\n",
    "nn = Neuron(w)\n",
    "output = nn.forward(inputs)\n",
    "output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Neural Network Architecture:***\n",
    "**Neural Networks as neurons in graphs.** Neural Networks are modeled as collections of neurons that are connected in an acyclic graph. In other words, the outputs of some neurons can become inputs to other neurons. Cycles are not allowed since that would imply an infinite loop in the forward pass of a network.\n",
    "\n",
    "![nn-architecture](./images/nn-architecture.png)\n",
    "\n",
    " ***Notice that when we say N-layer neural network, we do not count the input layer.***\n",
    "\n",
    "***Sizing neural networks:*** The size of neural networks are the number of neurons, or more commonly the number of parameters.\n",
    "1. The first network (left) has ***$4 + 2 = 6$*** neurons (not counting the inputs), ***$[3 * 4] + [4 * 2] = 20$*** weights and ***$4 + 2 = 6$** biases, for a total of ***26*** learnable parameters.\n",
    "2. The second network (right) has 4 + 4 + 1 = 9 neurons, [3 x 4] + [4 x 4] + [4 x 1] = 12 + 16 + 4 = 32 weights and 4 + 4 + 1 = 9 biases, for a total of 41 learnable parameters.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network comprises of three main layers- \n",
    "1. ***Input Layer:*** The left most layer is called the input layer, and the neurons within the layer are called input neurons. A neuron of this layer is of a special kind since it has no input and it only outputs an $x_j$ value the $j^{th}$ features.\n",
    "2. ***Hidden Layer:*** One or more Hidden Layers that are intermediate layers between the input and output layer and process the data by applying complex non-linear functions to them. These layers are the key component that enables a neural network to learn complex tasks and achieve excellent performance.\n",
    "$f(h_i)=f(\\sum_{i=1}^{j} w_{ij}*x_j+b)$ is also called **fully connected** layer.\n",
    "\n",
    "3. ***Output layer:*** An Output Layer that takes as input the processed data and produces the final results which is usually taken to represent the class scores (e.g. in classification), which are arbitrary real-valued numbers, or some kind of real-valued target (e.g. in regression). So, there will be no activation function in present.\n",
    "\n",
    "***Fully-connected Layer:*** Neurons in a fully connected layer have full connections to all activations in the previous layer, as seen in regular Neural Networks. In the regular NN neurons between two adjacent layers are fully pairwise connected, but neurons within a single layer share no connections.\n",
    "\n",
    "***Single Layer Neural Network:***  A single-layer neural network describes a network with no hidden layers (input directly mapped to output). In that sense, you can sometimes hear people say that logistic regression or SVMs are simply a special case of single-layer Neural Networks.\n",
    "\n",
    "***Feed Forward Neural Network:*** The feedforward neural network was the first and simplest type. In this network the information moves only from the input layer directly through any hidden layers to the output layer without cycles/loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: shape not correct\n",
    "import numpy as np\n",
    "\n",
    "# initialize parameters\n",
    "W1 = np.random.randn(1,3)\n",
    "W2 = np.random.randn(1,3)\n",
    "W3 =np.random.randn(1,3)\n",
    "b1= np.zeros(3)\n",
    "b2= np.zeros(3)\n",
    "b3= np.zeros(3)\n",
    "# forward-pass of a 3-layer neural network:\n",
    "f = lambda x: 1.0/(1.0 + np.exp(-x)) # activation function (use sigmoid)\n",
    "x = np.random.randn(3, 1) # random input vector of three numbers (3x1)\n",
    "h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activations (4x1)\n",
    "h2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer activations (4x1)\n",
    "out = np.dot(W3, h2) + b3 # output neuron (1x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output (vector): 20\n",
      "Output (matrix): [20 14]\n"
     ]
    }
   ],
   "source": [
    "# Weight vector example\n",
    "w_vector = np.array([2, 3, 4])  # Weight vector\n",
    "x_vector = np.array([1, 2, 3])  # Input vector\n",
    "\n",
    "output_vector = np.dot(w_vector, x_vector)\n",
    "print(\"Output (vector):\", output_vector)\n",
    "\n",
    "# Weight matrix example\n",
    "w_matrix = np.array([[2, 3, 4], [1, 2, 3]])  # Weight matrix\n",
    "x_matrix = np.array([1, 2, 3])  # Input vector\n",
    "\n",
    "output_matrix = np.dot(w_matrix, x_matrix)\n",
    "print(\"Output (matrix):\", output_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1) (2, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.03872, 0.09152],\n",
       "       [0.04576, 0.10816]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dgrad= np.array([[0.44], [0.52]])\n",
    "dw= np.array([[0.088, 0.176],[0.104, 0.208]])\n",
    "print(dgrad.shape, dw.shape)\n",
    "dgrad.T*dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
