{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Architectures:\n",
    "<a id='header_cell'></a>\n",
    "\n",
    "\n",
    "|No|Topic|pytorch|tensorflow|\n",
    ":--:|:--:|:--:|--:|\n",
    "01|[LeNet](#le_net_cell)|[Code](https://github.com/Limon-7/deep-learning-libray/blob/master/pytorch/CNN-Architecture/cnn_architecture.ipynb)|[Code]()|\n",
    "02|[AlexNet](#alex_net_cell)|[Code](https://github.com/Limon-7/deep-learning-libray/blob/master/pytorch/CNN-Architecture/cnn_architecture.ipynb)|[Code]()|\n",
    "03|[ImageNet](#image_net_cell)|[Code](https://github.com/Limon-7/deep-learning-libray/blob/master/pytorch/CNN-Architecture/cnn_architecture.ipynb)|[Code]()|\n",
    "04|[ZFNet](#zf_net_cell)|[Code](https://github.com/Limon-7/deep-learning-libray/blob/master/pytorch/CNN-Architecture/cnn_architecture.ipynb)|[Code]()|\n",
    "05|[VGGNet](#vgg_net_cell)|[Code](https://github.com/Limon-7/deep-learning-libray/blob/master/pytorch/CNN-Architecture/cnn_architecture.ipynb)|[Code]()|\n",
    "06|[GoogLeNet](#google_net_cell)|[Code](https://github.com/Limon-7/deep-learning-libray/blob/master/pytorch/CNN-Architecture/cnn_architecture.ipynb)|[Code]()|\n",
    "07|[ResNet](#res_net_cell)|[Code](https://github.com/Limon-7/deep-learning-libray/blob/master/pytorch/CNN-Architecture/cnn_architecture.ipynb)|[Code]()|\n",
    "08|[NiNet](#ni_net_cell)|[Code](https://github.com/Limon-7/deep-learning-libray/blob/master/pytorch/CNN-Architecture/cnn_architecture.ipynb)|[Code]()|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [01 LeNet:](#header_cell)\n",
    "<a id='le_net_cell'></a>\n",
    "The first successful applications of Convolutional Networks were developed by Yann LeCun in 1990’s. Of these, the best known is the LeNet architecture that was used to read zip codes, digits, etc.\n",
    "\n",
    "<p align=\"center\"><img width=\"80%\" src=\"../images/cnn_architecture/LeNet.png\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Structure of the LeNet network:\n",
    "LeNet5 is a small network, it contains the basic modules of deep learning: convolutional layer, pooling layer, and full link layer. It is the basis of other deep learning models. Here we analyze LeNet5 in depth.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| # |Layer       |Feature_Map|input_size    |Kernel_Size|stride |padding|output_size    |memory |Activation |Parameters | Connections   |\n",
    "|---|   ---      |:---:      |:---:         |:---:      |:---:  |:---:  |:---:          |---:   |:---:      |---:       |---:           |\n",
    "|   |input(image)|1          |$32\\times 32$ |-          |-      |-      |$32\\times 32$  |1024   |relu       |-          |-              |\n",
    "|1  |Conv        |6          |$32\\times 32$ |$5\\times 5$|1      |0      |$28\\times 28$  |4704   |relu       |156        |122304         |\n",
    "|2  |Avg Pooling |6          |$28\\times 28$ |$2\\times 2$|2      |0      |$14\\times 14$  |1176   |relu       |-          |-              |\n",
    "|3  |Conv        |16         |$14\\times 14$ |$5\\times 5$|1      |0      |$10\\times 10$  |1600   |relu       |2416       |326144         |\n",
    "|4  |Avg Pooling |16         |$10\\times 10$ |$2\\times 2$|2      |0      |$5\\times 5$    |400    |relu       |-          |-              |\n",
    "|5  |Conv        |120        |$5\\times 5$   |$5\\times 5$|1      |0      |$1\\times 1$    |120    |relu       |48120      ||\n",
    "|6  |FC          |-          |120           |-          |-      |-      |84             |84     |relu       |10164      |10164          |\n",
    "|7  |Output(FC)  |-          |84            |-          |-      |-      |10             |10     |softmax    |850        |850            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [02 AlexNet:](#header_cell)\n",
    "<a id='alex_net_cell'></a>\n",
    "\n",
    "The first work that popularized Convolutional Networks in Computer Vision was the AlexNet, developed by Alex Krizhevsky, Ilya Sutskever and Geoff Hinton. The AlexNet was submitted to the ImageNet ILSVRC challenge in 2012 and significantly outperformed the second runner-up (top 5 error of 16% compared to runner-up with 26% error). The Network had a very similar architecture to LeNet, but was deeper, bigger, and featured Convolutional Layers stacked on top of each other (previously it was common to only have a single CONV layer always immediately followed by a POOL layer).\n",
    "\n",
    "<p align=\"center\"><img width=\"80%\" src=\"../images/cnn_architecture/AlexNet.png\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [03 ImageNet:](#header_cell)\n",
    "<a id='image_net_cell'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [04 ZFNet:](#header_cell)\n",
    "<a id='zf_net_cell'></a>\n",
    "The ILSVRC 2013 winner was a Convolutional Network from Matthew Zeiler and Rob Fergus. It became known as the ZFNet (short for Zeiler & Fergus Net). It was an improvement on AlexNet by tweaking the architecture hyperparameters, in particular by expanding the size of the middle convolutional layers and making the stride and filter size on the first layer smaller.\n",
    "\n",
    "<p align=\"center\"><img width=\"80%\" src=\"../images/cnn_architecture/ZFNet.png\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [05 VGGNet:](#header_cell)\n",
    "<a id='vgg_net_cell'></a>\n",
    "The runner-up in ILSVRC 2014 was the network from Karen Simonyan and Andrew Zisserman that became known as the VGGNet. Its main contribution was in showing that the depth of the network is a critical component for good performance. Their final best network contains 16 CONV/FC layers and, appealingly, features an extremely homogeneous architecture that only performs 3x3 convolutions and 2x2 pooling from the beginning to the end. Their pretrained model is available for plug and play use in Caffe. A downside of the VGGNet is that it is more expensive to evaluate and uses a lot more memory and parameters (140M). Most of these parameters are in the first fully connected layer, and it was since found that these FC layers can be removed with no performance downgrade, significantly reducing the number of necessary parameters.\n",
    "\n",
    "<p align=\"center\"><img width=\"80%\" src=\"../images/cnn_architecture/VGGNet.png\" /></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INPUT: [224x224x3]        memory:  224*224*3=150K   weights: 0<br>\n",
    "CONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*3)*64 = 1,728<br>\n",
    "CONV3-64: [224x224x64]  memory:  224*224*64=3.2M   weights: (3*3*64)*64 = 36,864<br>\n",
    "POOL2: [112x112x64]  memory:  112*112*64=800K   weights: 0<br>\n",
    "CONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*64)*128 = 73,728<br>\n",
    "CONV3-128: [112x112x128]  memory:  112*112*128=1.6M   weights: (3*3*128)*128 = 147,456<br>\n",
    "POOL2: [56x56x128]  memory:  56*56*128=400K   weights: 0<br>\n",
    "CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*128)*256 = 294,912<br>\n",
    "CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824<br>\n",
    "CONV3-256: [56x56x256]  memory:  56*56*256=800K   weights: (3*3*256)*256 = 589,824<br>\n",
    "POOL2: [28x28x256]  memory:  28*28*256=200K   weights: 0<br>\n",
    "CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*256)*512 = 1,179,648<br>\n",
    "CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296<br>\n",
    "CONV3-512: [28x28x512]  memory:  28*28*512=400K   weights: (3*3*512)*512 = 2,359,296<br>\n",
    "POOL2: [14x14x512]  memory:  14*14*512=100K   weights: 0<br>\n",
    "CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296<br>\n",
    "CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296<br>\n",
    "CONV3-512: [14x14x512]  memory:  14*14*512=100K   weights: (3*3*512)*512 = 2,359,296<br>\n",
    "POOL2: [7x7x512]  memory:  7*7*512=25K  weights: 0<br>\n",
    "FC: [1x1x4096]  memory:  4096  weights: 7*7*512*4096 = 102,760,448<br>\n",
    "FC: [1x1x4096]  memory:  4096  weights: 4096*4096 = 16,777,216<br>\n",
    "FC: [1x1x1000]  memory:  1000 weights: 4096*1000 = 4,096,000<br>\n",
    "\n",
    "TOTAL memory: 24M * 4 bytes ~= 93MB / image (only forward! ~*2 for bwd)<br>\n",
    "TOTAL params: 138M parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [06 GoogLeNet:](#header_cell)\n",
    "<a id='google_net_cell'></a>\n",
    "The ILSVRC 2014 winner was a Convolutional Network from Szegedy et al. from Google. Its main contribution was the development of an Inception Module that dramatically reduced the number of parameters in the network (4M, compared to AlexNet with 60M). Additionally, this paper uses Average Pooling instead of Fully Connected layers at the top of the ConvNet, eliminating a large amount of parameters that do not seem to matter much. There are also several followup versions to the GoogLeNet, most recently Inception-v4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [07 ResNet:](#header_cell)\n",
    "<a id='res_net_cell'></a>\n",
    "ResNet. Residual Network developed by Kaiming He et al. was the winner of ILSVRC 2015. It features special skip connections and a heavy use of batch normalization. The architecture is also missing fully connected layers at the end of the network. The reader is also referred to Kaiming’s presentation (video, slides), and some recent experiments that reproduce these networks in Torch. ResNets are currently by far state of the art Convolutional Neural Network models and are the default choice for using ConvNets in practice (as of May 10, 2016). In particular, also see more recent developments that tweak the original architecture from Kaiming He et al. Identity Mappings in Deep Residual Networks (published March 2016)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [08 LiNet:](#header_cell)\n",
    "<a id='ni_net_cell'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
