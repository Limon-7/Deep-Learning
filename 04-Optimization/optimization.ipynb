{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization:\n",
    "Come up with a way of efficiently finding the parameters that minimize the loss function is called Optimizer.\n",
    "1. Parameter Update\n",
    "2. Hyperparameter Optimization\n",
    "\n",
    "#### 1. Parameter Updates:\n",
    "Parameter update, also known as weight update, is a crucial step in training a machine learning model. During the training process, the model's parameters (weights and biases) are adjusted or updated based on the information provided by the training data and the optimization algorithm being used. The goal is to find the optimal values of the parameters that minimize the loss or error of the model and enable it to make accurate predictions on new, unseen data.\n",
    "\n",
    "1. First-order (SGD), momentum, Nesterov momentum\n",
    "2. Annealing the learning rate\n",
    "3. Second-order methods\n",
    "4. Per-parameter adaptive learning rates (Adagrad, RMSProp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 First Order Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Second Order Optimization\n",
    "Second-order optimization methods, such as Newton's method and variants like the Gauss-Newton method or the Levenberg-Marquardt algorithm, use the second-order derivatives of the loss function with respect to the model parameters to update the parameters. These methods provide more information about the curvature of the loss function compared to first-order methods, which only use the gradient information.\n",
    "\n",
    "    The Newton's method requires the inversion of the Hessian matrix, which can be computationally expensive and may not always be feasible for large-scale problems. \n",
    "\n",
    "1. Newton's method or Newton-Raphson method:\n",
    "`θ_new = θ_old - H^(-1) * ∇L(θ_old)`\n",
    "2.  the Gauss-Newton method:\n",
    "3. Levenberg-Marquardt algorithm:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In practice,** it is currently not common to see L-BFGS or similar second-order methods applied to large-scale Deep Learning and Convolutional Neural Networks. Instead, SGD variants based on (Nesterov’s) momentum are more standard because they are simpler and scale more easily."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Hyperparameter optimization:\n",
    "Parameter optimization, also known as hyperparameter optimization, is the process of selecting the best values for the hyperparameters of a machine learning algorithm. Hyperparameters are adjustable settings that are not learned from the data, but rather set before the learning process begins. They control the behavior and performance of the machine learning model.\n",
    "\n",
    "    The goal of parameter optimization is to find the optimal combination of hyperparameter values that yields the best performance for a given learning algorithm and dataset. \n",
    "\n",
    "The most common hyperparameters in context of Neural Networks include:\n",
    "1. **the initial learning rate:** If the learning rate (LR) is too small, overfitting can occur. Large learning rates help to regularize the training but if the learning rate is too large, the training will diverge. \n",
    "2. **learning rate decay schedule (such as the decay constant):** Test with short runs of momentum values 0.99, 0.97, 0.95, and 0.9 to get the best value for momentum.\n",
    "3. **regularization strength (L2 penalty, dropout strength):** Weight decay is one form of regularization and it plays an important role in training so its value needs to be set properly [7]. Weight decay is defined as multiplying each weight in the gradient descent at each epoch by a factor $λ [0<λ<1]$.\n",
    "\n",
    "The common algorithom uses to choose best hypoparameter are following:\n",
    "1. Random Serach\n",
    "2. Local search\n",
    "3. Grid Search\n",
    "4. Bayesian Optimization\n",
    "5. Evolutionary Algorithms\n",
    "6. Gradient Base Optimization\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Random Search\n",
    "Random Search is a simple optimization algorithm that involves randomly sampling points from the search space to find the best solution. It does not utilize any gradient information or prior knowledge about the objective function, making it a straightforward and easy-to-implement approach for optimization tasks.\n",
    "\n",
    "1. Advantages:\n",
    "    1. Random Search is eaasy to implement.\n",
    "2. Limitations:\n",
    "    1. potentially slow convergence.\n",
    "    2. High number of iterations is required to get an optimal parameters.\n",
    "\n",
    "Random Search can serve as a baseline or starting point for optimization tasks, especially in situations where the search space is not well understood or there is no prior information available.\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "1.  Define a search space.\n",
    "2. Define the iteration number.\n",
    "3. Sample random parameters\n",
    "4. Evalute the score: score\n",
    "5. Update the best hyperparameters if the score is improved:\n",
    "    `if(score > best_score) best_score=score best_params=params`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'learning_rate': 0.09000000000000001, 'batch_size': 16, 'num_hidden_units': 128, 'dropout_rate': 0.2}\n",
      "Best score: 0.9893745487102236\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the search space\n",
    "search_space = {\n",
    "    'learning_rate': np.linspace(0.01, 0.1, 10),\n",
    "    'batch_size': [16, 32, 64, 128],\n",
    "    'num_hidden_units': [32, 64, 128, 256],\n",
    "    'dropout_rate': np.linspace(0.0, 0.5, 6)\n",
    "}\n",
    "\n",
    "num_iterations = 100\n",
    "\n",
    "def evaluate_model(params):\n",
    "    # Here, we can train and test a model or perform any other evaluation task\n",
    "    score = np.random.random()  # Random score for demonstration purposes\n",
    "    return score\n",
    "\n",
    "# Perform random search\n",
    "best_params = None\n",
    "best_score = float('-inf') ## Python assigns the highest possible float value\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    # Sample random hyperparameters\n",
    "    params = {param: np.random.choice(values) for param, values in search_space.items()}\n",
    "\n",
    "    # Evaluate the performance using the current hyperparameters\n",
    "    score = evaluate_model(params)\n",
    "\n",
    "    # Update the best hyperparameters if the score is improved\n",
    "    if score > best_score:\n",
    "        best_params = params\n",
    "        best_score = score\n",
    "\n",
    "# Print the best hyperparameters and the corresponding score\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "print(\"Best score:\", best_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Random Local Search\n",
    "Random Local Search is an optimization technique that aims to find the optimal solution within a search space by iteratively exploring the neighborhood of the current solution.\n",
    "It starts with an initial solution and iteratively explores the search space by randomly generating new solutions in the vicinity of the current solution. The new solution is accepted if it improves the objective function, and the process continues until a stopping criterion is met.\n",
    "\n",
    "Advantages:\n",
    "1. Efficient in situations where the search space has local optima or discontinuities.\n",
    "2. Can quickly escape local optima and explore different regions of the search space.\n",
    "\n",
    "Limitations:\n",
    "1. Highly dependent on the initial solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Solution: -0.09954309904125758\n",
      "Best Value: 3.9900911714332623\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "'''\n",
    "1.Initialize the best solution and its value.\n",
    "2. Generate a random neighbor\n",
    "3. Update the best solution if the neighbor is better\n",
    "'''\n",
    "def objective_function(x):\n",
    "    return -(x**2) + 4\n",
    "\n",
    "def random_neighbor(solution, search_range):\n",
    "    # Generate a random neighbor within the search range\n",
    "    neighbor = solution + random.uniform(-search_range, search_range)\n",
    "    return neighbor\n",
    "\n",
    "def random_local_search(search_range, max_iterations):\n",
    "    # step-1\n",
    "    best_solution = random.uniform(-search_range, search_range)\n",
    "    best_value = objective_function(best_solution)\n",
    "\n",
    "    # Perform random local search\n",
    "    iterations = 0\n",
    "    while iterations < max_iterations:\n",
    "        # step-2\n",
    "        neighbor = random_neighbor(best_solution, search_range)\n",
    "        neighbor_value = objective_function(neighbor)\n",
    "\n",
    "        # step-3\n",
    "        if neighbor_value > best_value:\n",
    "            best_solution = neighbor\n",
    "            best_value = neighbor_value\n",
    "\n",
    "        iterations += 1\n",
    "\n",
    "    return best_solution, best_value\n",
    "\n",
    "# Set the search range and maximum number of iterations\n",
    "search_range = 10\n",
    "max_iterations = 100\n",
    "\n",
    "# Run the random local search algorithm\n",
    "best_solution, best_value = random_local_search(search_range, max_iterations)\n",
    "\n",
    "# Print the result\n",
    "print(\"Best Solution:\", best_solution)\n",
    "print(\"Best Value:\", best_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Grid Search Algorithm:\n",
    "Grid search systematically explores all possible combinations of hyperparameter values within the specified ranges. It covers the entire search space, leaving no combination unchecked.\n",
    "\n",
    "1. Grid search can be computationally expensive, especially when dealing with a large number of hyperparameters or wide ranges of values for each hyperparameter. The time complexity increases exponentially with the number of hyperparameters and the number of values for each hyperparameter.\n",
    "2. Grid search is well-suited for cases where there is prior knowledge about the ranges and values of hyperparameters that are likely to perform well. It is useful when the impact of each hyperparameter value can be predicted.\n",
    "\n",
    "**Note:** Performing random search rather than grid search allows you to much more precisely discover good values for the important ones. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Bayesian Optimization:\n",
    "The Bayesian optimization algorithm is an iterative process that intelligently explores and exploits the search space to find the optimal set of hyperparameters. The Bayesian optimization algorithm leverages the probabilistic model to guide the search toward promising regions in the hyperparameter space, allowing for efficient exploration and exploitation. It adapts the search based on the model's uncertainty and the observed objective function values. \n",
    "\n",
    "By intelligently selecting the next set of hyperparameters to evaluate, Bayesian optimization can converge to good solutions with fewer evaluations compared to random or grid search, making it an effective technique for hyperparameter optimization in machine learning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Evolutionary Algorithms:\n",
    "Evolutionary algorithms mimic natural selection to iteratively search for optimal hyperparameter values. These algorithms maintain a population of candidate solutions and evolve them over generations using genetic operators such as mutation, crossover, and selection."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Gradient-Based Optimization:\n",
    "Some algorithms allow for gradient-based optimization of hyperparameters. This involves computing the gradients of a performance metric with respect to the hyperparameters and using optimization techniques such as gradient descent to update the hyperparameter values iteratively."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
